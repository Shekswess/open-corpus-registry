{"dataset_id": "a-m-team/AM-DeepSeek-R1-Distilled-1.4M", "dataset_url": "https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M", "stage": "post-training", "nature": "mixed", "content_types": "code, math, qa, reasoning", "tokens": null, "description": "A large-scale general reasoning task dataset composed of high-quality and challenging reasoning problems collected from numerous open-source datasets, semantically deduplicated, and cleaned. Responses are distilled from the reasoning model (mostly DeepSeek-R1) and verified through various methods.", "author": "a-m-team", "hf_id": "a-m-team/AM-DeepSeek-R1-Distilled-1.4M", "downloads": 1610, "likes": 172, "license": "cc-by-nc-4.0", "languages": ["zh", "en"], "task_categories": ["text-generation"], "created_at": "2025-03-09T10:23:33+00:00", "last_modified": "2025-03-30T01:30:08+00:00", "citation": null}
{"dataset_id": "agentica-org/deepscaler-preview-dataset", "dataset_url": "https://huggingface.co/datasets/agentica-org/deepscaler-preview-dataset", "stage": "post-training", "nature": "real", "content_types": "math", "tokens": null, "description": "The dataset consists of approximately 40,000 unique mathematics problem-answer pairs compiled from various math competitions and datasets, including AIME, AMC, Omni-MATH, and Still dataset.", "author": "agentica-org", "hf_id": "agentica-org/DeepScaleR-Preview-Dataset", "downloads": 8660, "likes": 183, "license": "mit", "languages": ["en"], "task_categories": null, "created_at": "2025-02-09T12:24:25+00:00", "last_modified": "2025-02-10T09:51:18+00:00", "citation": null}
{"dataset_id": "AI-MO/NuminaMath-1.5", "dataset_url": "https://huggingface.co/datasets/AI-MO/NuminaMath-1.5", "stage": "post-training", "nature": "real", "content_types": "math, reasoning", "tokens": null, "description": "This dataset contains approximately 900k competition-level math problems with Chain of Thought solutions, sourced from Chinese high school exercises, US and international math olympiads, and various online forums.", "author": "AI-MO", "hf_id": "AI-MO/NuminaMath-1.5", "downloads": 1693, "likes": 166, "license": "apache-2.0", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2025-02-10T12:34:15+00:00", "last_modified": "2025-02-10T13:28:01+00:00", "citation": null}
{"dataset_id": "AI-MO/NuminaMath-CoT", "dataset_url": "https://huggingface.co/datasets/AI-MO/NuminaMath-CoT", "stage": "post-training", "nature": "mixed", "content_types": "math, other, reasoning", "tokens": null, "description": "Approximately 860k math problems, where each solution is formatted in a Chain of Thought (CoT) manner. The sources of the dataset range from Chinese high school math exercises to US and international mathematics olympiad competition problems.", "author": "AI-MO", "hf_id": "AI-MO/NuminaMath-CoT", "downloads": 11904, "likes": 518, "license": "apache-2.0", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2024-07-15T20:14:23+00:00", "last_modified": "2024-11-25T05:31:43+00:00", "citation": null}
{"dataset_id": "AI-MO/NuminaMath-TIR", "dataset_url": "https://huggingface.co/datasets/AI-MO/NuminaMath-TIR", "stage": "post-training", "nature": "mixed", "content_types": "math, reasoning, tool-use", "tokens": null, "description": "The dataset focuses on tool-integrated reasoning (TIR) by selecting approximately 70k problems from the NuminaMath-CoT dataset, generating TORA-like reasoning paths using GPT-4, and filtering for accurate solutions.", "author": "AI-MO", "hf_id": "AI-MO/NuminaMath-TIR", "downloads": 2541, "likes": 140, "license": "apache-2.0", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2024-07-16T07:44:24+00:00", "last_modified": "2024-11-25T05:32:53+00:00", "citation": null}
{"dataset_id": "ai2-adapt-dev/flan_v2_converted", "dataset_url": "https://huggingface.co/datasets/ai2-adapt-dev/flan_v2_converted", "stage": "post-training", "nature": "real", "content_types": "instruction-following", "tokens": null, "description": "This is a converted version of the Flan dataset into Tulu SFT training format, using subsets from a reproduced version by the OpenOrca team.", "author": "ai2-adapt-dev", "hf_id": "ai2-adapt-dev/flan_v2_converted", "downloads": 165, "likes": 2, "license": null, "languages": null, "task_categories": null, "created_at": "2024-10-14T11:49:52+00:00", "last_modified": "2024-10-14T18:53:42+00:00", "citation": null}
{"dataset_id": "ajibawa-2023/Code-290k-ShareGPT", "dataset_url": "https://huggingface.co/datasets/ajibawa-2023/Code-290k-ShareGPT", "stage": "pretraining", "nature": "real", "content_types": "code, conversation", "tokens": null, "description": "This dataset contains around 290,000 sets of conversations in Vicuna/ShareGPT format, including code in multiple programming languages with detailed explanations.", "author": "ajibawa-2023", "hf_id": "ajibawa-2023/Code-290k-ShareGPT", "downloads": 71, "likes": 29, "license": "apache-2.0", "languages": ["en"], "task_categories": ["conversational", "text-generation"], "created_at": "2024-01-04T18:17:24+00:00", "last_modified": "2024-01-16T17:58:03+00:00", "citation": null}
{"dataset_id": "allenai/c4", "dataset_url": "https://huggingface.co/datasets/allenai/c4", "stage": "pretraining", "nature": "real", "content_types": "web", "tokens": null, "description": "A colossal, cleaned version of Common Crawl's web crawl corpus, prepared for pretraining language models.", "author": "allenai", "hf_id": "allenai/c4", "downloads": 637792, "likes": 494, "license": ["odc-by"], "languages": ["af", "am", "ar", "az", "be", "bg", "bn", "ca", "ceb", "co", "cs", "cy", "da", "de", "el", "en", "eo", "es", "et", "eu", "fa", "fi", "fil", "fr", "fy", "ga", "gd", "gl", "gu", "ha", "haw", "he", "hi", "hmn", "ht", "hu", "hy", "id", "ig", "is", "it", "iw", "ja", "jv", "ka", "kk", "km", "kn", "ko", "ku", "ky", "la", "lb", "lo", "lt", "lv", "mg", "mi", "mk", "ml", "mn", "mr", "ms", "mt", "my", "ne", "nl", "no", "ny", "pa", "pl", "ps", "pt", "ro", "ru", "sd", "si", "sk", "sl", "sm", "sn", "so", "sq", "sr", "st", "su", "sv", "sw", "ta", "te", "tg", "th", "tr", "uk", "und", "ur", "uz", "vi", "xh", "yi", "yo", "zh", "zu"], "task_categories": ["text-generation", "fill-mask"], "created_at": "2022-03-02T23:29:22+00:00", "last_modified": "2024-01-09T19:14:03+00:00", "citation": null}
{"dataset_id": "allenai/coconot", "dataset_url": "https://huggingface.co/datasets/allenai/coconot", "stage": "post-training", "nature": "real", "content_types": "conversation, evaluation, preference, safety", "tokens": null, "description": "CoCoNot is a dataset designed for benchmarking and enhancing noncompliance behavior in chat-based language models, featuring queries that should not be complied with and contrast queries that should be complied with.", "author": "allenai", "hf_id": "allenai/coconot", "downloads": 1268, "likes": 17, "license": null, "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2024-06-11T19:32:54+00:00", "last_modified": "2024-07-18T16:24:12+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-DPO-Model-Response-Pool", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-DPO-Model-Response-Pool", "stage": "post-training", "nature": "real", "content_types": "instruction-following", "tokens": null, "description": "This dataset contains up to 2.5 million responses for each model in the Olmo 3 DPO model pool, totaling about 71 million prompt-response pairs. Prompts are sourced from allenai/Dolci-Instruct-SFT and allenai/WildChat.", "author": "allenai", "hf_id": "allenai/Dolci-DPO-Model-Response-Pool", "downloads": 972, "likes": 1, "license": "odc-by", "languages": ["en"], "task_categories": null, "created_at": "2025-12-09T22:57:41+00:00", "last_modified": "2025-12-11T20:33:27+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-Instruct-DPO", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-Instruct-DPO", "stage": "post-training", "nature": "mixed", "content_types": "preference", "tokens": null, "description": "The Dolci Instruct DPO mixture contains 260,000 preference pairs used to preference tune Olmo 3 Instruct 7B, including pairs created with a preference heuristic and a delta-aware Ultrafeedback-esque GPT-judge pipeline, as well as multiturn preference pairs.", "author": "allenai", "hf_id": "allenai/Dolci-Instruct-DPO", "downloads": 804, "likes": 5, "license": "odc-by", "languages": null, "task_categories": null, "created_at": "2025-10-22T00:10:27+00:00", "last_modified": "2025-11-20T13:55:08+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-Instruct-RL", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-Instruct-RL", "stage": "post-training", "nature": "real", "content_types": "code, conversation, instruction-following, math", "tokens": null, "description": "Dolci-Instruct-RL is a reinforcement learning dataset used to train the Olmo-3-7B-Instruct model, containing 169,964 prompts across math, code, precise instruction following, and general chat.", "author": "allenai", "hf_id": "allenai/Dolci-Instruct-RL", "downloads": 389, "likes": 9, "license": null, "languages": null, "task_categories": null, "created_at": "2025-11-19T21:22:59+00:00", "last_modified": "2025-12-10T03:18:11+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-Instruct-SFT", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-Instruct-SFT", "stage": "post-training", "nature": "real", "content_types": "code, instruction-following, math, reasoning, tool-use", "tokens": null, "description": "The Dolci Instruct SFT mixture contains 2,152,112 samples used to train Olmo 3 7B Instruct SFT, comprising prompts from various sources including code, math, reasoning, and instruction-following tasks.", "author": "allenai", "hf_id": "allenai/Dolci-Instruct-SFT", "downloads": 2501, "likes": 24, "license": "odc-by", "languages": ["amh", "arb", "ary", "ars", "acq", "arz", "apc", "ben", "ceb", "dan", "deu", "ell", "eng", "eus", "fil", "fin", "fra", "gle", "guj", "hat", "hau", "hin", "hun", "ibo", "ind", "ita", "jav", "jpn", "kan", "kir", "kor", "kur", "lit", "mal", "mar", "mlg", "msa", "mya", "nep", "nld", "nso", "nya", "pan", "pes", "pol", "por", "pus", "rus", "sin", "sna", "snd", "som", "spa", "sqi", "srp", "sun", "swa", "swe", "tam", "tel", "tha", "tur", "ukr", "urd", "vie", "wol", "xho", "yor", "zho", "zul"], "task_categories": ["other"], "created_at": "2025-11-18T03:54:15+00:00", "last_modified": "2025-11-26T01:45:01+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-Instruct-SFT-No-Tools", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-Instruct-SFT-No-Tools", "stage": "post-training", "nature": "real", "content_types": "instruction-following", "tokens": null, "description": "A version of the new Instruct dataset with tool-use data removed, intended for research and educational use.", "author": "allenai", "hf_id": "allenai/Dolci-Instruct-SFT-No-Tools", "downloads": 866, "likes": 2, "license": "odc-by", "languages": null, "task_categories": null, "created_at": "2025-11-18T02:46:06+00:00", "last_modified": "2025-11-20T13:56:06+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-RL-Zero-Code-7B", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-RL-Zero-Code-7B", "stage": "post-training", "nature": "real", "content_types": "code", "tokens": null, "description": "Dolci RL-Zero Code is a dataset of 13.3k coding questions and answers for RLVR training of Olmo 3 7B RL-Zero Code.", "author": "allenai", "hf_id": "allenai/Dolci-RL-Zero-Code-7B", "downloads": 266, "likes": 6, "license": "odc-by", "languages": ["en"], "task_categories": ["reinforcement-learning"], "created_at": "2025-11-17T23:15:15+00:00", "last_modified": "2025-12-09T20:50:25+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-RL-Zero-General-7B", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-RL-Zero-General-7B", "stage": "post-training", "nature": "real", "content_types": "conversation", "tokens": null, "description": "Dolci-RL-Zero-General-7B is the reinforcement learning dataset used to train the Olmo3-RL-Zero-7B-General model. It contains general chat prompts sampled from the larger Dolci-Think-RL mixture.", "author": "allenai", "hf_id": "allenai/Dolci-RL-Zero-General-7B", "downloads": 206, "likes": 2, "license": "odc-by", "languages": ["en"], "task_categories": ["reinforcement-learning"], "created_at": "2025-11-17T23:15:15+00:00", "last_modified": "2025-12-09T20:51:25+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-RL-Zero-IF-7B", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-RL-Zero-IF-7B", "stage": "post-training", "nature": "real", "content_types": "instruction-following", "tokens": null, "description": "Dolci RL-Zero IF is a dataset of 13.3k instruction-following prompts and answers for RLVR training of Olmo 3 7B RL-Zero IF.", "author": "allenai", "hf_id": "allenai/Dolci-RL-Zero-IF-7B", "downloads": 201, "likes": 2, "license": "odc-by", "languages": ["en"], "task_categories": ["reinforcement-learning"], "created_at": "2025-11-17T23:15:15+00:00", "last_modified": "2025-12-09T20:50:41+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-RL-Zero-Math-7B", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-RL-Zero-Math-7B", "stage": "post-training", "nature": "real", "content_types": "math", "tokens": null, "description": "Dolci RL-Zero Math is a dataset of 13.3k math questions and answers for RLVR training of Olmo 3 RL-Zero Math 7B.", "author": "allenai", "hf_id": "allenai/Dolci-RL-Zero-Math-7B", "downloads": 546, "likes": 6, "license": "odc-by", "languages": ["en"], "task_categories": ["reinforcement-learning"], "created_at": "2025-11-17T23:15:15+00:00", "last_modified": "2025-12-09T20:50:07+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-Think-DPO-32B", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-Think-DPO-32B", "stage": "post-training", "nature": "real", "content_types": "preference", "tokens": null, "description": "The Dolci Think DPO mixture contains 200,000 preference pairs created with the preference heuristic described in Delta Learning, used to preference tune Olmo 3 Think 32B.", "author": "allenai", "hf_id": "allenai/Dolci-Think-DPO-32B", "downloads": 372, "likes": 6, "license": "odc-by", "languages": null, "task_categories": null, "created_at": "2025-11-20T04:33:06+00:00", "last_modified": "2025-11-20T13:56:40+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-Think-DPO-7B", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-Think-DPO-7B", "stage": "post-training", "nature": "real", "content_types": "preference", "tokens": null, "description": "The Dolci Think 7B DPO mixture contains 150,000 preference pairs created with the preference heuristic described in Delta Learning, used to preference tune Olmo 3 Think 7B.", "author": "allenai", "hf_id": "allenai/Dolci-Think-DPO-7B", "downloads": 966, "likes": 8, "license": "odc-by", "languages": null, "task_categories": null, "created_at": "2025-10-16T04:47:07+00:00", "last_modified": "2025-11-20T14:00:04+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-Think-RL-32B", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-Think-RL-32B", "stage": "post-training", "nature": "mixed", "content_types": "code, conversation, instruction-following, math", "tokens": null, "description": "Dolci-Think-RL is a deliberate reasoning RL dataset used for training the Olmo-3-32B-Think model, containing high-quality prompts covering math, code, precise instruction following, and general chat.", "author": "allenai", "hf_id": "allenai/Dolci-Think-RL-32B", "downloads": 653, "likes": 17, "license": null, "languages": null, "task_categories": null, "created_at": "2025-11-20T06:06:13+00:00", "last_modified": "2025-11-20T08:46:53+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-Think-RL-7B", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B", "stage": "post-training", "nature": "mixed", "content_types": "code, conversation, instruction-following, math, reasoning", "tokens": null, "description": "Dolci-Think-RL-7B is a reinforcement learning dataset used to train the Olmo-3-7B-Think model, containing 102,014 prompts designed to elicit deep reasoning across math, coding, precise instruction following, and general chat.", "author": "allenai", "hf_id": "allenai/Dolci-Think-RL-7B", "downloads": 1026, "likes": 9, "license": null, "languages": null, "task_categories": null, "created_at": "2025-11-18T01:55:08+00:00", "last_modified": "2025-11-20T08:46:12+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-Think-RL-7B-Completions-DPO", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-DPO", "stage": "post-training", "nature": "mixed", "content_types": "code, conversation, instruction-following, math, other", "tokens": null, "description": "A set of completions from the Olmo-3-7B-Think-DPO model over prompts used in Dolci-Think-RL, covering math, code, instruction following, general chat, and puzzles.", "author": "allenai", "hf_id": "allenai/Dolci-Think-RL-7B-Completions-DPO", "downloads": 364, "likes": 2, "license": "odc-by", "languages": null, "task_categories": null, "created_at": "2025-11-27T06:04:59+00:00", "last_modified": "2025-12-12T03:35:25+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-Think-RL-7B-Completions-SFT", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-SFT", "stage": "post-training", "nature": "mixed", "content_types": "code, conversation, instruction-following, math, other", "tokens": null, "description": "This dataset contains 5,031,398 completions from the Olmo-3-7B-Think-SFT model over prompts used in Dolci-Think-RL, covering domains such as Math, Code, Precise Instruction Following, General Chat, and Puzzles.", "author": "allenai", "hf_id": "allenai/Dolci-Think-RL-7B-Completions-SFT", "downloads": 703, "likes": 4, "license": "odc-by", "languages": null, "task_categories": null, "created_at": "2025-11-27T06:28:17+00:00", "last_modified": "2025-12-12T03:35:03+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-Think-SFT", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-Think-SFT", "stage": "post-training", "nature": "mixed", "content_types": "code, instruction-following, reasoning", "tokens": null, "description": "This dataset contains reasoning traces for instruction following, derived from multiple sources including OpenThoughts 3, SYNTHETIC-2, Nemotron Post-training dataset, and newly created prompts and traces. It was used for post-training of Olmo 3 models.", "author": "allenai", "hf_id": "allenai/Dolci-Think-SFT-32B", "downloads": 3089, "likes": 22, "license": "odc-by", "languages": ["en"], "task_categories": null, "created_at": "2025-11-17T22:15:52+00:00", "last_modified": "2025-11-25T23:15:24+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-Think-SFT-32B", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-Think-SFT-32B", "stage": "post-training", "nature": "mixed", "content_types": "code, instruction-following, other, reasoning, safety", "tokens": null, "description": "The dataset contains a mixture of existing reasoning traces and new prompts for post-training fine-tuning, including instruction following, code, and various other tasks.", "author": "allenai", "hf_id": "allenai/Dolci-Think-SFT-32B", "downloads": 3089, "likes": 22, "license": "odc-by", "languages": ["en"], "task_categories": null, "created_at": "2025-11-17T22:15:52+00:00", "last_modified": "2025-11-25T23:15:24+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-Think-SFT-7B", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-Think-SFT-7B", "stage": "post-training", "nature": "mixed", "content_types": "code, conversation, instruction-following, reasoning, safety", "tokens": null, "description": "The dataset contains a mixture of existing reasoning traces and new prompts and reasoning traces, used for post-training of the Olmo 3 7B Think model. It includes instruction following, code, and various other types of prompts with model reasoning.", "author": "allenai", "hf_id": "allenai/Dolci-Think-SFT-7B", "downloads": 2659, "likes": 8, "license": null, "languages": null, "task_categories": null, "created_at": "2025-10-16T00:50:16+00:00", "last_modified": "2025-11-25T23:10:57+00:00", "citation": null}
{"dataset_id": "allenai/Dolci-Think-SFT-Python", "dataset_url": "https://huggingface.co/datasets/allenai/Dolci-Think-SFT-Python", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following", "tokens": null, "description": "This dataset is licensed under ODC-BY. It is intended for research and educational use in accordance with Ai2's Responsible Use Guidelines.", "author": "allenai", "hf_id": "allenai/Dolci-Think-SFT-Python", "downloads": 671, "likes": 3, "license": null, "languages": null, "task_categories": null, "created_at": "2025-11-18T00:30:07+00:00", "last_modified": "2025-11-20T13:49:36+00:00", "citation": null}
{"dataset_id": "allenai/dolma", "dataset_url": "https://huggingface.co/datasets/allenai/dolma", "stage": "pretraining", "nature": "real", "content_types": "books, code, math, web, wikipedia", "tokens": 3000000000000, "description": "Dolma is a dataset of 3 trillion tokens from a diverse mix of web content, academic publications, code, books, and encyclopedic materials.", "author": "allenai", "hf_id": "allenai/dolma", "downloads": 1932, "likes": 971, "license": "odc-by", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2023-06-30T20:14:39+00:00", "last_modified": "2024-04-17T02:57:00+00:00", "citation": null}
{"dataset_id": "allenai/dolma3", "dataset_url": "https://huggingface.co/datasets/allenai/dolma3", "stage": "pretraining", "nature": "real", "content_types": "books, code, math, web, wikipedia", "tokens": 9310000000000, "description": "The Dolma 3 pool is a dataset of over 9 trillion tokens from a diverse mix of web content, academic publications, code, and more, intended for pretraining large language models.", "author": "allenai", "hf_id": "allenai/dolma3_pool", "downloads": 25570, "likes": 27, "license": "odc-by", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2025-10-16T18:13:38+00:00", "last_modified": "2025-12-11T19:42:46+00:00", "citation": null}
{"dataset_id": "allenai/dolma3_dolmino_mix-100B-1025", "dataset_url": "https://huggingface.co/datasets/allenai/dolma3_dolmino_mix-100B-1025", "stage": "midtraining", "nature": "synthetic", "content_types": "code, math, other, qa, reasoning, web", "tokens": 99950000000, "description": "The Dolma 3 Dolmino Mix (100B) is a mixture of high-quality data used for the second stage of training for the Olmo 3 7B model.", "author": "allenai", "hf_id": "allenai/dolma3_dolmino_mix-100B-1025", "downloads": 43610, "likes": 1, "license": "odc-by", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2025-10-12T22:14:27+00:00", "last_modified": "2025-11-24T18:41:57+00:00", "citation": null}
{"dataset_id": "allenai/dolma3_dolmino_mix-100B-1125", "dataset_url": "https://huggingface.co/datasets/allenai/dolma3_dolmino_mix-100B-1125", "stage": "midtraining", "nature": "synthetic", "content_types": "code, instruction-following, math, other, qa, reasoning, web", "tokens": 100000000000, "description": "This dataset contains a high-quality pool of data considered for the second stage of Olmo 3 32B, composed of various sources including math, code, QA, thinking, instruction, PDFs, and web pages.", "author": "allenai", "hf_id": "allenai/dolma3_dolmino_mix-100B-1125", "downloads": 138986, "likes": 10, "license": "odc-by", "languages": ["en"], "task_categories": null, "created_at": "2025-11-18T22:25:49+00:00", "last_modified": "2025-11-24T18:43:03+00:00", "citation": null}
{"dataset_id": "allenai/dolma3_dolmino_mix-10B-1025", "dataset_url": "https://huggingface.co/datasets/allenai/dolma3_dolmino_mix-10B-1025", "stage": "midtraining", "nature": "synthetic", "content_types": "other", "tokens": 10000000000, "description": "A smaller mixture of high-quality data at 10B tokens used for Olmo 3 stage 2 annealing training.", "author": "allenai", "hf_id": "allenai/dolma3_dolmino_mix-10B-1025", "downloads": 919, "likes": 0, "license": "odc-by", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2025-10-12T22:15:49+00:00", "last_modified": "2025-11-24T18:42:27+00:00", "citation": null}
{"dataset_id": "allenai/dolma3_dolmino_pool", "dataset_url": "https://huggingface.co/datasets/allenai/dolma3_dolmino_pool", "stage": "midtraining", "nature": "mixed", "content_types": "code, math, other, qa, web", "tokens": 2190000000000, "description": "This dataset contains a high-quality pool of data considered for the second stage of Olmo 3 7B training, comprising synthetic and web-sourced content across multiple domains.", "author": "allenai", "hf_id": "allenai/dolma3_dolmino_pool", "downloads": 401830, "likes": 6, "license": "odc-by", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2025-09-30T21:42:59+00:00", "last_modified": "2025-11-24T18:39:08+00:00", "citation": null}
{"dataset_id": "allenai/dolma3_longmino_mix-100B-1125", "dataset_url": "https://huggingface.co/datasets/allenai/dolma3_longmino_mix-100B-1125", "stage": "midtraining", "nature": "mixed", "content_types": "other", "tokens": 100000000000, "description": "The Dolma 3 Longmino Mix (100B) is the mixture of data used for the third stage of training for Olmo 3 32B model.", "author": "allenai", "hf_id": "allenai/dolma3_longmino_mix-100B-1125", "downloads": 77050, "likes": 6, "license": "odc-by", "languages": ["en"], "task_categories": null, "created_at": "2025-11-18T22:20:33+00:00", "last_modified": "2025-11-24T18:43:17+00:00", "citation": null}
{"dataset_id": "allenai/dolma3_longmino_mix-50B-1025", "dataset_url": "https://huggingface.co/datasets/allenai/dolma3_longmino_mix-50B-1025", "stage": "midtraining", "nature": "mixed", "content_types": "other", "tokens": 50000000000, "description": "The Dolma 3 Longmino Mix (50B) is the mixture of data used for the third stage of training for Olmo 3 7B model.", "author": "allenai", "hf_id": "allenai/dolma3_longmino_mix-50B-1025", "downloads": 5402, "likes": 3, "license": "odc-by", "languages": ["en"], "task_categories": null, "created_at": "2025-11-18T23:14:57+00:00", "last_modified": "2025-12-15T20:20:01+00:00", "citation": null}
{"dataset_id": "allenai/dolma3_longmino_pool", "dataset_url": "https://huggingface.co/datasets/allenai/dolma3_longmino_pool", "stage": "midtraining", "nature": "mixed", "content_types": "other", "tokens": 639000000000, "description": "Dolma 3 Longmino Pool is the full pool of documents considered for stage 3 (long context) extension training of Olmo 3 7B.", "author": "allenai", "hf_id": "allenai/dolma3_longmino_pool", "downloads": 45650, "likes": 8, "license": "odc-by", "languages": ["en"], "task_categories": null, "created_at": "2025-09-30T22:01:35+00:00", "last_modified": "2025-12-11T22:30:27+00:00", "citation": null}
{"dataset_id": "allenai/dolma3_mix-150B-1025", "dataset_url": "https://huggingface.co/datasets/allenai/dolma3_mix-150B-1025", "stage": "pretraining", "nature": "real", "content_types": "books, code, math, web, wikipedia", "tokens": 153300000000, "description": "Dolma 3 Sample: 150B Mix is a dataset composed of a sample of data for 1Bx5C and 7Bx1B, including sources like Common Crawl, olmOCR Science PDFs, StackEdu, FineMath, arXiv, and Wikipedia & Wikibooks.", "author": "allenai", "hf_id": "allenai/dolma3_mix-150B-1025", "downloads": 10588, "likes": 3, "license": "odc-by", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2025-10-09T23:43:59+00:00", "last_modified": "2025-12-12T17:31:28+00:00", "citation": null}
{"dataset_id": "allenai/dolma3_mix-6T-1025", "dataset_url": "https://huggingface.co/datasets/allenai/dolma3_mix-6T-1025", "stage": "pretraining", "nature": "real", "content_types": "books, code, math, web, wikipedia", "tokens": 6000000000000, "description": "The Dolma 3 Mix (6T) is the collection of data used during the pretraining stage to train the Olmo-3-1025-7B model. This dataset is made up of ~6 trillion tokens from a diverse mix of web content, academic publications, code, and more.", "author": "allenai", "hf_id": "allenai/dolma3_mix-6T-1025", "downloads": 57490, "likes": 22, "license": "odc-by", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2025-10-23T22:00:10+00:00", "last_modified": "2025-12-17T05:41:50+00:00", "citation": null}
{"dataset_id": "allenai/dolma3_pool", "dataset_url": "https://huggingface.co/datasets/allenai/dolma3_pool", "stage": "pretraining", "nature": "real", "content_types": "books, code, math, web, wikipedia", "tokens": 9310000000000, "description": "The Dolma 3 pool is a dataset of over 9 trillion tokens from a diverse mix of web content, academic publications, code, and more, used to train the first stage of Olmo 3 7B.", "author": "allenai", "hf_id": "allenai/dolma3_pool", "downloads": 25570, "likes": 27, "license": "odc-by", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2025-10-16T18:13:38+00:00", "last_modified": "2025-12-11T19:42:46+00:00", "citation": null}
{"dataset_id": "allenai/dolmino-mix-1124", "dataset_url": "https://huggingface.co/datasets/allenai/dolmino-mix-1124", "stage": "midtraining", "nature": "mixed", "content_types": "code, math, other, web, wikipedia", "tokens": 843000000000, "description": "A mixture of high-quality data used for the second stage of OLMo2 training, including web pages, STEM papers, encyclopedic content, code text, and synthetic math data.", "author": "allenai", "hf_id": "allenai/dolmino-mix-1124", "downloads": 48145, "likes": 88, "license": "odc-by", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2024-11-23T03:52:26+00:00", "last_modified": "2025-10-29T17:55:04+00:00", "citation": null}
{"dataset_id": "allenai/llama-3.1-tulu-3-405b-preference-mixture", "dataset_url": "https://huggingface.co/datasets/allenai/llama-3.1-tulu-3-405b-preference-mixture", "stage": "post-training", "nature": "synthetic", "content_types": "preference", "tokens": null, "description": "This preference mixture contains 360,924 generation pairs obtained using various models and was used for DPO on the Llama 3.1 Tulu 3 405B SFT checkpoint to obtain the Llama 3.1 Tulu 3 405B DPO.", "author": "allenai", "hf_id": "allenai/llama-3.1-tulu-3-405b-preference-mixture", "downloads": 162, "likes": 6, "license": null, "languages": null, "task_categories": null, "created_at": "2025-01-28T23:51:39+00:00", "last_modified": "2025-02-05T00:45:35+00:00", "citation": null}
{"dataset_id": "allenai/llama-3.1-tulu-3-70b-preference-mixture", "dataset_url": "https://huggingface.co/datasets/allenai/llama-3.1-tulu-3-70b-preference-mixture", "stage": "post-training", "nature": "real", "content_types": "preference", "tokens": null, "description": "This preference mixture contains 337,186 generation pairs obtained using various models, used for DPO on the Llama 3.1 Tulu 3 70B SFT checkpoint to obtain Llama 3.1 Tulu 3 70B DPO.", "author": "allenai", "hf_id": "allenai/llama-3.1-tulu-3-70b-preference-mixture", "downloads": 412, "likes": 19, "license": "odc-by", "languages": ["en"], "task_categories": null, "created_at": "2024-11-20T22:51:26+00:00", "last_modified": "2025-02-04T00:12:03+00:00", "citation": null}
{"dataset_id": "allenai/llama-3.1-tulu-3-8b-preference-mixture", "dataset_url": "https://huggingface.co/datasets/allenai/llama-3.1-tulu-3-8b-preference-mixture", "stage": "post-training", "nature": "real", "content_types": "preference", "tokens": null, "description": "This dataset is a preference mixture used for DPO on the Llama 3.1 Tulu 3 8B SFT checkpoint to obtain Llama 3.1 Tulu 3 8B DPO. It contains 272,898 generation pairs obtained using various models.", "author": "allenai", "hf_id": "allenai/llama-3.1-tulu-3-8b-preference-mixture", "downloads": 1666, "likes": 25, "license": "odc-by", "languages": null, "task_categories": null, "created_at": "2024-11-20T22:39:30+00:00", "last_modified": "2025-02-04T00:21:47+00:00", "citation": null}
{"dataset_id": "allenai/RLVR-GSM", "dataset_url": "https://huggingface.co/datasets/allenai/RLVR-GSM", "stage": "post-training", "nature": "real", "content_types": "math", "tokens": null, "description": "This dataset contains the GSM8k dataset formatted for use with reinforcement learning with verifiable rewards, part of the Tulu 3 release.", "author": "allenai", "hf_id": "allenai/RLVR-GSM", "downloads": 183, "likes": 7, "license": "mit", "languages": null, "task_categories": null, "created_at": "2024-11-18T22:16:49+00:00", "last_modified": "2024-11-21T05:50:32+00:00", "citation": null}
{"dataset_id": "allenai/RLVR-GSM-MATH-IF-Mixed-Constraints", "dataset_url": "https://huggingface.co/datasets/allenai/RLVR-GSM-MATH-IF-Mixed-Constraints", "stage": "post-training", "nature": "mixed", "content_types": "instruction-following, math, other", "tokens": null, "description": "This dataset contains data formatted for use with reinforcement learning with verifiable rewards, used to train the final Tulu 3 models. It includes subsets from GSM8k, MATH, and IF Prompts with verifiable constraints.", "author": "allenai", "hf_id": "allenai/RLVR-GSM-MATH-IF-Mixed-Constraints", "downloads": 1219, "likes": 30, "license": "odc-by", "languages": null, "task_categories": null, "created_at": "2024-11-18T19:03:54+00:00", "last_modified": "2024-11-26T04:25:35+00:00", "citation": null}
{"dataset_id": "allenai/RLVR-IFeval", "dataset_url": "https://huggingface.co/datasets/allenai/RLVR-IFeval", "stage": "post-training", "nature": "real", "content_types": "instruction-following", "tokens": null, "description": "This dataset contains instruction following data formatted for use with reinforcement learning with verifiable rewards, generated by sampling from the Tulu 2 SFT mixture and adding constraints from IFEval.", "author": "allenai", "hf_id": "allenai/RLVR-IFeval", "downloads": 691, "likes": 25, "license": "odc-by", "languages": null, "task_categories": null, "created_at": "2024-11-20T22:05:52+00:00", "last_modified": "2024-11-21T07:17:40+00:00", "citation": null}
{"dataset_id": "allenai/RLVR-MATH", "dataset_url": "https://huggingface.co/datasets/allenai/RLVR-MATH", "stage": "post-training", "nature": "real", "content_types": "evaluation, math", "tokens": null, "description": "The RLVR-MATH dataset is a collection of math problems designed for evaluating large language models on mathematical reasoning tasks.", "author": "allenai", "hf_id": "allenai/RLVR-MATH", "downloads": 122, "likes": 18, "license": null, "languages": null, "task_categories": null, "created_at": "2024-11-20T22:05:25+00:00", "last_modified": "2024-11-20T22:05:26+00:00", "citation": null}
{"dataset_id": "allenai/SciRIFF", "dataset_url": "https://huggingface.co/datasets/allenai/SciRIFF", "stage": "post-training", "nature": "real", "content_types": "instruction-following", "tokens": null, "description": "The SciRIFF dataset includes 137K instruction-following demonstrations for 54 scientific literature understanding tasks covering five essential scientific literature categories and spanning five domains.", "author": "allenai", "hf_id": "allenai/SciRIFF", "downloads": 310, "likes": 46, "license": "odc-by", "languages": ["en"], "task_categories": null, "created_at": "2024-06-11T05:45:26+00:00", "last_modified": "2024-06-13T06:27:05+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-hard-coded", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-hard-coded", "stage": "pretraining", "nature": "synthetic", "content_types": "code", "tokens": null, "description": "", "author": "allenai", "hf_id": "allenai/tulu-3-hard-coded-10x", "downloads": 85, "likes": 7, "license": "cc-by-4.0", "languages": ["en"], "task_categories": null, "created_at": "2024-11-20T21:09:38+00:00", "last_modified": "2024-11-21T06:12:16+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-hard-coded-10x", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-hard-coded-10x", "stage": "pretraining", "nature": "synthetic", "content_types": "code", "tokens": null, "description": "", "author": "allenai", "hf_id": "allenai/tulu-3-hard-coded-10x", "downloads": 85, "likes": 7, "license": "cc-by-4.0", "languages": ["en"], "task_categories": null, "created_at": "2024-11-20T21:09:38+00:00", "last_modified": "2024-11-21T06:12:16+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-IF-augmented-on-policy-70b", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-IF-augmented-on-policy-70b", "stage": "post-training", "nature": "mixed", "content_types": "preference", "tokens": null, "description": "This preference dataset contains prompts from SFT Data with constraints from IFEval, including 65,530 generation pairs obtained using various models.", "author": "allenai", "hf_id": "allenai/tulu-3-IF-augmented-on-policy-70b", "downloads": 55, "likes": 0, "license": null, "languages": null, "task_categories": null, "created_at": "2024-11-20T22:48:35+00:00", "last_modified": "2024-11-21T16:46:30+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-IF-augmented-on-policy-8b", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-IF-augmented-on-policy-8b", "stage": "post-training", "nature": "mixed", "content_types": "preference", "tokens": null, "description": "This preference dataset contains prompts from SFT Data with constraints from IFEval, including 65,530 generation pairs obtained using various models.", "author": "allenai", "hf_id": "allenai/tulu-3-IF-augmented-on-policy-8b", "downloads": 98, "likes": 1, "license": null, "languages": null, "task_categories": null, "created_at": "2024-11-20T22:32:12+00:00", "last_modified": "2024-11-21T16:51:08+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-personas-algebra", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-personas-algebra", "stage": "pretraining", "nature": "synthetic", "content_types": "general", "tokens": null, "description": "The README only contains an image and no textual description of the dataset.", "author": "allenai", "hf_id": "allenai/tulu-3-sft-personas-algebra", "downloads": 717, "likes": 7, "license": null, "languages": ["en"], "task_categories": null, "created_at": "2024-11-20T20:47:01+00:00", "last_modified": "2024-12-02T19:50:58+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-personas-math", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-personas-math", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following, math", "tokens": null, "description": "This dataset contains synthetically created examples to enhance model's capabilities to answer complex and hard math word problems, generated using personas.", "author": "allenai", "hf_id": "allenai/tulu-3-sft-personas-math", "downloads": 825, "likes": 13, "license": null, "languages": ["en"], "task_categories": null, "created_at": "2024-10-30T20:26:44+00:00", "last_modified": "2025-02-21T22:03:10+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-pref-personas-instruction-following", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-pref-personas-instruction-following", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following, preference", "tokens": null, "description": "This dataset contains 19890 preference examples synthetically created to enhance models' precise instruction following capabilities while satisfying several constraints. It contains preference pairs (chosen, reject responses) for preference tuning methods.", "author": "allenai", "hf_id": "allenai/tulu-3-pref-personas-instruction-following", "downloads": 399, "likes": 15, "license": null, "languages": null, "task_categories": null, "created_at": "2024-11-01T03:03:18+00:00", "last_modified": "2024-11-21T15:40:43+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-sft-mixture", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-sft-mixture", "stage": "post-training", "nature": "real", "content_types": "code, instruction-following, math, reasoning, safety", "tokens": null, "description": "The Tulu 3 SFT mixture contains 939,344 samples from various datasets used to train the Tulu 3 series of models.", "author": "allenai", "hf_id": "allenai/tulu-3-sft-mixture", "downloads": 11370, "likes": 206, "license": "odc-by", "languages": ["amh", "arb", "ary", "ars", "acq", "arz", "apc", "ben", "ceb", "dan", "deu", "ell", "eng", "eus", "fil", "fin", "fra", "gle", "guj", "hat", "hau", "hin", "hun", "ibo", "ind", "ita", "jav", "jpn", "kan", "kir", "kor", "kur", "lit", "mal", "mar", "mlg", "msa", "mya", "nep", "nld", "nso", "nya", "pan", "pes", "pol", "por", "pus", "rus", "sin", "sna", "snd", "som", "spa", "sqi", "srp", "sun", "swa", "swe", "tam", "tel", "tha", "tur", "ukr", "urd", "vie", "wol", "xho", "yor", "zho", "zul"], "task_categories": ["other"], "created_at": "2024-11-08T03:56:36+00:00", "last_modified": "2024-12-02T19:48:33+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-sft-personas-algebra", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-algebra", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following", "tokens": null, "description": "", "author": "allenai", "hf_id": "allenai/tulu-3-sft-personas-algebra", "downloads": 717, "likes": 7, "license": null, "languages": ["en"], "task_categories": null, "created_at": "2024-11-20T20:47:01+00:00", "last_modified": "2024-12-02T19:50:58+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-sft-personas-code", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-code", "stage": "post-training", "nature": "synthetic", "content_types": "code", "tokens": null, "description": "This dataset contains synthetically created examples to enhance models' coding capabilities, specifically focusing on Python programming questions grounded in real-world scenarios using personas.", "author": "allenai", "hf_id": "allenai/tulu-3-sft-personas-code", "downloads": 205, "likes": 15, "license": null, "languages": null, "task_categories": null, "created_at": "2024-10-30T20:27:12+00:00", "last_modified": "2024-11-01T18:26:37+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-sft-personas-instruction-following", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-instruction-following", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following", "tokens": null, "description": "This dataset contains synthetically created examples to enhance model's capabilities to follow instructions precisely and satisfy user constraints, using personas to generate diverse instructions.", "author": "allenai", "hf_id": "allenai/tulu-3-sft-personas-instruction-following", "downloads": 2202, "likes": 60, "license": "odc-by", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2024-10-30T20:27:20+00:00", "last_modified": "2024-11-21T15:57:21+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-sft-personas-math", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following, math", "tokens": null, "description": "This dataset contains synthetically created examples to enhance model's capabilities to answer complex and hard math word problems, generated using personas.", "author": "allenai", "hf_id": "allenai/tulu-3-sft-personas-math", "downloads": 825, "likes": 13, "license": null, "languages": ["en"], "task_categories": null, "created_at": "2024-10-30T20:26:44+00:00", "last_modified": "2025-02-21T22:03:10+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-sft-personas-math-filtered", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math-filtered", "stage": "post-training", "nature": "mixed", "content_types": "instruction-following, math", "tokens": null, "description": "A filtered version of the Tulu 3 SFT dataset where only questions where GPT-4o reached a majority vote over 5 completions are included, with all other prompts and completions removed.", "author": "allenai", "hf_id": "allenai/tulu-3-sft-personas-math-filtered", "downloads": 66, "likes": 2, "license": null, "languages": null, "task_categories": null, "created_at": "2025-01-22T01:06:34+00:00", "last_modified": "2025-02-21T22:02:46+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-sft-personas-math-grade", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math-grade", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following, math", "tokens": null, "description": "A filtered version of this dataset is available here: https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math-grade-filtered", "author": "allenai", "hf_id": "allenai/tulu-3-sft-personas-math-grade", "downloads": 135, "likes": 11, "license": null, "languages": null, "task_categories": null, "created_at": "2024-10-30T20:27:05+00:00", "last_modified": "2025-02-21T22:02:13+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-sft-personas-math-grade-filtered", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-sft-personas-math-grade-filtered", "stage": "post-training", "nature": "real", "content_types": "instruction-following, math", "tokens": null, "description": "A filtered version of the Tulu 3 SFT dataset where only questions where GPT-4o reached a majority vote over 5 completions are included.", "author": "allenai", "hf_id": "allenai/tulu-3-sft-personas-math-grade-filtered", "downloads": 39, "likes": 2, "license": null, "languages": null, "task_categories": null, "created_at": "2025-01-19T16:46:57+00:00", "last_modified": "2025-02-21T22:01:51+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-sft-prompts-ultrafeedback", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-sft-prompts-ultrafeedback", "stage": "post-training", "nature": "synthetic", "content_types": "preference", "tokens": null, "description": "This dataset contains prompts from Tulu 3 SFT and model-generated completions using various models, with preference annotations on four aspects using an LLM judge.", "author": "allenai", "hf_id": "allenai/tulu-3-sft-prompts-ultrafeedback", "downloads": 85, "likes": 3, "license": null, "languages": null, "task_categories": null, "created_at": "2024-11-21T05:10:32+00:00", "last_modified": "2024-11-21T16:40:21+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-sft-reused-off-policy", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-sft-reused-off-policy", "stage": "post-training", "nature": "synthetic", "content_types": "preference", "tokens": null, "description": "This preference dataset contains prompts from the SFT mixture and 96,911 generation pairs obtained using various models.", "author": "allenai", "hf_id": "allenai/tulu-3-sft-reused-off-policy", "downloads": 60, "likes": 1, "license": null, "languages": null, "task_categories": null, "created_at": "2024-11-20T22:31:43+00:00", "last_modified": "2024-11-21T16:53:27+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-sft-reused-on-policy-70b", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-sft-reused-on-policy-70b", "stage": "post-training", "nature": "mixed", "content_types": "preference", "tokens": null, "description": "This preference dataset contains prompts from Tulu-3-SFT and includes 19,444 generation pairs obtained using various models, intended for research and educational use.", "author": "allenai", "hf_id": "allenai/tulu-3-sft-reused-on-policy-70b", "downloads": 45, "likes": 0, "license": null, "languages": null, "task_categories": null, "created_at": "2024-11-20T22:47:51+00:00", "last_modified": "2024-11-21T16:47:54+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-sft-reused-on-policy-8b", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-sft-reused-on-policy-8b", "stage": "post-training", "nature": "real", "content_types": "preference", "tokens": null, "description": "This preference dataset contains prompts from the SFT mixture and 19,444 generation pairs obtained using various models.", "author": "allenai", "hf_id": "allenai/tulu-3-sft-reused-on-policy-8b", "downloads": 53, "likes": 0, "license": null, "languages": null, "task_categories": null, "created_at": "2024-11-20T22:32:01+00:00", "last_modified": "2024-11-21T16:53:06+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-ultrafeedback-cleaned-on-policy-70b", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-ultrafeedback-cleaned-on-policy-70b", "stage": "post-training", "nature": "mixed", "content_types": "preference", "tokens": null, "description": "This preference dataset contains prompts from Ai2's cleaned version of Ultrafeedback, further filtered to remove instances from ShareGPT. It includes 41.6k generation pairs obtained using various models.", "author": "allenai", "hf_id": "allenai/tulu-3-ultrafeedback-cleaned-on-policy-70b", "downloads": 46, "likes": 0, "license": null, "languages": null, "task_categories": null, "created_at": "2024-11-20T22:49:19+00:00", "last_modified": "2024-11-21T15:59:11+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-ultrafeedback-cleaned-on-policy-8b", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-ultrafeedback-cleaned-on-policy-8b", "stage": "post-training", "nature": "mixed", "content_types": "preference", "tokens": null, "description": "This preference dataset contains prompts from Ai2's cleaned version of Ultrafeedback, further filtered to remove instances from ShareGPT, with 41.6k generation pairs obtained using various models.", "author": "allenai", "hf_id": "allenai/tulu-3-ultrafeedback-cleaned-on-policy-8b", "downloads": 81, "likes": 3, "license": null, "languages": null, "task_categories": null, "created_at": "2024-11-20T22:32:32+00:00", "last_modified": "2024-11-21T16:48:22+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-wildchat-if-on-policy-70b", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-wildchat-if-on-policy-70b", "stage": "post-training", "nature": "mixed", "content_types": "preference", "tokens": null, "description": "This preference dataset contains prompts from WildChat with constraints and 10,792 generation pairs obtained using various models, including some on-policy data from allenai/Llama-3.1-Tulu-3-70B.", "author": "allenai", "hf_id": "allenai/tulu-3-wildchat-if-on-policy-70b", "downloads": 53, "likes": 1, "license": null, "languages": null, "task_categories": null, "created_at": "2024-11-20T22:47:58+00:00", "last_modified": "2024-11-21T16:47:07+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-wildchat-if-on-policy-8b", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-wildchat-if-on-policy-8b", "stage": "post-training", "nature": "mixed", "content_types": "preference", "tokens": null, "description": "This preference dataset contains prompts from WildChat with constraints and 10,792 generation pairs obtained using various models, including on-policy data from allenai/Llama-3.1-Tulu-3-8B.", "author": "allenai", "hf_id": "allenai/tulu-3-wildchat-if-on-policy-8b", "downloads": 58, "likes": 1, "license": null, "languages": null, "task_categories": null, "created_at": "2024-11-20T22:32:06+00:00", "last_modified": "2024-11-21T16:52:21+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-wildchat-reused-on-policy-70b", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-wildchat-reused-on-policy-70b", "stage": "post-training", "nature": "mixed", "content_types": "preference", "tokens": null, "description": "This preference dataset contains prompts from WildChat and generation pairs obtained using various models, intended for research and educational use.", "author": "allenai", "hf_id": "allenai/tulu-3-wildchat-reused-on-policy-70b", "downloads": 41, "likes": 1, "license": null, "languages": null, "task_categories": null, "created_at": "2024-11-20T22:49:10+00:00", "last_modified": "2024-11-21T16:43:05+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-wildchat-reused-on-policy-8b", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-wildchat-reused-on-policy-8b", "stage": "post-training", "nature": "mixed", "content_types": "preference", "tokens": null, "description": "This preference dataset contains prompts from WildChat and generation pairs obtained using various models, intended for research and educational use.", "author": "allenai", "hf_id": "allenai/tulu-3-wildchat-reused-on-policy-8b", "downloads": 176, "likes": 1, "license": null, "languages": null, "task_categories": null, "created_at": "2024-11-20T22:32:24+00:00", "last_modified": "2024-11-21T16:50:25+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-wildchat-ultrafeedback", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-wildchat-ultrafeedback", "stage": "post-training", "nature": "synthetic", "content_types": "preference", "tokens": null, "description": "This dataset is a preference dataset containing prompts from WildChat and model-generated completions from various models, used for Tulu 3 preference mixture.", "author": "allenai", "hf_id": "allenai/tulu-3-wildchat-ultrafeedback", "downloads": 119, "likes": 2, "license": null, "languages": null, "task_categories": null, "created_at": "2024-11-21T05:03:42+00:00", "last_modified": "2024-11-21T16:36:43+00:00", "citation": null}
{"dataset_id": "allenai/tulu-3-wildchat-unused", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-3-wildchat-unused", "stage": "post-training", "nature": "synthetic", "content_types": "preference", "tokens": null, "description": "This preference dataset contains prompts from WildChat and generation pairs obtained using various models, intended for research and educational use.", "author": "allenai", "hf_id": "allenai/tulu-3-wildchat-unused", "downloads": 77, "likes": 2, "license": null, "languages": null, "task_categories": null, "created_at": "2024-11-20T22:48:51+00:00", "last_modified": "2024-11-21T16:43:34+00:00", "citation": null}
{"dataset_id": "allenai/tulu-v2-sft-mixture", "dataset_url": "https://huggingface.co/datasets/allenai/tulu-v2-sft-mixture", "stage": "post-training", "nature": "mixed", "content_types": "code, conversation, instruction-following, other, qa", "tokens": null, "description": "The dataset is a mixture used to fine-tune the Tulu V2 language model, consisting of various sources including FLAN, Open Assistant 1, ShareGPT, GPT4-Alpaca, Code-Alpaca, LIMA, WizardLM Evol Instruct, Open-Orca, hardcoded prompts, and science examples.", "author": "allenai", "hf_id": "allenai/tulu-v2-sft-mixture", "downloads": 882, "likes": 134, "license": "odc-by", "languages": ["en"], "task_categories": ["question-answering", "conversational", "text-generation"], "created_at": "2023-11-13T21:56:34+00:00", "last_modified": "2024-05-24T21:29:24+00:00", "citation": null}
{"dataset_id": "allenai/ultrafeedback_binarized_cleaned", "dataset_url": "https://huggingface.co/datasets/allenai/ultrafeedback_binarized_cleaned", "stage": "post-training", "nature": "real", "content_types": "preference", "tokens": null, "description": "This is a version of the UltraFeedback binarized dataset with TruthfulQA prompts removed and source annotations added.", "author": "allenai", "hf_id": "allenai/ultrafeedback_binarized_cleaned", "downloads": 1317, "likes": 71, "license": "mit", "languages": null, "task_categories": null, "created_at": "2023-11-29T18:03:33+00:00", "last_modified": "2023-12-01T18:42:09+00:00", "citation": null}
{"dataset_id": "allenai/WildChat", "dataset_url": "https://huggingface.co/datasets/allenai/WildChat", "stage": "post-training", "nature": "real", "content_types": "conversation", "tokens": null, "description": "WildChat is a collection of 650K conversations between human users and ChatGPT, gathered by offering online users free access to OpenAI's GPT-3.5 and GPT-4. It includes diverse user-chatbot interactions such as ambiguous requests, code-switching, and political discussions, and is intended for instructional fine-tuning and studying user behaviors.", "author": "allenai", "hf_id": "allenai/WildChat", "downloads": 3305, "likes": 169, "license": "odc-by", "languages": null, "task_categories": ["text-generation", "question-answering", "text2text-generation"], "created_at": "2023-10-27T23:53:36+00:00", "last_modified": "2025-08-11T20:35:20+00:00", "citation": null}
{"dataset_id": "allenai/WildChat-1M", "dataset_url": "https://huggingface.co/datasets/allenai/WildChat-1M", "stage": "post-training", "nature": "real", "content_types": "conversation", "tokens": null, "description": "WildChat is a collection of 1 million conversations between human users and ChatGPT, alongside demographic data, including state, country, hashed IP addresses, and request headers.", "author": "allenai", "hf_id": "allenai/WildChat-1M", "downloads": 12059, "likes": 403, "license": "odc-by", "languages": null, "task_categories": ["text-generation", "question-answering", "text2text-generation"], "created_at": "2024-05-03T05:48:22+00:00", "last_modified": "2024-10-17T18:04:41+00:00", "citation": null}
{"dataset_id": "allenai/wildguardmix", "dataset_url": "https://huggingface.co/datasets/allenai/wildguardmix", "stage": "post-training", "nature": "mixed", "content_types": "safety", "tokens": null, "description": "WildGuardMix is a dataset designed for training and evaluating safety classifiers in LLMs, consisting of synthetic data, in-the-wild user-LLLM interactions, and existing annotator-written data covering both harmful and benign scenarios.", "author": "allenai", "hf_id": "allenai/wildguardmix", "downloads": 4002, "likes": 54, "license": "odc-by", "languages": ["en"], "task_categories": ["text-classification"], "created_at": "2024-06-15T03:38:29+00:00", "last_modified": "2024-06-29T06:29:47+00:00", "citation": null}
{"dataset_id": "allenai/wildjailbreak", "dataset_url": "https://huggingface.co/datasets/allenai/wildjailbreak", "stage": "post-training", "nature": "synthetic", "content_types": "evaluation, safety", "tokens": null, "description": "WildJailbreak is a synthetic safety-training dataset containing 262K prompt-response pairs of vanilla and adversarial jailbreak queries to mitigate exaggerated safety behaviors in language models.", "author": "allenai", "hf_id": "allenai/wildjailbreak", "downloads": 3413, "likes": 93, "license": "odc-by", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2024-06-18T04:31:27+00:00", "last_modified": "2024-08-08T05:39:06+00:00", "citation": null}
{"dataset_id": "allenai/winogrande", "dataset_url": "https://huggingface.co/datasets/allenai/winogrande", "stage": "post-training", "nature": "real", "content_types": "reasoning", "tokens": null, "description": "WinoGrande is a collection of 44k problems inspired by the Winograd Schema Challenge, formulated as a fill-in-a-blank task with binary options requiring commonsense reasoning.", "author": "allenai", "hf_id": "allenai/winogrande", "downloads": 159107, "likes": 73, "license": null, "languages": ["en"], "task_categories": null, "created_at": "2022-03-02T23:29:22+00:00", "last_modified": "2025-07-11T17:33:56+00:00", "citation": null}
{"dataset_id": "BAAI/OpenSeek-Synthetic-Reasoning-Data-Examples", "dataset_url": "https://huggingface.co/datasets/BAAI/OpenSeek-Synthetic-Reasoning-Data-Examples", "stage": "pretraining", "nature": "synthetic", "content_types": "code, math, reasoning", "tokens": null, "description": "The dataset contains synthetic reasoning data synthesized from raw documents across math, code, and general knowledge domains, including chain-of-thought reasoning samples.", "author": "BAAI", "hf_id": "BAAI/OpenSeek-Synthetic-Reasoning-Data-Examples", "downloads": 333, "likes": 24, "license": "cc-by-sa-4.0", "languages": ["en"], "task_categories": null, "created_at": "2025-02-25T09:54:37+00:00", "last_modified": "2025-03-11T07:55:28+00:00", "citation": null}
{"dataset_id": "BAAI/TACO", "dataset_url": "https://huggingface.co/datasets/BAAI/TACO", "stage": "post-training", "nature": "real", "content_types": "code, instruction-following", "tokens": null, "description": "TACO is a benchmark for code generation with 26443 problems, used to evaluate language models' ability to generate code from natural language specifications.", "author": "BAAI", "hf_id": "BAAI/TACO", "downloads": 2352, "likes": 129, "license": "apache-2.0", "languages": ["code"], "task_categories": ["text-generation"], "created_at": "2023-12-20T11:27:47+00:00", "last_modified": "2024-06-19T09:16:49+00:00", "citation": null}
{"dataset_id": "bespokelabs/Bespoke-Stratos-17k", "dataset_url": "https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k", "stage": "post-training", "nature": "synthetic", "content_types": "code, math, reasoning", "tokens": null, "description": "Bespoke-Stratos-17k is a reasoning dataset containing questions, reasoning traces, and answers, created by replicating and improving the Berkeley Sky-T1 data pipeline using SFT distillation data from DeepSeek-R1.", "author": "bespokelabs", "hf_id": "bespokelabs/Bespoke-Stratos-17k", "downloads": 7426, "likes": 336, "license": "apache-2.0", "languages": ["en"], "task_categories": null, "created_at": "2025-01-21T09:38:20+00:00", "last_modified": "2025-01-31T00:00:38+00:00", "citation": null}
{"dataset_id": "bethgelab/CuratedThoughts", "dataset_url": "https://huggingface.co/datasets/bethgelab/CuratedThoughts", "stage": "post-training", "nature": "real", "content_types": "math, reasoning", "tokens": null, "description": "CuratedThoughts is a filtered version of OpenR1-Math-220k, OpenThoughts-114k, and OpenThoughts-114k-math datasets, designed to enhance their suitability for GRPO-based training by removing problematic entries such as questions with subquestions, requests for mathematical proofs, references to figures, and entries with no ground truth.", "author": "bethgelab", "hf_id": "bethgelab/CuratedThoughts", "downloads": 387, "likes": 44, "license": null, "languages": null, "task_categories": null, "created_at": "2025-02-16T10:45:26+00:00", "last_modified": "2025-02-26T16:42:56+00:00", "citation": null}
{"dataset_id": "bigcode/commitpackft", "dataset_url": "https://huggingface.co/datasets/bigcode/commitpackft", "stage": "post-training", "nature": "real", "content_types": "code, instruction-following", "tokens": null, "description": "CommitPackFT is a 2GB filtered version of CommitPack to contain only high-quality commit messages that resemble natural language instructions.", "author": "bigcode", "hf_id": "bigcode/commitpackft", "downloads": 197116, "likes": 79, "license": "mit", "languages": ["code"], "task_categories": null, "created_at": "2023-06-27T06:54:48+00:00", "last_modified": "2023-08-20T07:13:43+00:00", "citation": null}
{"dataset_id": "bigcode/self-oss-instruct-sc2-exec-filter-50k", "dataset_url": "https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k", "stage": "post-training", "nature": "real", "content_types": "code, instruction-following", "tokens": null, "description": "Final self-alignment training dataset for StarCoder2-Instruct, containing seed Python functions, concepts, instructions, and execution-validated responses.", "author": "bigcode", "hf_id": "bigcode/self-oss-instruct-sc2-exec-filter-50k", "downloads": 320, "likes": 104, "license": "odc-by", "languages": null, "task_categories": null, "created_at": "2024-04-19T23:40:21+00:00", "last_modified": "2024-11-04T19:00:05+00:00", "citation": null}
{"dataset_id": "bigcode/starcoder2data-extras", "dataset_url": "https://huggingface.co/datasets/bigcode/starcoder2data-extras", "stage": "pretraining", "nature": "real", "content_types": "code, other, web, wikipedia", "tokens": null, "description": "This dataset contains extra sources used to train the StarCoder2 family of models, including Kaggle notebooks, StackOverflow conversations, GitHub issues, Open-Web-Math, high quality code files, Wikipedia, ArXiv, intermediate representations of various languages, and library documentation.", "author": "bigcode", "hf_id": "bigcode/starcoder2data-extras", "downloads": 1982, "likes": 10, "license": null, "languages": null, "task_categories": null, "created_at": "2025-01-07T12:24:04+00:00", "last_modified": "2025-03-19T19:33:51+00:00", "citation": null}
{"dataset_id": "bigcode/the-stack", "dataset_url": "https://huggingface.co/datasets/bigcode/the-stack", "stage": "pretraining", "nature": "real", "content_types": "code", "tokens": null, "description": "The Stack contains over 6TB of permissively-licensed source code files covering 358 programming languages, serving as a pre-training dataset for Code LLMs.", "author": "bigcode", "hf_id": "bigcode/the-stack", "downloads": 20470, "likes": 899, "license": ["other"], "languages": ["code"], "task_categories": ["text-generation"], "created_at": "2022-10-03T03:34:54+00:00", "last_modified": "2023-04-13T12:15:50+00:00", "citation": null}
{"dataset_id": "bigcode/the-stack-dedup", "dataset_url": "https://huggingface.co/datasets/bigcode/the-stack-dedup", "stage": "pretraining", "nature": "real", "content_types": "code", "tokens": null, "description": "The Stack contains over 6TB of permissively-licensed source code files covering 358 programming languages, serving as a pre-training dataset for Code LLMs.", "author": "bigcode", "hf_id": "bigcode/the-stack-dedup", "downloads": 12948, "likes": 375, "license": ["other"], "languages": ["code"], "task_categories": ["text-generation"], "created_at": "2022-10-06T17:49:19+00:00", "last_modified": "2023-08-17T08:21:58+00:00", "citation": null}
{"dataset_id": "bigcode/the-stack-v2", "dataset_url": "https://huggingface.co/datasets/bigcode/the-stack-v2", "stage": "pretraining", "nature": "real", "content_types": "code", "tokens": 900000000000, "description": "The Stack v2 contains over 3B files in 600+ programming and markup languages, serving as a pre-training dataset for Code LLMs derived from the Software Heritage archive.", "author": "bigcode", "hf_id": "bigcode/the-stack-v2", "downloads": 7645, "likes": 437, "license": ["other"], "languages": ["code"], "task_categories": ["text-generation"], "created_at": "2024-02-26T04:26:48+00:00", "last_modified": "2024-04-23T15:52:32+00:00", "citation": null}
{"dataset_id": "bigcode/the-stack-v2-dedup", "dataset_url": "https://huggingface.co/datasets/bigcode/the-stack-v2-dedup", "stage": "pretraining", "nature": "real", "content_types": "code", "tokens": 900000000000, "description": "The Stack v2 is a dataset containing over 3B files in 600+ programming and markup languages, derived from the Software Heritage archive. It serves as a pre-training dataset for Code LLMs.", "author": "bigcode", "hf_id": "bigcode/the-stack-v2-dedup", "downloads": 3983, "likes": 109, "license": ["other"], "languages": ["code"], "task_categories": ["text-generation"], "created_at": "2024-02-26T09:58:00+00:00", "last_modified": "2024-04-23T16:03:22+00:00", "citation": null}
{"dataset_id": "bigcode/the-stack-v2-train-full-ids", "dataset_url": "https://huggingface.co/datasets/bigcode/the-stack-v2-train-full-ids", "stage": "pretraining", "nature": "real", "content_types": "code", "tokens": null, "description": "The Stack v2 contains over 3B files in 600+ programming and markup languages, serving as a pre-training dataset for Code LLMs derived from the Software Heritage archive.", "author": "bigcode", "hf_id": "bigcode/the-stack-v2-train-full-ids", "downloads": 2158, "likes": 56, "license": ["other"], "languages": ["code"], "task_categories": ["text-generation"], "created_at": "2024-02-25T13:41:16+00:00", "last_modified": "2024-06-06T11:51:36+00:00", "citation": null}
{"dataset_id": "bigcode/the-stack-v2-train-smol-ids", "dataset_url": "https://huggingface.co/datasets/bigcode/the-stack-v2-train-smol-ids", "stage": "pretraining", "nature": "real", "content_types": "code", "tokens": null, "description": "The Stack v2 contains over 3B files in 600+ programming and markup languages, serving as a pre-training dataset for Code LLMs derived from the Software Heritage archive.", "author": "bigcode", "hf_id": "bigcode/the-stack-v2-train-smol-ids", "downloads": 1705, "likes": 44, "license": ["other"], "languages": ["code"], "task_categories": ["text-generation"], "created_at": "2024-02-27T11:49:09+00:00", "last_modified": "2024-04-23T16:03:46+00:00", "citation": null}
{"dataset_id": "BytedTsinghua-SIA/DAPO-Math-17k", "dataset_url": "https://huggingface.co/datasets/BytedTsinghua-SIA/DAPO-Math-17k", "stage": "post-training", "nature": "real", "content_types": "math", "tokens": null, "description": "DAPO-Math-17k is a dataset designed for mathematical problem solving, containing 17,000 problems and their solutions.", "author": "BytedTsinghua-SIA", "hf_id": "BytedTsinghua-SIA/DAPO-Math-17k", "downloads": 6933, "likes": 132, "license": "apache-2.0", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2025-03-17T09:44:12+00:00", "last_modified": "2025-04-18T11:20:51+00:00", "citation": null}
{"dataset_id": "camel-ai/biology", "dataset_url": "https://huggingface.co/datasets/camel-ai/biology", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following, other", "tokens": null, "description": "The dataset consists of 20K problem-solution pairs generated using gpt-4 across 25 biology topics and their subtopics, structured for research purposes.", "author": "camel-ai", "hf_id": "camel-ai/biology", "downloads": 809, "likes": 55, "license": "cc-by-nc-4.0", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2023-04-16T01:30:03+00:00", "last_modified": "2023-05-23T21:11:56+00:00", "citation": null}
{"dataset_id": "camel-ai/chemistry", "dataset_url": "https://huggingface.co/datasets/camel-ai/chemistry", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following", "tokens": null, "description": "The dataset consists of 20K problem-solution pairs generated using gpt-4 across 25 chemistry topics and their subtopics.", "author": "camel-ai", "hf_id": "camel-ai/chemistry", "downloads": 697, "likes": 62, "license": "cc-by-nc-4.0", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2023-04-16T01:30:56+00:00", "last_modified": "2023-05-23T21:12:52+00:00", "citation": null}
{"dataset_id": "camel-ai/physics", "dataset_url": "https://huggingface.co/datasets/camel-ai/physics", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following", "tokens": null, "description": "Physics dataset composed of 20K problem-solution pairs generated using gpt-4, covering 25 physics topics with 25 subtopics each and 32 problems per topic-subtopic pair.", "author": "camel-ai", "hf_id": "camel-ai/physics", "downloads": 1022, "likes": 96, "license": "cc-by-nc-4.0", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2023-04-11T22:49:01+00:00", "last_modified": "2023-05-23T21:12:11+00:00", "citation": null}
{"dataset_id": "cerebras/SlimPajama-627B", "dataset_url": "https://huggingface.co/datasets/cerebras/SlimPajama-627B", "stage": "pretraining", "nature": "real", "content_types": "books, code, math, other, web, wikipedia", "tokens": 627000000000, "description": "SlimPajama is a cleaned and deduplicated version of the RedPajama dataset, containing 627B tokens and consisting of 59166 jsonl files.", "author": "cerebras", "hf_id": "cerebras/SlimPajama-627B", "downloads": 57321, "likes": 510, "license": null, "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2023-06-07T18:45:02+00:00", "last_modified": "2023-07-07T23:13:12+00:00", "citation": null}
{"dataset_id": "chargoddard/commitpack-ft-instruct-rated", "dataset_url": "https://huggingface.co/datasets/chargoddard/commitpack-ft-instruct-rated", "stage": "post-training", "nature": "real", "content_types": "code, instruction-following", "tokens": null, "description": "This dataset is derived from Octocode's CommitPackFT, augmented with a quality analysis of instruction-response pairs by a local model to identify pairs lacking sufficient context or mismatched commit messages.", "author": "chargoddard", "hf_id": "chargoddard/commitpack-ft-instruct-rated", "downloads": 424, "likes": 4, "license": null, "languages": ["en"], "task_categories": null, "created_at": "2023-08-23T04:23:34+00:00", "last_modified": "2023-08-23T09:05:10+00:00", "citation": null}
{"dataset_id": "ClusterlabAi/101_billion_arabic_words_dataset", "dataset_url": "https://huggingface.co/datasets/ClusterlabAi/101_billion_arabic_words_dataset", "stage": "pretraining", "nature": "real", "content_types": "web", "tokens": null, "description": "The dataset consists of 101 billion words extracted and cleaned from web content, specifically targeting Arabic text, intended for training and fine-tuning Large Language Models (LLMs) capable of understanding and generating Arabic text.", "author": "ClusterlabAi", "hf_id": "ClusterlabAi/101_billion_arabic_words_dataset", "downloads": 1296, "likes": 68, "license": "apache-2.0", "languages": ["ar"], "task_categories": ["text-generation"], "created_at": "2024-04-05T21:13:30+00:00", "last_modified": "2024-06-16T18:57:47+00:00", "citation": null}
{"dataset_id": "codeparrot/apps", "dataset_url": "https://huggingface.co/datasets/codeparrot/apps", "stage": "post-training", "nature": "real", "content_types": "code, instruction-following", "tokens": null, "description": "APPS is a benchmark for code generation with 10000 problems, used to evaluate language models' ability to generate code from natural language specifications.", "author": "codeparrot", "hf_id": "codeparrot/apps", "downloads": 16053, "likes": 189, "license": ["mit"], "languages": ["code"], "task_categories": ["text-generation"], "created_at": "2022-06-15T13:20:26+00:00", "last_modified": "2022-10-20T15:00:15+00:00", "citation": null}
{"dataset_id": "cognitivecomputations/dolphin-coder", "dataset_url": "https://huggingface.co/datasets/cognitivecomputations/dolphin-coder", "stage": "post-training", "nature": "real", "content_types": "code", "tokens": null, "description": "This dataset is transformed from a Kaggle dataset and is used to train the dolphin-coder model.", "author": "QuixiAI", "hf_id": "QuixiAI/dolphin-coder", "downloads": 539, "likes": 58, "license": "apache-2.0", "languages": ["en"], "task_categories": null, "created_at": "2023-12-07T05:04:39+00:00", "last_modified": "2023-12-07T06:46:14+00:00", "citation": null}
{"dataset_id": "CohereForAI/aya_dataset", "dataset_url": "https://huggingface.co/datasets/CohereForAI/aya_dataset", "stage": "post-training", "nature": "real", "content_types": "instruction-following", "tokens": null, "description": "The Aya Dataset is a multilingual instruction fine-tuning dataset containing 204k human-annotated prompt-completion pairs across 65 languages, curated by an open-science community via the Aya Annotation Platform from Cohere Labs.", "author": "CohereLabs", "hf_id": "CohereLabs/aya_dataset", "downloads": 4172, "likes": 331, "license": "apache-2.0", "languages": ["amh", "arb", "ary", "ars", "acq", "arz", "apc", "ben", "ceb", "dan", "deu", "ell", "eng", "eus", "fil", "fin", "fra", "gle", "guj", "hat", "hau", "hin", "hun", "ibo", "ind", "ita", "jav", "jpn", "kan", "kir", "kor", "kur", "lit", "mal", "mar", "mlg", "msa", "mya", "nep", "nld", "nso", "nya", "pan", "pes", "pol", "por", "pus", "rus", "sin", "sna", "snd", "som", "spa", "sqi", "srp", "sun", "swa", "swe", "tam", "tel", "tha", "tur", "ukr", "urd", "vie", "wol", "xho", "yor", "zho", "zul"], "task_categories": ["other"], "created_at": "2024-01-31T21:40:16+00:00", "last_modified": "2025-04-15T08:51:55+00:00", "citation": null}
{"dataset_id": "CohereLabs/aya_dataset", "dataset_url": "https://huggingface.co/datasets/CohereLabs/aya_dataset", "stage": "post-training", "nature": "real", "content_types": "instruction-following", "tokens": null, "description": "The Aya Dataset is a multilingual instruction fine-tuning dataset containing 204k human-annotated prompt-completion pairs across 65 languages, curated by an open-science community via the Aya Annotation Platform.", "author": "CohereLabs", "hf_id": "CohereLabs/aya_dataset", "downloads": 4172, "likes": 331, "license": "apache-2.0", "languages": ["amh", "arb", "ary", "ars", "acq", "arz", "apc", "ben", "ceb", "dan", "deu", "ell", "eng", "eus", "fil", "fin", "fra", "gle", "guj", "hat", "hau", "hin", "hun", "ibo", "ind", "ita", "jav", "jpn", "kan", "kir", "kor", "kur", "lit", "mal", "mar", "mlg", "msa", "mya", "nep", "nld", "nso", "nya", "pan", "pes", "pol", "por", "pus", "rus", "sin", "sna", "snd", "som", "spa", "sqi", "srp", "sun", "swa", "swe", "tam", "tel", "tha", "tur", "ukr", "urd", "vie", "wol", "xho", "yor", "zho", "zul"], "task_categories": ["other"], "created_at": "2024-01-31T21:40:16+00:00", "last_modified": "2025-04-15T08:51:55+00:00", "citation": null}
{"dataset_id": "coseal/CodeUltraFeedback_binarized", "dataset_url": "https://huggingface.co/datasets/coseal/CodeUltraFeedback_binarized", "stage": "post-training", "nature": "synthetic", "content_types": "code, instruction-following", "tokens": null, "description": "Instructions coming soon", "author": "coseal", "hf_id": "coseal/CodeUltraFeedback_binarized", "downloads": 36, "likes": 17, "license": "mit", "languages": null, "task_categories": ["text-generation"], "created_at": "2024-03-13T22:31:41+00:00", "last_modified": "2024-03-18T15:59:56+00:00", "citation": null}
{"dataset_id": "croissantllm/croissant_dataset", "dataset_url": "https://huggingface.co/datasets/croissantllm/croissant_dataset", "stage": "pretraining", "nature": "real", "content_types": "other", "tokens": null, "description": "CroissantLLM is a bilingual French-English language model dataset.", "author": "croissantllm", "hf_id": "croissantllm/croissant_dataset", "downloads": 7054, "likes": 7, "license": null, "languages": ["fr", "en"], "task_categories": ["translation", "text-generation", "text2text-generation", "fill-mask"], "created_at": "2024-02-08T09:27:30+00:00", "last_modified": "2024-10-03T14:34:42+00:00", "citation": null}
{"dataset_id": "CyberNative/Code_Vulnerability_Security_DPO", "dataset_url": "https://huggingface.co/datasets/CyberNative/Code_Vulnerability_Security_DPO", "stage": "post-training", "nature": "synthetic", "content_types": "code", "tokens": null, "description": "The dataset contains synthetic pairs of vulnerable and fixed code snippets across multiple programming languages, designed to train AI models to identify and mitigate code vulnerabilities.", "author": "CyberNative", "hf_id": "CyberNative/Code_Vulnerability_Security_DPO", "downloads": 1003, "likes": 134, "license": "apache-2.0", "languages": null, "task_categories": null, "created_at": "2024-02-28T03:14:52+00:00", "last_modified": "2024-02-29T15:24:07+00:00", "citation": null}
{"dataset_id": "deepmind/code_contests", "dataset_url": "https://huggingface.co/datasets/deepmind/code_contests", "stage": "post-training", "nature": "real", "content_types": "code", "tokens": null, "description": "CodeContests is a competitive programming dataset for machine-learning consisting of programming problems from various sources, including test cases and both correct and incorrect human solutions in multiple languages.", "author": "deepmind", "hf_id": "deepmind/code_contests", "downloads": 2838688, "likes": 198, "license": ["cc-by-4.0"], "languages": ["en"], "task_categories": ["translation"], "created_at": "2022-07-19T16:02:55+00:00", "last_modified": "2023-06-11T12:22:30+00:00", "citation": null}
{"dataset_id": "efficientscaling/Z1-Code-Reasoning-107K", "dataset_url": "https://huggingface.co/datasets/efficientscaling/Z1-Code-Reasoning-107K", "stage": "midtraining", "nature": "real", "content_types": "code, reasoning", "tokens": null, "description": "The dataset Z1-Code-Reasoning-107K is designed for training large language models to reason with shifted thinking, specifically focusing on code and reasoning tasks.", "author": "efficientscaling", "hf_id": "efficientscaling/Z1-Code-Reasoning-107K", "downloads": 87, "likes": 18, "license": null, "languages": null, "task_categories": null, "created_at": "2025-04-01T13:07:21+00:00", "last_modified": "2025-04-02T03:42:21+00:00", "citation": null}
{"dataset_id": "EleutherAI/pile", "dataset_url": "https://huggingface.co/datasets/EleutherAI/pile", "stage": "pretraining", "nature": "real", "content_types": "books, code, multilingual, other, web, wikipedia", "tokens": null, "description": "The Pile is a 825 GiB diverse, open source language modelling data set that consists of 22 smaller, high-quality datasets combined together.", "author": "EleutherAI", "hf_id": "EleutherAI/pile", "downloads": 2719, "likes": 471, "license": "other", "languages": ["en"], "task_categories": ["text-generation", "fill-mask"], "created_at": "2022-03-02T23:29:22+00:00", "last_modified": "2023-05-03T15:58:14+00:00", "citation": null}
{"dataset_id": "epfml/FineWeb2-embedded", "dataset_url": "https://huggingface.co/datasets/epfml/FineWeb2-embedded", "stage": "pretraining", "nature": "real", "content_types": "web", "tokens": 512, "description": "FineWeb2-embedded is an extension of the FineWeb2 dataset, annotated with document-level XLM-RoBERTa embeddings for 20 languages, useful for tasks like document clustering and multilingual research.", "author": "epfml", "hf_id": "epfml/FineWeb2-embedded", "downloads": 5382, "likes": 4, "license": "odc-by", "languages": ["ru", "zh", "de", "ja", "es", "fr", "it", "pt", "pl", "nl", "id", "tr", "cs", "vi", "sv", "fa", "ar", "el", "da", "hu"], "task_categories": ["text-generation"], "created_at": "2025-02-17T18:18:25+00:00", "last_modified": "2025-02-19T14:32:51+00:00", "citation": null}
{"dataset_id": "epfml/FineWeb2-HQ", "dataset_url": "https://huggingface.co/datasets/epfml/FineWeb2-HQ", "stage": "pretraining", "nature": "real", "content_types": "web", "tokens": null, "description": "FineWeb2-HQ is a high-quality, model-filtered pretraining dataset derived as a subset of FineWeb2, spanning 20 languages. It enables around 6x faster pretraining compared to the base dataset by selecting the top 10% quality documents of FineWeb2 in each language.", "author": "epfml", "hf_id": "epfml/FineWeb2-HQ", "downloads": 21700, "likes": 39, "license": "odc-by", "languages": ["ru", "zh", "de", "ja", "es", "fr", "it", "pt", "pl", "nl", "id", "tr", "cs", "vi", "sv", "fa", "ar", "el", "da", "hu"], "task_categories": ["text-generation"], "created_at": "2025-02-17T09:55:30+00:00", "last_modified": "2025-02-19T21:39:01+00:00", "citation": null}
{"dataset_id": "EricLu/SCP-116K", "dataset_url": "https://huggingface.co/datasets/EricLu/SCP-116K", "stage": "post-training", "nature": "real", "content_types": "math, qa, reasoning", "tokens": null, "description": "SCP-116K is a large-scale dataset containing 274,166 high-quality scientific problem-solution pairs automatically extracted from web-crawled documents, covering multiple scientific disciplines including physics, chemistry, biology, and mathematics.", "author": "EricLu", "hf_id": "EricLu/SCP-116K", "downloads": 920, "likes": 121, "license": "cc-by-nc-sa-4.0", "languages": ["en"], "task_categories": ["text-generation", "question-answering"], "created_at": "2025-01-26T07:21:44+00:00", "last_modified": "2025-03-17T11:00:15+00:00", "citation": null}
{"dataset_id": "facebook/natural_reasoning", "dataset_url": "https://huggingface.co/datasets/facebook/natural_reasoning", "stage": "post-training", "nature": "real", "content_types": "other, reasoning", "tokens": null, "description": "NaturalReasoning is a large-scale dataset for general reasoning tasks, consisting of high-quality challenging reasoning questions backtranslated from pretraining corpora. It includes reference answers extracted from original documents and model-generated responses.", "author": "facebook", "hf_id": "facebook/natural_reasoning", "downloads": 1403, "likes": 546, "license": "cc-by-nc-4.0", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2025-01-30T23:29:32+00:00", "last_modified": "2025-02-21T06:02:40+00:00", "citation": null}
{"dataset_id": "FreedomIntelligence/medical-o1-reasoning-SFT", "dataset_url": "https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT", "stage": "post-training", "nature": "real", "content_types": "instruction-following, reasoning", "tokens": null, "description": "This dataset is used to fine-tune HuatuoGPT-o1, a medical LLM designed for advanced medical reasoning. It is constructed using GPT-4o, which searches for solutions to verifiable medical problems and validates them through a medical verifier.", "author": "FreedomIntelligence", "hf_id": "FreedomIntelligence/medical-o1-reasoning-SFT", "downloads": 5284, "likes": 1009, "license": "apache-2.0", "languages": ["en", "zh"], "task_categories": ["question-answering", "text-generation"], "created_at": "2024-12-28T03:29:08+00:00", "last_modified": "2025-04-22T15:11:21+00:00", "citation": null}
{"dataset_id": "FreedomIntelligence/medical-o1-verifiable-problem", "dataset_url": "https://huggingface.co/datasets/FreedomIntelligence/medical-o1-verifiable-problem", "stage": "post-training", "nature": "real", "content_types": "reasoning", "tokens": null, "description": "This dataset features open-ended medical problems designed to improve LLMs' medical reasoning, including open-ended questions and ground-truth answers based on challenging medical exams.", "author": "FreedomIntelligence", "hf_id": "FreedomIntelligence/medical-o1-verifiable-problem", "downloads": 652, "likes": 119, "license": "apache-2.0", "languages": ["en"], "task_categories": ["question-answering", "text-generation"], "created_at": "2024-12-28T03:29:26+00:00", "last_modified": "2024-12-30T02:56:46+00:00", "citation": null}
{"dataset_id": "FreedomIntelligence/Medical-R1-Distill-Data", "dataset_url": "https://huggingface.co/datasets/FreedomIntelligence/Medical-R1-Distill-Data", "stage": "post-training", "nature": "mixed", "content_types": "reasoning", "tokens": null, "description": "This dataset is an SFT dataset distilled from Deepseek-R1, based on medical verifiable problems from HuatuoGPT-o1.", "author": "FreedomIntelligence", "hf_id": "FreedomIntelligence/Medical-R1-Distill-Data", "downloads": 529, "likes": 67, "license": "apache-2.0", "languages": ["en", "zh"], "task_categories": ["question-answering", "text-generation"], "created_at": "2025-02-22T03:13:19+00:00", "last_modified": "2025-02-22T06:55:02+00:00", "citation": null}
{"dataset_id": "GAIR/LIMO", "dataset_url": "https://huggingface.co/datasets/GAIR/LIMO", "stage": "midtraining", "nature": "real", "content_types": "reasoning", "tokens": null, "description": "Dataset for LIMO: Less is More for Reasoning.", "author": "GAIR", "hf_id": "GAIR/LIMO", "downloads": 1221, "likes": 175, "license": "apache-2.0", "languages": ["en"], "task_categories": null, "created_at": "2025-02-05T06:43:28+00:00", "last_modified": "2025-02-10T07:42:21+00:00", "citation": null}
{"dataset_id": "glaiveai/glaive-code-assistant-v3", "dataset_url": "https://huggingface.co/datasets/glaiveai/glaive-code-assistant-v3", "stage": "pretraining", "nature": "synthetic", "content_types": "code", "tokens": null, "description": "Glaive-code-assistant-v3 is a dataset of ~1M code problems and solutions generated using Glaives synthetic data generation platform.", "author": "glaiveai", "hf_id": "glaiveai/glaive-code-assistant-v3", "downloads": 464, "likes": 57, "license": "apache-2.0", "languages": null, "task_categories": null, "created_at": "2024-01-03T20:39:51+00:00", "last_modified": "2024-05-20T18:23:03+00:00", "citation": null}
{"dataset_id": "glaiveai/glaive-function-calling-v2", "dataset_url": "https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2", "stage": "post-training", "nature": "real", "content_types": "instruction-following, preference", "tokens": null, "description": "The dataset is designed for training models to perform function calling tasks, containing demonstrations and preferences for function calls.", "author": "glaiveai", "hf_id": "glaiveai/glaive-function-calling-v2", "downloads": 2519, "likes": 478, "license": "apache-2.0", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2023-08-15T19:31:27+00:00", "last_modified": "2023-09-27T18:04:08+00:00", "citation": null}
{"dataset_id": "glaiveai/reasoning-v1-20m", "dataset_url": "https://huggingface.co/datasets/glaiveai/reasoning-v1-20m", "stage": "post-training", "nature": "synthetic", "content_types": "reasoning", "tokens": 35800000000, "description": "A synthetic reasoning dataset containing 22 million general reasoning questions and responses generated using deepseek-ai/DeepSeek-R1-Distill-Llama-70B, covering diverse non code/math topics like social and natural sciences, education, creative writing and general conversations.", "author": "glaiveai", "hf_id": "glaiveai/reasoning-v1-20m", "downloads": 3427, "likes": 219, "license": "apache-2.0", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2025-02-27T20:40:05+00:00", "last_modified": "2025-03-19T13:21:37+00:00", "citation": null}
{"dataset_id": "google/IFEval", "dataset_url": "https://huggingface.co/datasets/google/IFEval", "stage": "post-training", "nature": "real", "content_types": "evaluation, instruction-following", "tokens": null, "description": "This dataset contains prompts used in the Instruction-Following Eval (IFEval) benchmark for large language models, featuring around 500 verifiable instructions that can be checked by heuristics.", "author": "google", "hf_id": "google/IFEval", "downloads": 43928, "likes": 116, "license": "apache-2.0", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2023-12-22T11:20:03+00:00", "last_modified": "2024-08-14T08:21:56+00:00", "citation": null}
{"dataset_id": "Hindi-data-hub/odaigen_hindi_pre_trained_sp", "dataset_url": "https://huggingface.co/datasets/Hindi-data-hub/odaigen_hindi_pre_trained_sp", "stage": "pretraining", "nature": "real", "content_types": "books, multilingual, other, web, wikipedia", "tokens": 1270000000, "description": "This dataset contains various pre-training datasets for Hindi language, including Wikipedia, Dialecthindi, ai4bharat IndicParaphrase, Miracl Corpus, Oscar, and bigscience xP3all, each providing diverse textual data suitable for training large-scale language models.", "author": "Hindi-data-hub", "hf_id": "Hindi-data-hub/odaigen_hindi_pre_trained_sp", "downloads": 141, "likes": 6, "license": "cc-by-nc-4.0", "languages": ["hi"], "task_categories": ["text-classification"], "created_at": "2024-02-26T12:54:31+00:00", "last_modified": "2024-07-16T13:35:36+00:00", "citation": null}
{"dataset_id": "hkust-nlp/CodeIO-PyEdu-Reasoning", "dataset_url": "https://huggingface.co/datasets/hkust-nlp/CodeIO-PyEdu-Reasoning", "stage": "post-training", "nature": "real", "content_types": "code, reasoning", "tokens": null, "description": "The dataset contains code input-output pairs focused on educational Python reasoning tasks, designed to condense reasoning patterns for code models.", "author": "hkust-nlp", "hf_id": "hkust-nlp/CodeIO-PyEdu-Reasoning", "downloads": 119, "likes": 56, "license": "odc-by", "languages": null, "task_categories": null, "created_at": "2025-02-09T09:47:50+00:00", "last_modified": "2025-06-18T06:37:02+00:00", "citation": null}
{"dataset_id": "hkust-nlp/CodeIO-PyEdu-Reasoning-Raw", "dataset_url": "https://huggingface.co/datasets/hkust-nlp/CodeIO-PyEdu-Reasoning-Raw", "stage": "midtraining", "nature": "real", "content_types": "code, reasoning", "tokens": null, "description": "The dataset contains raw data for a processed PythonEdu-Reasoning dataset, focusing on code input-output prediction for reasoning patterns.", "author": "hkust-nlp", "hf_id": "hkust-nlp/CodeIO-PyEdu-Reasoning-Raw", "downloads": 48, "likes": 2, "license": "odc-by", "languages": null, "task_categories": null, "created_at": "2025-02-13T03:44:31+00:00", "last_modified": "2025-06-18T06:37:31+00:00", "citation": null}
{"dataset_id": "huggingface-legal/takedown-notices", "dataset_url": "https://huggingface.co/datasets/huggingface-legal/takedown-notices", "stage": "pretraining", "nature": "real", "content_types": "other", "tokens": null, "description": "The dataset contains takedown notices received by the Hugging Face team.", "author": "huggingface-legal", "hf_id": "huggingface-legal/takedown-notices", "downloads": 1839, "likes": 28, "license": "cc-by-nc-nd-4.0", "languages": null, "task_categories": null, "created_at": "2022-06-28T09:04:19+00:00", "last_modified": "2025-11-12T13:44:22+00:00", "citation": null}
{"dataset_id": "HuggingFaceFW/finepdfs", "dataset_url": "https://huggingface.co/datasets/HuggingFaceFW/finepdfs", "stage": "pretraining", "nature": "real", "content_types": "other", "tokens": 3000000000000, "description": "FinePDFs is the largest publicly available corpus sourced exclusively from PDFs, containing about 3 trillion tokens across 475 million documents in 1733 languages.", "author": "HuggingFaceFW", "hf_id": "HuggingFaceFW/finepdfs", "downloads": 30297, "likes": 689, "license": "odc-by", "languages": ["aai", "aak", "aau", "aaz", "aba", "abi", "abk", "abn", "abq", "abs", "abt", "abx", "aby", "abz", "aca", "acd", "ace", "acf", "ach", "acm", "acn", "acr", "acu", "ada", "ade", "adh", "adi", "adj", "adl", "ady", "adz", "aeb", "aer", "aeu", "aey", "afr", "agd", "agg", "agm", "agn", "agr", "agt", "agu", "agw", "agx", "aha", "ahk", "aia", "aii", "aim", "ain", "ajg", "aji", "ajz", "akb", "ake", "akh", "akp", "alj", "aln", "alp", "alq", "als", "alt", "aly", "alz", "ame", "amf", "amh", "ami", "amk", "amm", "amn", "amp", "amr", "amu", "amx", "ang", "anm", "ann", "anp", "anv", "any", "aoi", "aoj", "aom", "aoz", "apb", "apc", "ape", "apn", "apr", "apt", "apu", "apw", "apy", "apz", "arb", "are", "arg", "arl", "arn", "arp", "arq", "ars", "ary", "arz", "asg", "asm", "aso", "ast", "ata", "atb", "atd", "atg", "ati", "atj", "atq", "att", "auc", "aui", "auy", "ava", "avk", "avn", "avt", "avu", "awa", "awb", "awx", "ayo", "ayp", "ayr", "azb", "azg", "azj", "azz", "bak", "bam", "ban", "bao", "bar", "bas", "bav", "bba", "bbb", "bbc", "bbj", "bbk", "bbo", "bbr", "bch", "bci", "bcl", "bco", "bcw", "bdd", "bdh", "bdq", "bea", "bef", "bel", "bem", "ben", "beq", "bew", "bex", "bfd", "bfo", "bgr", "bgs", "bgt", "bgz", "bhg", "bhl", "bho", "bhp", "bhw", "bhz", "bib", "big", "bim", "bin", "bis", "biu", "biv", "bjn", "bjp", "bjr", "bjv", "bkd", "bkl", "bkq", "bku", "bkv", "bla", "blh", "blk", "blw", "blz", "bmh", "bmk", "bmq", "bmr", "bmu", "bmv", "bno", "bnp", "boa", "bod", "boj", "bom", "bon", "bos", "bov", "box", "bpr", "bps", "bpy", "bqc", "bqj", "bqp", "bre", "brh", "bru", "brx", "bsc", "bsn", "bsp", "bsq", "bss", "btd", "bth", "bts", "btt", "btx", "bud", "bug", "buk", "bul", "bum", "bus", "bvc", "bvd", "bvr", "bvz", "bwd", "bwi", "bwq", "bwu", "bxh", "bxr", "byr", "byv", "byx", "bzd", "bzh", "bzi", "bzj", "caa", "cab", "cac", "caf", "cag", "cak", "cao", "cap", "caq", "car", "cas", "cat", "cav", "cax", "cbc", "cbi", "cbk", "cbr", "cbs", "cbt", "cbu", "cbv", "cce", "cco", "ccp", "ceb", "ceg", "cek", "ces", "cfm", "cgc", "cgg", "cha", "chd", "che", "chf", "chj", "chk", "cho", "chq", "chr", "chu", "chv", "chw", "chz", "cjk", "cjo", "cjp", "cjs", "cjv", "ckb", "cko", "ckt", "cle", "clu", "cly", "cme", "cmn", "cmo", "cmr", "cnh", "cni", "cnk", "cnl", "cnt", "cnw", "coe", "cof", "cok", "con", "cop", "cor", "cos", "cot", "cou", "cpa", "cpb", "cpc", "cpu", "cpy", "crh", "crj", "crk", "crl", "crm", "crn", "crs", "crt", "crx", "csb", "csk", "cso", "csw", "csy", "cta", "ctd", "cto", "ctp", "ctu", "cub", "cuc", "cui", "cuk", "cul", "cut", "cux", "cwe", "cwt", "cya", "cym", "czt", "daa", "dad", "daf", "dag", "dah", "dak", "dan", "dar", "ddg", "ddn", "ded", "des", "deu", "dga", "dgc", "dgi", "dgr", "dgz", "dhg", "dhm", "dhv", "did", "dig", "dik", "diq", "dis", "diu", "div", "dje", "djk", "djr", "dks", "dln", "dng", "dnj", "dnw", "dob", "doi", "dop", "dos", "dow", "drg", "dru", "dsb", "dtb", "dtp", "dts", "dty", "dua", "due", "dug", "duo", "dur", "dwr", "dww", "dyi", "dyo", "dyu", "dzo", "ebk", "efi", "eka", "ekk", "eko", "ell", "emi", "eml", "emp", "enb", "enl", "enm", "enq", "enx", "epo", "eri", "ese", "esi", "esk", "ess", "esu", "eto", "etr", "etu", "eus", "eve", "ewe", "ewo", "ext", "eza", "faa", "fad", "fai", "fal", "fan", "fao", "far", "fas", "fat", "ffm", "fij", "fil", "fin", "fit", "fkv", "fmu", "fon", "for", "fra", "frd", "fro", "frp", "frr", "fry", "fub", "fud", "fue", "fuf", "fuh", "fuq", "fur", "fuv", "gaa", "gag", "gah", "gai", "gam", "gaw", "gaz", "gbi", "gbo", "gbr", "gcf", "gcr", "gde", "gdg", "gdn", "gdr", "geb", "gej", "gfk", "ghs", "gid", "gil", "giz", "gjn", "gkn", "gla", "gle", "glg", "glk", "glv", "gmh", "gmv", "gna", "gnb", "gnd", "gng", "gnn", "gnw", "goa", "gof", "gog", "goh", "gom", "gor", "gos", "got", "gqr", "grc", "grt", "gso", "gsw", "gub", "guc", "gud", "gug", "guh", "gui", "guj", "guk", "gul", "gum", "gun", "guo", "guq", "gur", "guu", "guw", "gux", "guz", "gvc", "gvf", "gvl", "gvn", "gwi", "gwr", "gya", "gym", "gyr", "hac", "hae", "hag", "hak", "hat", "hav", "haw", "hay", "hbo", "hch", "heb", "heg", "heh", "her", "hif", "hig", "hil", "hin", "hix", "hla", "hmo", "hmr", "hne", "hnj", "hnn", "hns", "hop", "hot", "hra", "hrv", "hrx", "hsb", "hto", "hub", "hui", "hun", "hus", "huu", "huv", "hvn", "hwc", "hye", "hyw", "ian", "iba", "ibg", "ibo", "icr", "ido", "idu", "ifa", "ifb", "ife", "ifk", "ifu", "ify", "ige", "ign", "ike", "ikk", "ikt", "ikw", "ilb", "ile", "ilo", "imo", "ina", "inb", "ind", "inh", "ino", "iou", "ipi", "iqw", "iri", "irk", "iry", "isd", "ish", "isl", "iso", "ita", "itv", "ium", "ivb", "ivv", "iws", "ixl", "izr", "izz", "jaa", "jac", "jae", "jam", "jav", "jbo", "jbu", "jic", "jiv", "jmc", "jpn", "jra", "jun", "jvn", "kaa", "kab", "kac", "kak", "kal", "kam", "kan", "kao", "kaq", "kas", "kat", "kaz", "kbc", "kbd", "kbh", "kbm", "kbo", "kbp", "kbq", "kbr", "kby", "kca", "kcg", "kck", "kdc", "kde", "kdh", "kdi", "kdj", "kdl", "kdr", "kea", "kei", "kek", "ken", "keo", "ker", "kew", "kez", "kff", "kgf", "kgk", "kgp", "kgr", "kha", "khk", "khm", "khs", "khz", "kia", "kij", "kik", "kin", "kir", "kiu", "kix", "kjb", "kje", "kjh", "kjs", "kkc", "kki", "kkj", "kkl", "kle", "klt", "klv", "kmb", "kmg", "kmh", "kmk", "kmm", "kmo", "kmr", "kms", "kmu", "kmy", "knc", "kne", "knf", "kng", "knj", "knk", "kno", "knv", "knx", "kny", "kog", "koi", "koo", "kor", "kos", "kpe", "kpf", "kpg", "kpj", "kpq", "kpr", "kpv", "kpw", "kpx", "kpz", "kqc", "kqe", "kqf", "kql", "kqn", "kqo", "kqp", "kqs", "kqw", "kqy", "krc", "kri", "krj", "krl", "kru", "krx", "ksb", "ksc", "ksd", "ksf", "ksh", "ksj", "ksp", "ksr", "kss", "ksw", "ktb", "ktj", "ktm", "kto", "ktu", "ktz", "kua", "kub", "kud", "kue", "kuj", "kum", "kup", "kus", "kvg", "kvj", "kvn", "kwd", "kwf", "kwi", "kwj", "kwn", "kwy", "kxc", "kxm", "kxw", "kyc", "kyf", "kyg", "kyq", "kyu", "kyz", "kze", "kzf", "kzj", "lac", "lad", "lai", "laj", "lam", "lao", "lap", "lat", "lbb", "lbe", "lbj", "lbk", "lcm", "lcp", "ldi", "ldn", "lee", "lef", "leh", "lem", "leu", "lew", "lex", "lez", "lfn", "lgg", "lgl", "lgm", "lhi", "lhu", "lia", "lid", "lif", "lij", "lim", "lin", "lip", "lis", "lit", "liv", "ljp", "lki", "llb", "lld", "llg", "lln", "lmk", "lmo", "lmp", "lnd", "lob", "loe", "log", "lok", "lol", "lom", "loq", "loz", "lrc", "lsi", "lsm", "ltg", "ltz", "lua", "lub", "luc", "lud", "lue", "lug", "lun", "luo", "lus", "lvs", "lwg", "lwo", "lww", "lzh", "maa", "mad", "maf", "mag", "mah", "mai", "maj", "mak", "mal", "mam", "maq", "mar", "mas", "mau", "mav", "maw", "maz", "mbb", "mbc", "mbd", "mbf", "mbh", "mbi", "mbj", "mbl", "mbs", "mbt", "mca", "mcb", "mcd", "mcf", "mck", "mcn", "mco", "mcp", "mcq", "mcu", "mda", "mdf", "mdy", "med", "mee", "mej", "mek", "men", "meq", "mer", "met", "meu", "mev", "mfe", "mfg", "mfh", "mfi", "mfk", "mfq", "mfy", "mfz", "mgc", "mgh", "mgo", "mgr", "mhi", "mhl", "mhr", "mhw", "mhx", "mhy", "mib", "mic", "mie", "mif", "mig", "mih", "mil", "mim", "min", "mio", "mip", "miq", "mir", "mit", "miy", "miz", "mjc", "mjw", "mkd", "mkl", "mkn", "mks", "mkz", "mlh", "mlp", "mlt", "mlu", "mmn", "mmo", "mmx", "mna", "mnb", "mnf", "mni", "mnk", "mns", "mnw", "mnx", "mny", "moa", "moc", "mog", "moh", "mop", "mor", "mos", "mox", "mpg", "mph", "mpm", "mpp", "mps", "mpt", "mpx", "mqb", "mqj", "mqy", "mrg", "mri", "mrj", "mrq", "mrv", "mrw", "msb", "msc", "mse", "msk", "msy", "mta", "mtg", "mti", "mto", "mtp", "mua", "mug", "muh", "mui", "mup", "mur", "mus", "mux", "muy", "mva", "mvn", "mvp", "mwc", "mwf", "mwl", "mwm", "mwn", "mwp", "mwq", "mwv", "mww", "mxb", "mxp", "mxq", "mxt", "mxv", "mya", "myb", "myk", "myu", "myv", "myw", "myx", "myy", "mza", "mzh", "mzk", "mzl", "mzm", "mzn", "mzw", "mzz", "nab", "naf", "nah", "nak", "nap", "naq", "nas", "nav", "naw", "nba", "nbc", "nbe", "nbl", "nbq", "nbu", "nca", "nch", "ncj", "ncl", "ncq", "nct", "ncu", "ncx", "ndc", "nde", "ndh", "ndi", "ndj", "ndo", "nds", "ndz", "neb", "new", "nfa", "nfr", "ngb", "ngc", "ngl", "ngp", "ngu", "nhd", "nhe", "nhg", "nhi", "nhk", "nho", "nhr", "nhu", "nhw", "nhx", "nhy", "nia", "nif", "nii", "nij", "nim", "nin", "nio", "niu", "niy", "njb", "njm", "njn", "njo", "njz", "nkf", "nko", "nld", "nlg", "nma", "nmf", "nmh", "nmo", "nmw", "nmz", "nnb", "nng", "nnh", "nnl", "nno", "nnp", "nnq", "nnw", "noa", "nob", "nod", "nog", "non", "nop", "not", "nou", "nov", "nph", "npi", "npl", "npo", "npy", "nqo", "nre", "nrf", "nri", "nrm", "nsa", "nse", "nsm", "nsn", "nso", "nss", "nst", "nsu", "ntp", "ntr", "ntu", "nuj", "nus", "nuy", "nvm", "nwb", "nwi", "nwx", "nxd", "nya", "nyf", "nyk", "nyn", "nyo", "nyu", "nyy", "nza", "nzi", "nzm", "obo", "oci", "ogo", "ojb", "oke", "oku", "okv", "old", "olo", "omb", "omw", "ong", "ons", "ood", "opm", "orv", "ory", "oss", "ota", "otd", "ote", "otm", "otn", "oto", "otq", "ots", "otw", "oym", "ozm", "pab", "pad", "pag", "pah", "pam", "pan", "pao", "pap", "pau", "pbb", "pbc", "pbi", "pbt", "pcd", "pck", "pcm", "pdc", "pdt", "pem", "pfe", "pfl", "phm", "pib", "pio", "pir", "pis", "pjt", "pkb", "plg", "pls", "plt", "plu", "plw", "pma", "pmf", "pmq", "pms", "pmx", "pnb", "pne", "pnt", "pny", "poe", "poh", "poi", "pol", "pon", "por", "pos", "pot", "pov", "poy", "ppk", "ppo", "pps", "prf", "prg", "pri", "prq", "pse", "pss", "ptp", "ptu", "pui", "pwg", "pwn", "pww", "pxm", "qub", "quc", "quf", "qug", "quh", "qul", "qup", "qus", "quw", "quy", "quz", "qva", "qvc", "qve", "qvh", "qvi", "qvm", "qvn", "qvo", "qvs", "qvw", "qvz", "qwh", "qxh", "qxl", "qxn", "qxo", "qxr", "rad", "rai", "rap", "rar", "rav", "raw", "rcf", "rej", "rel", "rgu", "rhg", "ria", "rim", "rjs", "rkb", "rmc", "rme", "rml", "rmn", "rmo", "rmq", "rmy", "rnd", "rng", "rnl", "roh", "ron", "roo", "rop", "row", "rro", "rtm", "rub", "rue", "ruf", "rug", "run", "rup", "rus", "rwo", "sab", "sag", "sah", "san", "sas", "sat", "sba", "sbd", "sbe", "sbl", "sbs", "sby", "sck", "scn", "sco", "sda", "sdc", "sdh", "sdo", "sdq", "seh", "ses", "sey", "sfw", "sgb", "sgc", "sgh", "sgs", "sgw", "sgz", "shi", "shk", "shn", "shp", "shu", "sid", "sig", "sil", "sim", "sin", "sja", "sjo", "sju", "skg", "skr", "sld", "slk", "sll", "slv", "sma", "sme", "smj", "smk", "sml", "smn", "smo", "sms", "smt", "sna", "snc", "snd", "snf", "snp", "snw", "sny", "soe", "som", "sop", "soq", "sot", "soy", "spa", "spl", "spm", "spp", "sps", "spy", "srd", "sri", "srm", "srn", "srp", "srq", "srr", "ssd", "ssg", "ssw", "ssx", "stn", "stp", "stq", "sua", "suc", "sue", "suk", "sun", "sur", "sus", "suz", "swb", "swc", "swe", "swg", "swh", "swk", "swp", "sxb", "sxn", "syb", "syc", "syl", "szl", "szy", "tab", "tac", "tah", "taj", "tam", "tap", "taq", "tar", "tat", "tav", "taw", "tay", "tbc", "tbg", "tbk", "tbl", "tbo", "tbw", "tby", "tbz", "tca", "tcc", "tcf", "tcs", "tcy", "tcz", "ted", "tee", "tel", "tem", "teo", "ter", "tet", "tew", "tfr", "tgk", "tgo", "tgp", "tha", "thk", "thl", "tif", "tig", "tih", "tik", "tim", "tir", "tiv", "tiy", "tke", "tkl", "tkr", "tku", "tlb", "tlf", "tlh", "tlj", "tll", "tly", "tmc", "tmd", "tna", "tnc", "tnk", "tnn", "tnp", "tnr", "tob", "toc", "tod", "tog", "toh", "toi", "toj", "tok", "ton", "too", "top", "tos", "tpa", "tpi", "tpm", "tpp", "tpt", "tpw", "tpz", "tqo", "trc", "trn", "tro", "trp", "trq", "trs", "trv", "tsc", "tsg", "tsn", "tso", "tsw", "tsz", "ttc", "tte", "ttj", "ttq", "tuc", "tue", "tuf", "tui", "tuk", "tul", "tum", "tuo", "tur", "tuv", "tvk", "tvl", "twi", "twu", "twx", "txq", "txu", "tyv", "tzh", "tzj", "tzl", "tzm", "tzo", "ubr", "ubu", "udm", "udu", "uig", "ukr", "umb", "upv", "ura", "urb", "urd", "urh", "uri", "urk", "urt", "urw", "ury", "usa", "usp", "uth", "uvh", "uvl", "uzn", "uzs", "vag", "vap", "var", "vec", "ven", "vep", "vid", "vie", "viv", "vls", "vmk", "vmw", "vmy", "vol", "vot", "vro", "vun", "vut", "waj", "wal", "wap", "war", "wat", "way", "wba", "wbm", "wbp", "wed", "wer", "wes", "wew", "whg", "whk", "wib", "wim", "wiu", "wln", "wls", "wlv", "wlx", "wmt", "wmw", "wnc", "wnu", "wob", "wol", "wos", "wrk", "wrs", "wsg", "wsk", "wuu", "wuv", "wwa", "xal", "xav", "xbi", "xbr", "xed", "xho", "xla", "xmf", "xmm", "xmv", "xnn", "xog", "xon", "xrb", "xsb", "xsi", "xsm", "xsr", "xsu", "xtd", "xtm", "xtn", "xuo", "yaa", "yad", "yal", "yam", "yan", "yao", "yap", "yaq", "yat", "yaz", "ybb", "yby", "ycn", "ydd", "yim", "yka", "yle", "yli", "yml", "yom", "yon", "yor", "yrb", "yre", "yrk", "yrl", "yss", "yua", "yue", "yuj", "yup", "yut", "yuw", "yuz", "yva", "zaa", "zab", "zac", "zad", "zae", "zai", "zam", "zao", "zar", "zas", "zat", "zav", "zaw", "zca", "zdj", "zea", "zgh", "zia", "ziw", "zne", "zom", "zos", "zpa", "zpc", "zpg", "zpi", "zpj", "zpl", "zpm", "zpo", "zpq", "zpt", "zpu", "zpv", "zpz", "zsm", "zsr", "ztq", "zty", "zul", "zyb", "zyp"], "task_categories": ["text-generation"], "created_at": "2025-09-05T20:08:33+00:00", "last_modified": "2025-12-02T17:49:28+00:00", "citation": null}
{"dataset_id": "HuggingFaceFW/finepdfs-edu", "dataset_url": "https://huggingface.co/datasets/HuggingFaceFW/finepdfs-edu", "stage": "post-training", "nature": "real", "content_types": "books, multilingual", "tokens": null, "description": "FinePDFs-Edu dataset consists of 350B+ tokens of educational PDFs filtered from FinePDFs dataset covering 69 languages.", "author": "HuggingFaceFW", "hf_id": "HuggingFaceFW/finepdfs-edu", "downloads": 6154, "likes": 59, "license": "odc-by", "languages": ["en", "de", "ja", "fr", "es", "it", "ru", "pt", "pl", "nl", "cs", "zh", "ro", "sv", "hu", "sk", "uk", "th", "da", "id", "el", "fi", "ca", "tr", "dag", "hr", "fa", "bg", "nb", "kiu", "ar", "vi", "sr", "ko", "sl", "lt", "hi", "he", "bs", "ms", "et", "lv", "bn", "frp", "is", "glk", "eu", "gl", "sq", "mk", "mr", "ne", "ka", "la", "pcm", "mt", "cy", "vec", "hy", "nrm", "wuu", "anp", "bcc", "ur", "af", "az", "ta", "kk", "nn"], "task_categories": ["text-generation"], "created_at": "2025-11-11T09:36:02+00:00", "last_modified": "2025-11-11T18:49:02+00:00", "citation": null}
{"dataset_id": "HuggingFaceFW/finepdfs_edu", "dataset_url": "https://huggingface.co/datasets/HuggingFaceFW/finepdfs_edu", "stage": "post-training", "nature": "real", "content_types": "books, multilingual", "tokens": null, "description": "FinePDFs-Edu dataset consists of 350B+ tokens of educational PDFs filtered from FinePDFs dataset covering 69 languages.", "author": "HuggingFaceFW", "hf_id": "HuggingFaceFW/finepdfs-edu", "downloads": 6154, "likes": 59, "license": "odc-by", "languages": ["en", "de", "ja", "fr", "es", "it", "ru", "pt", "pl", "nl", "cs", "zh", "ro", "sv", "hu", "sk", "uk", "th", "da", "id", "el", "fi", "ca", "tr", "dag", "hr", "fa", "bg", "nb", "kiu", "ar", "vi", "sr", "ko", "sl", "lt", "hi", "he", "bs", "ms", "et", "lv", "bn", "frp", "is", "glk", "eu", "gl", "sq", "mk", "mr", "ne", "ka", "la", "pcm", "mt", "cy", "vec", "hy", "nrm", "wuu", "anp", "bcc", "ur", "af", "az", "ta", "kk", "nn"], "task_categories": ["text-generation"], "created_at": "2025-11-11T09:36:02+00:00", "last_modified": "2025-11-11T18:49:02+00:00", "citation": null}
{"dataset_id": "HuggingFaceFW/finepdfs_eng_Latn_labeled", "dataset_url": "https://huggingface.co/datasets/HuggingFaceFW/finepdfs_eng_Latn_labeled", "stage": "post-training", "nature": "real", "content_types": "instruction-following, other", "tokens": null, "description": "The dataset consists of fine-tuned PDFs in English using Latin script, labeled for various tasks.", "author": "HuggingFaceFW", "hf_id": "HuggingFaceFW/finepdfs_eng_Latn_labeled", "downloads": 265, "likes": 2, "license": null, "languages": null, "task_categories": null, "created_at": "2025-10-06T20:23:43+00:00", "last_modified": "2025-10-06T20:28:25+00:00", "citation": null}
{"dataset_id": "HuggingFaceFW/fineweb", "dataset_url": "https://huggingface.co/datasets/HuggingFaceFW/fineweb", "stage": "pretraining", "nature": "real", "content_types": "web", "tokens": 18500000000000, "description": "The FineWeb dataset consists of more than 18.5T tokens of cleaned and deduplicated English web data from CommonCrawl, processed for LLM pretraining.", "author": "HuggingFaceFW", "hf_id": "HuggingFaceFW/fineweb", "downloads": 174132, "likes": 2571, "license": "odc-by", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2024-04-18T14:33:13+00:00", "last_modified": "2025-07-11T20:16:53+00:00", "citation": null}
{"dataset_id": "HuggingFaceFW/fineweb-2", "dataset_url": "https://huggingface.co/datasets/HuggingFaceFW/fineweb-2", "stage": "pretraining", "nature": "real", "content_types": "web", "tokens": 30000000000, "description": "This is the second iteration of the popular FineWeb dataset, bringing high quality pretraining data to over 1000 languages. The dataset is fully reproducible, available under the permissive ODC-By 1.0 license and extensively validated through hundreds of ablation experiments.", "author": "HuggingFaceFW", "hf_id": "HuggingFaceFW/fineweb-2", "downloads": 58447, "likes": 707, "license": "odc-by", "languages": ["aai", "aak", "aau", "aaz", "aba", "abi", "abk", "abn", "abq", "abs", "abt", "abx", "aby", "abz", "aca", "acd", "ace", "acf", "ach", "acm", "acn", "acr", "acu", "ada", "ade", "adh", "adi", "adj", "adl", "ady", "adz", "aeb", "aer", "aeu", "aey", "afr", "agd", "agg", "agm", "agn", "agr", "agt", "agu", "agw", "agx", "aha", "ahk", "aia", "aii", "aim", "ain", "ajg", "aji", "ajz", "akb", "ake", "akh", "akp", "alj", "aln", "alp", "alq", "als", "alt", "aly", "alz", "ame", "amf", "amh", "ami", "amk", "amm", "amn", "amp", "amr", "amu", "amx", "ang", "anm", "ann", "anp", "anv", "any", "aoi", "aoj", "aom", "aoz", "apb", "apc", "ape", "apn", "apr", "apt", "apu", "apw", "apy", "apz", "arb", "are", "arg", "arl", "arn", "arp", "arq", "ars", "ary", "arz", "asg", "asm", "aso", "ast", "ata", "atb", "atd", "atg", "ati", "atj", "atq", "att", "auc", "aui", "auy", "ava", "avk", "avn", "avt", "avu", "awa", "awb", "awx", "ayo", "ayp", "ayr", "azb", "azg", "azj", "azz", "bak", "bam", "ban", "bao", "bar", "bas", "bav", "bba", "bbb", "bbc", "bbj", "bbk", "bbo", "bbr", "bch", "bci", "bcl", "bco", "bcw", "bdd", "bdh", "bdq", "bea", "bef", "bel", "bem", "ben", "beq", "bew", "bex", "bfd", "bfo", "bgr", "bgs", "bgt", "bgz", "bhg", "bhl", "bho", "bhp", "bhw", "bhz", "bib", "big", "bim", "bin", "bis", "biu", "biv", "bjn", "bjp", "bjr", "bjv", "bkd", "bkl", "bkq", "bku", "bkv", "bla", "blh", "blk", "blw", "blz", "bmh", "bmk", "bmq", "bmr", "bmu", "bmv", "bno", "bnp", "boa", "bod", "boj", "bom", "bon", "bos", "bov", "box", "bpr", "bps", "bpy", "bqc", "bqj", "bqp", "bre", "brh", "bru", "brx", "bsc", "bsn", "bsp", "bsq", "bss", "btd", "bth", "bts", "btt", "btx", "bud", "bug", "buk", "bul", "bum", "bus", "bvc", "bvd", "bvr", "bvz", "bwd", "bwi", "bwq", "bwu", "bxh", "bxr", "byr", "byv", "byx", "bzd", "bzh", "bzi", "bzj", "caa", "cab", "cac", "caf", "cag", "cak", "cao", "cap", "caq", "car", "cas", "cat", "cav", "cax", "cbc", "cbi", "cbk", "cbr", "cbs", "cbt", "cbu", "cbv", "cce", "cco", "ccp", "ceb", "ceg", "cek", "ces", "cfm", "cgc", "cgg", "cha", "chd", "che", "chf", "chj", "chk", "cho", "chq", "chr", "chu", "chv", "chw", "chz", "cjk", "cjo", "cjp", "cjs", "cjv", "ckb", "cko", "ckt", "cle", "clu", "cly", "cme", "cmn", "cmo", "cmr", "cnh", "cni", "cnk", "cnl", "cnt", "cnw", "coe", "cof", "cok", "con", "cop", "cor", "cos", "cot", "cou", "cpa", "cpb", "cpc", "cpu", "cpy", "crh", "crj", "crk", "crl", "crm", "crn", "crs", "crt", "crx", "csb", "csk", "cso", "csw", "csy", "cta", "ctd", "cto", "ctp", "ctu", "cub", "cuc", "cui", "cuk", "cul", "cut", "cux", "cwe", "cwt", "cya", "cym", "czt", "daa", "dad", "daf", "dag", "dah", "dak", "dan", "dar", "ddg", "ddn", "ded", "des", "deu", "dga", "dgc", "dgi", "dgr", "dgz", "dhg", "dhm", "dhv", "did", "dig", "dik", "diq", "dis", "diu", "div", "dje", "djk", "djr", "dks", "dln", "dng", "dnj", "dnw", "dob", "doi", "dop", "dos", "dow", "drg", "dru", "dsb", "dtb", "dtp", "dts", "dty", "dua", "due", "dug", "duo", "dur", "dwr", "dww", "dyi", "dyo", "dyu", "dzo", "ebk", "efi", "eka", "ekk", "eko", "ell", "emi", "eml", "emp", "enb", "enl", "enm", "enq", "enx", "epo", "eri", "ese", "esi", "esk", "ess", "esu", "eto", "etr", "etu", "eus", "eve", "ewe", "ewo", "ext", "eza", "faa", "fad", "fai", "fal", "fan", "fao", "far", "fas", "fat", "ffm", "fij", "fil", "fin", "fit", "fkv", "fmu", "fon", "for", "fra", "frd", "fro", "frp", "frr", "fry", "fub", "fud", "fue", "fuf", "fuh", "fuq", "fur", "fuv", "gaa", "gag", "gah", "gai", "gam", "gaw", "gaz", "gbi", "gbo", "gbr", "gcf", "gcr", "gde", "gdg", "gdn", "gdr", "geb", "gej", "gfk", "ghs", "gid", "gil", "giz", "gjn", "gkn", "gla", "gle", "glg", "glk", "glv", "gmh", "gmv", "gna", "gnb", "gnd", "gng", "gnn", "gnw", "goa", "gof", "gog", "goh", "gom", "gor", "gos", "got", "gqr", "grc", "grt", "gso", "gsw", "gub", "guc", "gud", "gug", "guh", "gui", "guj", "guk", "gul", "gum", "gun", "guo", "guq", "gur", "guu", "guw", "gux", "guz", "gvc", "gvf", "gvl", "gvn", "gwi", "gwr", "gya", "gym", "gyr", "hac", "hae", "hag", "hak", "hat", "hau", "hav", "haw", "hay", "hbo", "hch", "heb", "heg", "heh", "her", "hif", "hig", "hil", "hin", "hix", "hla", "hmo", "hmr", "hne", "hnj", "hnn", "hns", "hop", "hot", "hra", "hrv", "hrx", "hsb", "hto", "hub", "hui", "hun", "hus", "huu", "huv", "hvn", "hwc", "hye", "hyw", "ian", "iba", "ibg", "ibo", "icr", "ido", "idu", "ifa", "ifb", "ife", "ifk", "ifu", "ify", "ige", "ign", "ike", "ikk", "ikt", "ikw", "ilb", "ile", "ilo", "imo", "ina", "inb", "ind", "inh", "ino", "iou", "ipi", "iqw", "iri", "irk", "iry", "isd", "ish", "isl", "iso", "ita", "itv", "ium", "ivb", "ivv", "iws", "ixl", "izr", "izz", "jaa", "jac", "jae", "jam", "jav", "jbo", "jbu", "jic", "jiv", "jmc", "jpn", "jra", "jun", "jvn", "kaa", "kab", "kac", "kak", "kal", "kam", "kan", "kao", "kaq", "kas", "kat", "kaz", "kbc", "kbd", "kbh", "kbm", "kbo", "kbp", "kbq", "kbr", "kby", "kca", "kcg", "kck", "kdc", "kde", "kdh", "kdi", "kdj", "kdl", "kdr", "kea", "kei", "kek", "ken", "keo", "ker", "kew", "kez", "kff", "kgf", "kgk", "kgp", "kgr", "kha", "khk", "khm", "khs", "khz", "kia", "kij", "kik", "kin", "kir", "kiu", "kix", "kjb", "kje", "kjh", "kjs", "kkc", "kki", "kkj", "kkl", "kle", "klt", "klv", "kmb", "kmg", "kmh", "kmk", "kmm", "kmo", "kmr", "kms", "kmu", "kmy", "knc", "kne", "knf", "kng", "knj", "knk", "kno", "knv", "knx", "kny", "kog", "koi", "koo", "kor", "kos", "kpe", "kpf", "kpg", "kpj", "kpq", "kpr", "kpv", "kpw", "kpx", "kpz", "kqc", "kqe", "kqf", "kql", "kqn", "kqo", "kqp", "kqs", "kqw", "kqy", "krc", "kri", "krj", "krl", "kru", "krx", "ksb", "ksc", "ksd", "ksf", "ksh", "ksj", "ksp", "ksr", "kss", "ksw", "ktb", "ktj", "ktm", "kto", "ktu", "ktz", "kua", "kub", "kud", "kue", "kuj", "kum", "kup", "kus", "kvg", "kvj", "kvn", "kwd", "kwf", "kwi", "kwj", "kwn", "kwy", "kxc", "kxm", "kxw", "kyc", "kyf", "kyg", "kyq", "kyu", "kyz", "kze", "kzf", "kzj", "lac", "lad", "lai", "laj", "lam", "lao", "lap", "lat", "lbb", "lbe", "lbj", "lbk", "lcm", "lcp", "ldi", "ldn", "lee", "lef", "leh", "lem", "leu", "lew", "lex", "lez", "lfn", "lgg", "lgl", "lgm", "lhi", "lhu", "lia", "lid", "lif", "lij", "lim", "lin", "lip", "lis", "lit", "liv", "ljp", "lki", "llb", "lld", "llg", "lln", "lmk", "lmo", "lmp", "lnd", "lob", "loe", "log", "lok", "lol", "lom", "loq", "loz", "lrc", "lsi", "lsm", "ltg", "ltz", "lua", "lub", "luc", "lud", "lue", "lug", "lun", "luo", "lus", "lvs", "lwg", "lwo", "lww", "lzh", "maa", "mad", "maf", "mag", "mah", "mai", "maj", "mak", "mal", "mam", "maq", "mar", "mas", "mau", "mav", "maw", "maz", "mbb", "mbc", "mbd", "mbf", "mbh", "mbi", "mbj", "mbl", "mbs", "mbt", "mca", "mcb", "mcd", "mcf", "mck", "mcn", "mco", "mcp", "mcq", "mcu", "mda", "mdf", "mdy", "med", "mee", "mej", "mek", "men", "meq", "mer", "met", "meu", "mev", "mfe", "mfg", "mfh", "mfi", "mfk", "mfq", "mfy", "mfz", "mgc", "mgh", "mgo", "mgr", "mhi", "mhl", "mhr", "mhw", "mhx", "mhy", "mib", "mic", "mie", "mif", "mig", "mih", "mil", "mim", "min", "mio", "mip", "miq", "mir", "mit", "miy", "miz", "mjc", "mjw", "mkd", "mkl", "mkn", "mks", "mkz", "mlh", "mlp", "mlt", "mlu", "mmn", "mmo", "mmx", "mna", "mnb", "mnf", "mni", "mnk", "mns", "mnw", "mnx", "mny", "moa", "moc", "mog", "moh", "mop", "mor", "mos", "mox", "mpg", "mph", "mpm", "mpp", "mps", "mpt", "mpx", "mqb", "mqj", "mqy", "mrg", "mri", "mrj", "mrq", "mrv", "mrw", "msb", "msc", "mse", "msk", "msy", "mta", "mtg", "mti", "mto", "mtp", "mua", "mug", "muh", "mui", "mup", "mur", "mus", "mux", "muy", "mva", "mvn", "mvp", "mwc", "mwf", "mwl", "mwm", "mwn", "mwp", "mwq", "mwv", "mww", "mxb", "mxp", "mxq", "mxt", "mxv", "mya", "myb", "myk", "myu", "myv", "myw", "myx", "myy", "mza", "mzh", "mzk", "mzl", "mzm", "mzn", "mzw", "mzz", "nab", "naf", "nah", "nak", "nan", "nap", "naq", "nas", "nav", "naw", "nba", "nbc", "nbe", "nbl", "nbq", "nbu", "nca", "nch", "ncj", "ncl", "ncq", "nct", "ncu", "ncx", "ndc", "nde", "ndh", "ndi", "ndj", "ndo", "nds", "ndz", "neb", "new", "nfa", "nfr", "ngb", "ngc", "ngl", "ngp", "ngu", "nhd", "nhe", "nhg", "nhi", "nhk", "nho", "nhr", "nhu", "nhw", "nhx", "nhy", "nia", "nif", "nii", "nij", "nim", "nin", "nio", "niu", "niy", "njb", "njm", "njn", "njo", "njz", "nkf", "nko", "nld", "nlg", "nma", "nmf", "nmh", "nmo", "nmw", "nmz", "nnb", "nng", "nnh", "nnl", "nno", "nnp", "nnq", "nnw", "noa", "nob", "nod", "nog", "non", "nop", "not", "nou", "nov", "nph", "npi", "npl", "npo", "npy", "nqo", "nre", "nrf", "nri", "nrm", "nsa", "nse", "nsm", "nsn", "nso", "nss", "nst", "nsu", "ntp", "ntr", "ntu", "nuj", "nus", "nuy", "nvm", "nwb", "nwi", "nwx", "nxd", "nya", "nyf", "nyk", "nyn", "nyo", "nyu", "nyy", "nza", "nzi", "nzm", "obo", "oci", "ogo", "ojb", "oke", "oku", "okv", "old", "olo", "omb", "omw", "ong", "ons", "ood", "opm", "orv", "ory", "oss", "ota", "otd", "ote", "otm", "otn", "oto", "otq", "ots", "otw", "oym", "ozm", "pab", "pad", "pag", "pah", "pam", "pan", "pao", "pap", "pau", "pbb", "pbc", "pbi", "pbt", "pcd", "pck", "pcm", "pdc", "pdt", "pem", "pfe", "pfl", "phm", "pib", "pio", "pir", "pis", "pjt", "pkb", "plg", "pls", "plt", "plu", "plw", "pma", "pmf", "pmq", "pms", "pmx", "pnb", "pne", "pnt", "pny", "poe", "poh", "poi", "pol", "pon", "por", "pos", "pot", "pov", "poy", "ppk", "ppo", "pps", "prf", "prg", "pri", "prq", "pse", "pss", "ptp", "ptu", "pui", "pwg", "pwn", "pww", "pxm", "qub", "quc", "quf", "qug", "quh", "qul", "qup", "qus", "quw", "quy", "quz", "qva", "qvc", "qve", "qvh", "qvi", "qvm", "qvn", "qvo", "qvs", "qvw", "qvz", "qwh", "qxh", "qxl", "qxn", "qxo", "qxr", "rad", "rai", "rap", "rar", "rav", "raw", "rcf", "rej", "rel", "rgu", "rhg", "ria", "rim", "rjs", "rkb", "rmc", "rme", "rml", "rmn", "rmo", "rmq", "rmy", "rnd", "rng", "rnl", "roh", "ron", "roo", "rop", "row", "rro", "rtm", "rub", "rue", "ruf", "rug", "run", "rup", "rus", "rwo", "sab", "sag", "sah", "san", "sas", "sat", "sba", "sbd", "sbe", "sbl", "sbs", "sby", "sck", "scn", "sco", "sda", "sdc", "sdh", "sdo", "sdq", "seh", "ses", "sey", "sfw", "sgb", "sgc", "sgh", "sgs", "sgw", "sgz", "shi", "shk", "shn", "shp", "shu", "sid", "sig", "sil", "sim", "sin", "sja", "sjo", "sju", "skg", "skr", "sld", "slk", "sll", "slv", "sma", "sme", "smj", "smk", "sml", "smn", "smo", "sms", "smt", "sna", "snc", "snd", "snf", "snn", "snp", "snw", "sny", "soe", "som", "sop", "soq", "sot", "soy", "spa", "spl", "spm", "spp", "sps", "spy", "srd", "sri", "srm", "srn", "srp", "srq", "srr", "ssd", "ssg", "ssw", "ssx", "stn", "stp", "stq", "sua", "suc", "sue", "suk", "sun", "sur", "sus", "suz", "swb", "swc", "swe", "swg", "swh", "swk", "swp", "sxb", "sxn", "syb", "syc", "syl", "szl", "szy", "tab", "tac", "tah", "taj", "tam", "tap", "taq", "tar", "tat", "tav", "taw", "tay", "tbc", "tbg", "tbk", "tbl", "tbo", "tbw", "tby", "tbz", "tca", "tcc", "tcf", "tcs", "tcy", "tcz", "ted", "tee", "tel", "tem", "teo", "ter", "tet", "tew", "tfr", "tgk", "tgo", "tgp", "tha", "thk", "thl", "tif", "tig", "tih", "tik", "tim", "tir", "tiv", "tiy", "tke", "tkl", "tkr", "tku", "tlb", "tlf", "tlh", "tlj", "tll", "tly", "tmc", "tmd", "tna", "tnc", "tnk", "tnn", "tnp", "tnr", "tob", "toc", "tod", "tog", "toh", "toi", "toj", "tok", "ton", "too", "top", "tos", "tpa", "tpi", "tpm", "tpp", "tpt", "tpw", "tpz", "tqo", "trc", "trn", "tro", "trp", "trq", "trs", "trv", "tsc", "tsg", "tsn", "tso", "tsw", "tsz", "ttc", "tte", "ttj", "ttq", "tuc", "tue", "tuf", "tui", "tuk", "tul", "tum", "tuo", "tur", "tuv", "tvk", "tvl", "twi", "twu", "twx", "txq", "txu", "tyv", "tzh", "tzj", "tzl", "tzm", "tzo", "ubr", "ubu", "udm", "udu", "uig", "ukr", "umb", "upv", "ura", "urb", "urd", "urh", "uri", "urk", "urt", "urw", "ury", "usa", "usp", "uth", "uvh", "uvl", "uzn", "uzs", "vag", "vap", "var", "vec", "ven", "vep", "vid", "vie", "viv", "vls", "vmk", "vmw", "vmy", "vol", "vot", "vro", "vun", "vut", "waj", "wal", "wap", "war", "wat", "way", "wba", "wbm", "wbp", "wed", "wer", "wes", "wew", "whg", "whk", "wib", "wim", "wiu", "wln", "wls", "wlv", "wlx", "wmt", "wmw", "wnc", "wnu", "wob", "wol", "wos", "wrk", "wrs", "wsg", "wsk", "wuu", "wuv", "wwa", "xal", "xav", "xbi", "xbr", "xed", "xho", "xla", "xmf", "xmm", "xmv", "xnn", "xog", "xon", "xrb", "xsb", "xsi", "xsm", "xsr", "xsu", "xtd", "xtm", "xtn", "xuo", "yaa", "yad", "yal", "yam", "yan", "yao", "yap", "yaq", "yat", "yaz", "ybb", "yby", "ycn", "ydd", "yim", "yka", "yle", "yli", "yml", "yom", "yon", "yor", "yrb", "yre", "yrk", "yrl", "yss", "yua", "yue", "yuj", "yup", "yut", "yuw", "yuz", "yva", "zaa", "zab", "zac", "zad", "zae", "zai", "zam", "zao", "zar", "zas", "zat", "zav", "zaw", "zca", "zdj", "zea", "zgh", "zia", "ziw", "zne", "zom", "zos", "zpa", "zpc", "zpg", "zpi", "zpj", "zpl", "zpm", "zpo", "zpq", "zpt", "zpu", "zpv", "zpz", "zsm", "zsr", "ztq", "zty", "zul", "zyb", "zyp"], "task_categories": ["text-generation"], "created_at": "2024-12-05T16:23:59+00:00", "last_modified": "2025-10-27T18:32:07+00:00", "citation": null}
{"dataset_id": "HuggingFaceFW/fineweb-edu", "dataset_url": "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu", "stage": "pretraining", "nature": "real", "content_types": "web", "tokens": 1300000000000, "description": "FineWeb-Edu is a dataset of 1.3 trillion tokens of educational web pages filtered from the FineWeb dataset using an educational quality classifier developed with annotations generated by LLama3-70B-Instruct.", "author": "HuggingFaceFW", "hf_id": "HuggingFaceFW/fineweb-edu", "downloads": 282200, "likes": 886, "license": "odc-by", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2024-05-28T14:32:57+00:00", "last_modified": "2025-07-11T20:16:53+00:00", "citation": null}
{"dataset_id": "HuggingFaceFW/fineweb-edu-score-2", "dataset_url": "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu-score-2", "stage": "pretraining", "nature": "real", "content_types": "web", "tokens": 5400000000000, "description": "FineWeb-Edu-score-2 is a dataset of 5.4 trillion tokens of educational web pages filtered from the FineWeb dataset using an educational quality classifier.", "author": "HuggingFaceFW", "hf_id": "HuggingFaceFW/fineweb-edu-score-2", "downloads": 26208, "likes": 82, "license": "odc-by", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2024-05-28T17:30:16+00:00", "last_modified": "2025-07-11T20:16:52+00:00", "citation": null}
{"dataset_id": "HuggingFaceFW/finewiki", "dataset_url": "https://huggingface.co/datasets/HuggingFaceFW/finewiki", "stage": "pretraining", "nature": "real", "content_types": "multilingual, wikipedia", "tokens": null, "description": "This dataset is an updated and better extracted version of the wikimedia/Wikipedia dataset, parsed from Wikipedia HTML dumps from August 2025 covering 325 languages. It fully renders templates, removes redirects and disambiguation pages, and includes detailed metadata.", "author": "HuggingFaceFW", "hf_id": "HuggingFaceFW/finewiki", "downloads": 9260, "likes": 270, "license": ["cc-by-sa-4.0", "gfdl"], "languages": null, "task_categories": ["text-generation"], "created_at": "2025-10-13T12:22:30+00:00", "last_modified": "2025-10-22T11:02:22+00:00", "citation": null}
{"dataset_id": "HuggingFaceH4/no_robots", "dataset_url": "https://huggingface.co/datasets/HuggingFaceH4/no_robots", "stage": "post-training", "nature": "real", "content_types": "code, conversation, instruction-following, qa", "tokens": null, "description": "No Robots is a high-quality dataset of 10,000 instructions and demonstrations created by skilled human annotators, designed for supervised fine-tuning (SFT) to improve instruction following in language models.", "author": "HuggingFaceH4", "hf_id": "HuggingFaceH4/no_robots", "downloads": 3128, "likes": 516, "license": "cc-by-nc-4.0", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2023-11-10T12:23:22+00:00", "last_modified": "2024-04-18T08:40:39+00:00", "citation": null}
{"dataset_id": "HuggingFaceH4/stack-exchange-preferences", "dataset_url": "https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences", "stage": "post-training", "nature": "real", "content_types": "preference", "tokens": null, "description": "This dataset contains questions and answers from the Stack Overflow Data Dump filtered to have >=2 answers, for the purpose of preference model training.", "author": "HuggingFaceH4", "hf_id": "HuggingFaceH4/stack-exchange-preferences", "downloads": 3605, "likes": 133, "license": "cc-by-sa-4.0", "languages": ["en"], "task_categories": ["question-answering"], "created_at": "2023-02-11T03:24:28+00:00", "last_modified": "2023-03-08T03:37:53+00:00", "citation": null}
{"dataset_id": "HuggingFaceTB/cosmopedia", "dataset_url": "https://huggingface.co/datasets/HuggingFaceTB/cosmopedia", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following, other", "tokens": 25000000000, "description": "Cosmopedia is a dataset of synthetic textbooks, blogposts, stories, posts and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1, containing over 30 million files and 25 billion tokens.", "author": "HuggingFaceTB", "hf_id": "HuggingFaceTB/cosmopedia", "downloads": 48626, "likes": 649, "license": "apache-2.0", "languages": ["en"], "task_categories": null, "created_at": "2024-02-18T20:23:48+00:00", "last_modified": "2024-08-12T22:05:49+00:00", "citation": null}
{"dataset_id": "HuggingFaceTB/cosmopedia-100k", "dataset_url": "https://huggingface.co/datasets/HuggingFaceTB/cosmopedia-100k", "stage": "pretraining", "nature": "synthetic", "content_types": "books, other, web", "tokens": null, "description": "This is a 100k subset of Cosmopedia dataset, a synthetic dataset of textbooks, blogposts, stories, posts and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1.", "author": "HuggingFaceTB", "hf_id": "HuggingFaceTB/cosmopedia-100k", "downloads": 1342, "likes": 45, "license": "apache-2.0", "languages": ["en"], "task_categories": null, "created_at": "2024-02-19T14:13:13+00:00", "last_modified": "2024-02-19T23:41:30+00:00", "citation": null}
{"dataset_id": "HuggingFaceTB/everyday-conversations-llama3.1-2k", "dataset_url": "https://huggingface.co/datasets/HuggingFaceTB/everyday-conversations-llama3.1-2k", "stage": "post-training", "nature": "synthetic", "content_types": "conversation", "tokens": null, "description": "This dataset contains 2.2k multi-turn conversations generated by Llama-3.1-70B-Instruct, covering everyday topics and elementary science with simple exchanges between a User and an AI Assistant.", "author": "HuggingFaceTB", "hf_id": "HuggingFaceTB/everyday-conversations-llama3.1-2k", "downloads": 655, "likes": 122, "license": "apache-2.0", "languages": ["en"], "task_categories": null, "created_at": "2024-08-12T23:54:14+00:00", "last_modified": "2025-01-29T23:16:26+00:00", "citation": null}
{"dataset_id": "HuggingFaceTB/finemath", "dataset_url": "https://huggingface.co/datasets/HuggingFaceTB/finemath", "stage": "pretraining", "nature": "real", "content_types": "math, reasoning, web", "tokens": 34000000000, "description": "FineMath is a dataset of mathematical educational content filtered from CommonCrawl, containing explanations and step-by-step problem solving.", "author": "HuggingFaceTB", "hf_id": "HuggingFaceTB/finemath", "downloads": 11398, "likes": 344, "license": "odc-by", "languages": null, "task_categories": null, "created_at": "2024-11-25T15:23:13+00:00", "last_modified": "2025-02-06T10:31:11+00:00", "citation": null}
{"dataset_id": "HuggingFaceTB/finemath_contamination_report", "dataset_url": "https://huggingface.co/datasets/HuggingFaceTB/finemath_contamination_report", "stage": "post-training", "nature": "real", "content_types": "math, other", "tokens": null, "description": "This dataset contains suspected benchmark-contaminated pages that were removed from the FineMath dataset.", "author": "HuggingFaceTB", "hf_id": "HuggingFaceTB/finemath_contamination_report", "downloads": 67, "likes": 1, "license": null, "languages": null, "task_categories": null, "created_at": "2024-11-28T13:20:06+00:00", "last_modified": "2025-01-07T16:20:41+00:00", "citation": null}
{"dataset_id": "HuggingFaceTB/github-issues-notebooks", "dataset_url": "https://huggingface.co/datasets/HuggingFaceTB/github-issues-notebooks", "stage": "pretraining", "nature": "real", "content_types": "code, conversation", "tokens": 12700000000, "description": "A collection of two code datasets intended for language models training, sourced from GitHub issues and Kaggle notebooks. These datasets are a modified part of the StarCoder2 model training corpus.", "author": "HuggingFaceTB", "hf_id": "HuggingFaceTB/issues-kaggle-notebooks", "downloads": 711, "likes": 13, "license": null, "languages": null, "task_categories": null, "created_at": "2025-03-18T23:08:56+00:00", "last_modified": "2025-03-19T20:00:18+00:00", "citation": null}
{"dataset_id": "HuggingFaceTB/instruct-data-basics-smollm-H4", "dataset_url": "https://huggingface.co/datasets/HuggingFaceTB/instruct-data-basics-smollm-H4", "stage": "post-training", "nature": "real", "content_types": "conversation, instruction-following", "tokens": null, "description": "The dataset contains basic instructions and answers for SmolLM-Instruct models training, including responses to greetings and simple questions like 'Who are you'.", "author": "HuggingFaceTB", "hf_id": "HuggingFaceTB/instruct-data-basics-smollm-H4", "downloads": 211, "likes": 2, "license": null, "languages": null, "task_categories": null, "created_at": "2024-08-06T13:58:46+00:00", "last_modified": "2025-07-24T16:50:26+00:00", "citation": null}
{"dataset_id": "HuggingFaceTB/issues-kaggle-notebooks", "dataset_url": "https://huggingface.co/datasets/HuggingFaceTB/issues-kaggle-notebooks", "stage": "pretraining", "nature": "real", "content_types": "code", "tokens": 12700000000, "description": "A collection of two code datasets intended for language models training, sourced from GitHub issues and Kaggle notebooks. These datasets have undergone filtering to remove low-quality content, duplicates and PII.", "author": "HuggingFaceTB", "hf_id": "HuggingFaceTB/issues-kaggle-notebooks", "downloads": 711, "likes": 13, "license": null, "languages": null, "task_categories": null, "created_at": "2025-03-18T23:08:56+00:00", "last_modified": "2025-03-19T20:00:18+00:00", "citation": null}
{"dataset_id": "HuggingFaceTB/Magpie-Pro-300K-Filtered-H4", "dataset_url": "https://huggingface.co/datasets/HuggingFaceTB/Magpie-Pro-300K-Filtered-H4", "stage": "post-training", "nature": "real", "content_types": "instruction-following", "tokens": null, "description": "The dataset is formatted to be compatible with the alignment-handbook for SFT.", "author": "HuggingFaceTB", "hf_id": "HuggingFaceTB/Magpie-Pro-300K-Filtered-H4", "downloads": 169, "likes": 5, "license": null, "languages": null, "task_categories": null, "created_at": "2024-07-03T21:41:07+00:00", "last_modified": "2024-08-17T22:37:01+00:00", "citation": null}
{"dataset_id": "HuggingFaceTB/OpenHermes-2.5-H4", "dataset_url": "https://huggingface.co/datasets/HuggingFaceTB/OpenHermes-2.5-H4", "stage": "post-training", "nature": "real", "content_types": "instruction-following", "tokens": null, "description": "The dataset is formatted to be compatible with the alignment-handbook for SFT.", "author": "HuggingFaceTB", "hf_id": "HuggingFaceTB/OpenHermes-2.5-H4", "downloads": 243, "likes": 6, "license": null, "languages": null, "task_categories": null, "created_at": "2024-07-03T22:31:03+00:00", "last_modified": "2024-08-17T22:36:05+00:00", "citation": null}
{"dataset_id": "HuggingFaceTB/self-oss-instruct-sc2-H4", "dataset_url": "https://huggingface.co/datasets/HuggingFaceTB/self-oss-instruct-sc2-H4", "stage": "post-training", "nature": "real", "content_types": "instruction-following", "tokens": null, "description": "This dataset is formatted to be compatible with the alignment-handbook for SFT, based on the StarCoder2-Self-Instruct-OSS-50k dataset.", "author": "HuggingFaceTB", "hf_id": "HuggingFaceTB/self-oss-instruct-sc2-H4", "downloads": 61, "likes": 4, "license": null, "languages": null, "task_categories": null, "created_at": "2024-07-03T22:43:13+00:00", "last_modified": "2024-08-17T22:38:04+00:00", "citation": null}
{"dataset_id": "HuggingFaceTB/smollm-corpus", "dataset_url": "https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus", "stage": "pretraining", "nature": "mixed", "content_types": "code, other, web", "tokens": 220000000000, "description": "A curated collection of high-quality educational and synthetic data designed for training small language models, including synthetic textbooks, blog posts, stories, Python educational code, and educational web pages.", "author": "HuggingFaceTB", "hf_id": "HuggingFaceTB/smollm-corpus", "downloads": 15352, "likes": 407, "license": "odc-by", "languages": ["en"], "task_categories": null, "created_at": "2024-07-15T13:51:48+00:00", "last_modified": "2024-09-06T07:04:57+00:00", "citation": null}
{"dataset_id": "HuggingFaceTB/smoltalk", "dataset_url": "https://huggingface.co/datasets/HuggingFaceTB/smoltalk", "stage": "post-training", "nature": "mixed", "content_types": "code, conversation, instruction-following, math, other, reasoning", "tokens": 2048, "description": "This is a synthetic dataset designed for supervised finetuning (SFT) of LLMs, containing 1M samples used to build the SmolLM2-Instruct family of models. It covers diverse tasks including text editing, rewriting, summarization, and reasoning.", "author": "HuggingFaceTB", "hf_id": "HuggingFaceTB/smoltalk", "downloads": 6933, "likes": 385, "license": null, "languages": ["en"], "task_categories": null, "created_at": "2024-11-17T15:52:41+00:00", "last_modified": "2025-02-10T16:36:16+00:00", "citation": null}
{"dataset_id": "HuggingFaceTB/smoltalk2", "dataset_url": "https://huggingface.co/datasets/HuggingFaceTB/smoltalk2", "stage": "post-training", "nature": "real", "content_types": "conversation, instruction-following, preference, reasoning, tool-use", "tokens": 35172100000, "description": "This dataset contains three subsets (Mid, SFT, Preference) that correspond to the three phases of Post-Training for SmolLM3-3B.", "author": "HuggingFaceTB", "hf_id": "HuggingFaceTB/smoltalk2", "downloads": 7219, "likes": 132, "license": null, "languages": null, "task_categories": null, "created_at": "2025-07-10T13:42:16+00:00", "last_modified": "2025-10-31T13:49:55+00:00", "citation": null}
{"dataset_id": "HuggingFaceTB/stack-edu", "dataset_url": "https://huggingface.co/datasets/HuggingFaceTB/stack-edu", "stage": "pretraining", "nature": "real", "content_types": "code", "tokens": 125000000000, "description": "Stack-Edu is a 125B token dataset of educational code filtered from The Stack v2, intended for Language Models training.", "author": "HuggingFaceTB", "hf_id": "HuggingFaceTB/stack-edu", "downloads": 1978, "likes": 60, "license": null, "languages": null, "task_categories": null, "created_at": "2025-03-18T12:30:40+00:00", "last_modified": "2025-03-20T13:51:54+00:00", "citation": null}
{"dataset_id": "iamtarun/code_instructions_120k_alpaca", "dataset_url": "https://huggingface.co/datasets/iamtarun/code_instructions_120k_alpaca", "stage": "post-training", "nature": "synthetic", "content_types": "code, instruction-following", "tokens": null, "description": "This dataset adds a prompt column in alpaca style to code instructions.", "author": "iamtarun", "hf_id": "iamtarun/code_instructions_120k_alpaca", "downloads": 458, "likes": 61, "license": null, "languages": null, "task_categories": ["text-generation", "question-answering", "text2text-generation"], "created_at": "2023-07-23T17:34:03+00:00", "last_modified": "2023-07-27T15:49:10+00:00", "citation": null}
{"dataset_id": "Infi-MM/InfiMM-WebMath-40B", "dataset_url": "https://huggingface.co/datasets/Infi-MM/InfiMM-WebMath-40B", "stage": "pretraining", "nature": "real", "content_types": "math, multilingual, vision, web", "tokens": 40000000000, "description": "InfiMM-WebMath-40B is a large-scale, open-source multimodal dataset designed for mathematical reasoning tasks, incorporating both text and images extracted from web documents to advance the pre-training of Multimodal Large Language Models.", "author": "Infi-MM", "hf_id": "Infi-MM/InfiMM-WebMath-40B", "downloads": 1503, "likes": 68, "license": "odc-by", "languages": ["en", "zh"], "task_categories": ["image-text-to-text"], "created_at": "2024-09-12T22:26:18+00:00", "last_modified": "2025-07-26T03:39:30+00:00", "citation": null}
{"dataset_id": "INK-USC/riddle_sense", "dataset_url": "https://huggingface.co/datasets/INK-USC/riddle_sense", "stage": "post-training", "nature": "real", "content_types": "qa", "tokens": null, "description": "RiddleSense is a dataset for answering riddle-style commonsense questions, containing 5.7k examples designed to test complex commonsense reasoning, understanding of figurative language, and counterfactual reasoning skills.", "author": "INK-USC", "hf_id": "INK-USC/riddle_sense", "downloads": 1274, "likes": 26, "license": ["other"], "languages": ["en"], "task_categories": ["question-answering"], "created_at": "2022-03-02T23:29:22+00:00", "last_modified": "2024-01-18T11:14:43+00:00", "citation": null}
{"dataset_id": "Intelligent-Internet/II-Thought-RL-v0", "dataset_url": "https://huggingface.co/datasets/Intelligent-Internet/II-Thought-RL-v0", "stage": "post-training", "nature": "real", "content_types": "code, math, other, reasoning", "tokens": null, "description": "II-Thought RL v0 is a large-scale, multi-task dataset designed for Reinforcement Learning, consisting of high-quality question-answer pairs across domains like mathematics, code, science, and medical problems.", "author": "Intelligent-Internet", "hf_id": "Intelligent-Internet/II-Thought-RL-v0", "downloads": 204, "likes": 54, "license": null, "languages": null, "task_categories": null, "created_at": "2025-03-24T10:32:37+00:00", "last_modified": "2025-03-28T15:26:57+00:00", "citation": null}
{"dataset_id": "ise-uiuc/Magicoder-Evol-Instruct-110K", "dataset_url": "https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K", "stage": "post-training", "nature": "real", "content_types": "code", "tokens": null, "description": "A decontaminated version of evol-codealpaca-v1, processed using the same decontamination method as StarCoder.", "author": "ise-uiuc", "hf_id": "ise-uiuc/Magicoder-Evol-Instruct-110K", "downloads": 2625, "likes": 170, "license": "apache-2.0", "languages": null, "task_categories": ["text-generation", "conversational"], "created_at": "2023-12-03T20:05:56+00:00", "last_modified": "2023-12-28T03:23:17+00:00", "citation": null}
{"dataset_id": "jondurbin/bagel-v0.5", "dataset_url": "https://huggingface.co/datasets/jondurbin/bagel-v0.5", "stage": "post-training", "nature": "synthetic", "content_types": "preference", "tokens": null, "description": "BAGEL is a synthetic preference dataset designed to enhance alignment in language models through preference ranking of model-generated responses.", "author": "jondurbin", "hf_id": "jondurbin/bagel-v0.5", "downloads": 67, "likes": 14, "license": "cc-by-nc-4.0", "languages": null, "task_categories": null, "created_at": "2024-03-31T06:22:04+00:00", "last_modified": "2024-04-11T11:57:36+00:00", "citation": null}
{"dataset_id": "KbsdJames/Omni-MATH", "dataset_url": "https://huggingface.co/datasets/KbsdJames/Omni-MATH", "stage": "post-training", "nature": "real", "content_types": "math", "tokens": null, "description": "Omni-MATH is a benchmark designed to assess LLMs' mathematical reasoning at the Olympiad level, comprising 4428 competition-level problems categorized into 33 sub-domains and spanning 10 difficulty levels.", "author": "KbsdJames", "hf_id": "KbsdJames/Omni-MATH", "downloads": 3932, "likes": 119, "license": "apache-2.0", "languages": ["en"], "task_categories": null, "created_at": "2024-09-14T06:45:45+00:00", "last_modified": "2024-10-12T09:02:05+00:00", "citation": null}
{"dataset_id": "KodCode/KodCode-V1", "dataset_url": "https://huggingface.co/datasets/KodCode/KodCode-V1", "stage": "post-training", "nature": "synthetic", "content_types": "code", "tokens": null, "description": "KodCode is a fully-synthetic open-source dataset providing verifiable solutions and tests for coding tasks across various domains and difficulty levels, designed for supervised fine-tuning (SFT) and RL tuning.", "author": "KodCode", "hf_id": "KodCode/KodCode-V1", "downloads": 2489, "likes": 101, "license": "cc-by-nc-4.0", "languages": ["en"], "task_categories": null, "created_at": "2025-02-28T23:37:53+00:00", "last_modified": "2025-03-17T07:56:27+00:00", "citation": null}
{"dataset_id": "KodCode/KodCode-V1-SFT-4o", "dataset_url": "https://huggingface.co/datasets/KodCode/KodCode-V1-SFT-4o", "stage": "post-training", "nature": "synthetic", "content_types": "code", "tokens": null, "description": "KodCode-V1-SFT-4o is a synthetic dataset for supervised fine-tuning (SFT) of coding models, containing verified solutions and tests generated using gpt-4o for various coding tasks across multiple domains and difficulty levels.", "author": "KodCode", "hf_id": "KodCode/KodCode-V1-SFT-4o", "downloads": 174, "likes": 10, "license": "cc-by-nc-4.0", "languages": ["en"], "task_categories": ["question-answering"], "created_at": "2025-03-13T07:17:15+00:00", "last_modified": "2025-03-16T21:59:33+00:00", "citation": null}
{"dataset_id": "KodCode/KodCode-V1-SFT-R1", "dataset_url": "https://huggingface.co/datasets/KodCode/KodCode-V1-SFT-R1", "stage": "post-training", "nature": "synthetic", "content_types": "code", "tokens": null, "description": "KodCode-V1-SFT-R1 is a synthetic dataset for supervised fine-tuning (SFT) in coding tasks, containing verified solutions and tests generated from coding questions using DeepSeek R1.", "author": "KodCode", "hf_id": "KodCode/KodCode-V1-SFT-R1", "downloads": 937, "likes": 35, "license": "cc-by-nc-4.0", "languages": ["en"], "task_categories": ["question-answering"], "created_at": "2025-03-01T05:04:31+00:00", "last_modified": "2025-03-17T07:57:30+00:00", "citation": null}
{"dataset_id": "Kwai-Klear/KlearReasoner-MathSub-30K", "dataset_url": "https://huggingface.co/datasets/Kwai-Klear/KlearReasoner-MathSub-30K", "stage": "post-training", "nature": "real", "content_types": "math, reasoning", "tokens": null, "description": "This dataset is a 30K-entry subset of the Klear-Reasoner Math RL dataset, containing filtered responses from DeepSeek-R1-0120 that passed a rule-based validator for mathematical correctness and format compliance.", "author": "Kwai-Klear", "hf_id": "Kwai-Klear/KlearReasoner-MathSub-30K", "downloads": 85, "likes": 3, "license": "apache-2.0", "languages": ["en"], "task_categories": null, "created_at": "2025-08-11T13:14:16+00:00", "last_modified": "2025-09-27T04:40:14+00:00", "citation": null}
{"dataset_id": "layoric/tiny-codes-alpaca", "dataset_url": "https://huggingface.co/datasets/layoric/tiny-codes-alpaca", "stage": "pretraining", "nature": "real", "content_types": "code", "tokens": null, "description": "The README indicates that more information is needed for the dataset card, and no specific details about the dataset content are provided.", "author": "layoric", "hf_id": "layoric/tiny-codes-alpaca", "downloads": 199, "likes": 2, "license": null, "languages": null, "task_categories": null, "created_at": "2023-10-08T02:25:40+00:00", "last_modified": "2023-10-08T02:28:04+00:00", "citation": null}
{"dataset_id": "LEXam-Benchmark/LEXam", "dataset_url": "https://huggingface.co/datasets/LEXam-Benchmark/LEXam", "stage": "post-training", "nature": "real", "content_types": "evaluation, reasoning", "tokens": null, "description": "LEXam is a benchmark dataset consisting of 340 law exams from Swiss, EU, and international law, designed to evaluate legal reasoning in AI systems.", "author": "LEXam-Benchmark", "hf_id": "LEXam-Benchmark/LEXam", "downloads": 1404, "likes": 35, "license": "cc-by-4.0", "languages": ["en", "de"], "task_categories": ["text-classification", "text-generation"], "created_at": "2025-05-16T07:52:59+00:00", "last_modified": "2025-12-16T07:39:35+00:00", "citation": null}
{"dataset_id": "lighteval/squad_v2", "dataset_url": "https://huggingface.co/datasets/lighteval/squad_v2", "stage": "post-training", "nature": "real", "content_types": "qa", "tokens": null, "description": "SQuAD v2 is a reading comprehension dataset containing questions posed on Wikipedia articles, where the answer is a segment of the context or cannot be found in the context.", "author": "lighteval", "hf_id": "lighteval/squad_v2", "downloads": 359, "likes": 0, "license": null, "languages": null, "task_categories": null, "created_at": "2025-07-21T17:20:00+00:00", "last_modified": "2025-07-21T17:20:03+00:00", "citation": null}
{"dataset_id": "lightonai/ArabicWeb24", "dataset_url": "https://huggingface.co/datasets/lightonai/ArabicWeb24", "stage": "pretraining", "nature": "real", "content_types": "web", "tokens": 28000000000, "description": "The ArabicWeb24 dataset consists of more than 28 billion tokens of cleaned and deduplicated Arabic web data from a customized crawl.", "author": "lightonai", "hf_id": "lightonai/ArabicWeb24", "downloads": 3479, "likes": 21, "license": "odc-by", "languages": ["ar"], "task_categories": ["text-generation"], "created_at": "2024-08-06T11:01:34+00:00", "last_modified": "2024-09-23T11:40:16+00:00", "citation": null}
{"dataset_id": "LipengCS/Table-GPT", "dataset_url": "https://huggingface.co/datasets/LipengCS/Table-GPT", "stage": "post-training", "nature": "mixed", "content_types": "instruction-following, other", "tokens": null, "description": "This dataset contains training and test data for 18 diverse table-related tasks, including table understanding, data transformation, table matching, data cleaning, and data augmentation.", "author": "LipengCS", "hf_id": "LipengCS/Table-GPT", "downloads": 759, "likes": 33, "license": "mit", "languages": null, "task_categories": null, "created_at": "2024-05-21T21:03:00+00:00", "last_modified": "2024-05-21T21:11:30+00:00", "citation": null}
{"dataset_id": "liwu/MNBVC", "dataset_url": "https://huggingface.co/datasets/liwu/MNBVC", "stage": "pretraining", "nature": "real", "content_types": "books, code, conversation, instruction-following, math, multilingual, other, qa, reasoning, web, wikipedia", "tokens": null, "description": "MNBVC is a large Chinese language corpus collected from various sources including academic papers, blogs, books, corporate reports, code, crawls, forums, games, government documents, laws, math materials, news, parallel data, patents, Q&A, Wikipedia, and medical documents.", "author": "liwu", "hf_id": "liwu/MNBVC", "downloads": 97408, "likes": 571, "license": ["mit"], "languages": ["zh"], "task_categories": ["text-generation", "fill-mask"], "created_at": "2023-02-13T14:00:47+00:00", "last_modified": "2025-12-03T13:19:11+00:00", "citation": null}
{"dataset_id": "LLM360/MegaMath", "dataset_url": "https://huggingface.co/datasets/LLM360/MegaMath", "stage": "pretraining", "nature": "mixed", "content_types": "code, math, other, qa, web", "tokens": 371600000000, "description": "MegaMath is an open math pretraining dataset curated from diverse, math-focused sources, including web data, code data, and synthetic data.", "author": "LLM360", "hf_id": "LLM360/MegaMath", "downloads": 37793, "likes": 108, "license": "odc-by", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2025-03-30T20:15:00+00:00", "last_modified": "2025-04-09T13:17:50+00:00", "citation": null}
{"dataset_id": "loubnabnl/ultrachat_questions_about_world", "dataset_url": "https://huggingface.co/datasets/loubnabnl/ultrachat_questions_about_world", "stage": "post-training", "nature": "real", "content_types": "other", "tokens": null, "description": "This is the \"Questions about the world\" subset of UltraChat.", "author": "HuggingFaceTB", "hf_id": "HuggingFaceTB/ultrachat_questions_about_world", "downloads": 232, "likes": 7, "license": "mit", "languages": ["en"], "task_categories": null, "created_at": "2024-01-09T15:45:37+00:00", "last_modified": "2024-02-20T12:29:06+00:00", "citation": null}
{"dataset_id": "m-a-p/MAP-CC", "dataset_url": "https://huggingface.co/datasets/m-a-p/MAP-CC", "stage": "pretraining", "nature": "real", "content_types": "books, other, qa, web, wikipedia", "tokens": 800000000000, "description": "An open-source Chinese pretraining dataset with a scale of 800 billion tokens, offering the NLP community high-quality Chinese pretraining data.", "author": "m-a-p", "hf_id": "m-a-p/MAP-CC", "downloads": 2872, "likes": 76, "license": "cc-by-nc-nd-4.0", "languages": null, "task_categories": null, "created_at": "2024-04-02T22:05:53+00:00", "last_modified": "2024-07-11T02:55:46+00:00", "citation": null}
{"dataset_id": "Magpie-Align/Magpie-Pro-300K-Filtered", "dataset_url": "https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following", "tokens": null, "description": "This dataset contains 300K high-quality instruction-response pairs extracted from Llama-3-70B-Instruct using the Magpie self-synthesis method, filtered for input quality, instruction reward, and response length.", "author": "Magpie-Align", "hf_id": "Magpie-Align/Magpie-Pro-300K-Filtered", "downloads": 744, "likes": 51, "license": "llama3", "languages": null, "task_categories": null, "created_at": "2024-06-11T09:28:20+00:00", "last_modified": "2024-08-28T04:39:02+00:00", "citation": null}
{"dataset_id": "Magpie-Align/Magpie-Reasoning-V1-150K", "dataset_url": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V1-150K", "stage": "post-training", "nature": "synthetic", "content_types": "code, instruction-following, math, reasoning", "tokens": null, "description": "This dataset contains 150K high-quality instruction-response pairs focused on reasoning, generated by Qwen2-72B-Instruct for instructions and Llama 3 70B Instruct for responses, aimed at augmenting model reasoning capabilities.", "author": "Magpie-Align", "hf_id": "Magpie-Align/Magpie-Reasoning-V1-150K", "downloads": 193, "likes": 57, "license": "llama3", "languages": ["en"], "task_categories": null, "created_at": "2024-07-11T22:02:20+00:00", "last_modified": "2025-01-27T19:59:05+00:00", "citation": null}
{"dataset_id": "Magpie-Align/Magpie-Reasoning-V1-150K-CoT-Deepseek-R1-Llama-70B", "dataset_url": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V1-150K-CoT-Deepseek-R1-Llama-70B", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following, reasoning", "tokens": null, "description": "This dataset contains 150K instruction-response pairs focused on reasoning tasks, generated by Qwen2-72B-Instruct for instructions and DeepSeek-R1-Distill-Llama-70B for responses using the Magpie framework.", "author": "Magpie-Align", "hf_id": "Magpie-Align/Magpie-Reasoning-V1-150K-CoT-Deepseek-R1-Llama-70B", "downloads": 102, "likes": 17, "license": null, "languages": null, "task_categories": null, "created_at": "2025-01-27T19:47:15+00:00", "last_modified": "2025-01-27T19:52:24+00:00", "citation": null}
{"dataset_id": "Magpie-Align/Magpie-Reasoning-V1-150K-CoT-QwQ", "dataset_url": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V1-150K-CoT-QwQ", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following, reasoning", "tokens": null, "description": "This dataset contains 150K instruction-response pairs focused on reasoning tasks, generated by Qwen2-72B-Instruct for instructions and QwQ-32B-Preview for responses using the Magpie framework.", "author": "Magpie-Align", "hf_id": "Magpie-Align/Magpie-Reasoning-V1-150K-CoT-QwQ", "downloads": 120, "likes": 8, "license": null, "languages": null, "task_categories": null, "created_at": "2025-01-09T08:32:55+00:00", "last_modified": "2025-01-27T19:57:56+00:00", "citation": null}
{"dataset_id": "Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B", "dataset_url": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following, reasoning", "tokens": null, "description": "This dataset contains 250K instruction-response pairs generated by prompting aligned LLMs, specifically using Llama 3.1 70B Instruct and Llama 3.3 70B Instruct for instructions and DeepSeek-R1-Distill-Llama-70B for responses, aimed at augmenting reasoning capabilities.", "author": "Magpie-Align", "hf_id": "Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B", "downloads": 692, "likes": 100, "license": "llama3.3", "languages": ["en"], "task_categories": null, "created_at": "2025-01-25T04:54:44+00:00", "last_modified": "2025-01-27T19:53:38+00:00", "citation": null}
{"dataset_id": "Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Llama3", "dataset_url": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Llama3", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following, reasoning", "tokens": null, "description": "This dataset contains instruction-response pairs generated by Meta's Llama 3.1 and 3.3 70B Instruct models using Magpie, focusing on Chain-of-Thought (CoT) patterns in both instructions and responses.", "author": "Magpie-Align", "hf_id": "Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Llama3", "downloads": 116, "likes": 10, "license": "llama3.1", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2025-01-02T04:43:02+00:00", "last_modified": "2025-01-27T19:56:51+00:00", "citation": null}
{"dataset_id": "Magpie-Align/Magpie-Reasoning-V2-250K-CoT-QwQ", "dataset_url": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-QwQ", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following, reasoning", "tokens": null, "description": "This dataset contains 250K instruction-response pairs generated using the Magpie framework, where instructions are created by Llama-3.1 and Llama-3.3 70B Instruct models and responses are generated by QwQ-32B-Preview.", "author": "Magpie-Align", "hf_id": "Magpie-Align/Magpie-Reasoning-V2-250K-CoT-QwQ", "downloads": 148, "likes": 17, "license": "llama3.1", "languages": ["en"], "task_categories": null, "created_at": "2025-01-09T08:44:09+00:00", "last_modified": "2025-01-27T19:55:41+00:00", "citation": null}
{"dataset_id": "Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Skywork-O1-Llama-3.1-8B", "dataset_url": "https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Skywork-O1-Llama-3.1-8B", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following, reasoning", "tokens": null, "description": "This dataset contains 250K instruction-response pairs generated by prompting aligned LLMs, specifically using Llama 3.1 70B Instruct for instructions and Skywork-o1-Open-Llama-3.1-8B for responses, aimed at augmenting reasoning capabilities.", "author": "Magpie-Align", "hf_id": "Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Skywork-O1-Llama-3.1-8B", "downloads": 85, "likes": 6, "license": "llama3.1", "languages": ["en"], "task_categories": null, "created_at": "2025-01-09T08:52:37+00:00", "last_modified": "2025-01-27T19:56:13+00:00", "citation": null}
{"dataset_id": "math-ai/AutoMathText", "dataset_url": "https://huggingface.co/datasets/math-ai/AutoMathText", "stage": "pretraining", "nature": "real", "content_types": "books, code, math, web", "tokens": null, "description": "AutoMathText is an extensive dataset of around 200 GB of mathematical texts sourced from websites, arXiv, and GitHub, autonomously selected by the Qwen-72B model.", "author": "math-ai", "hf_id": "math-ai/AutoMathText", "downloads": 6557, "likes": 182, "license": "cc-by-sa-4.0", "languages": ["en"], "task_categories": ["text-generation", "question-answering"], "created_at": "2024-01-24T01:39:26+00:00", "last_modified": "2025-07-16T05:03:38+00:00", "citation": null}
{"dataset_id": "MatrixStudio/Codeforces-Python-Submissions", "dataset_url": "https://huggingface.co/datasets/MatrixStudio/Codeforces-Python-Submissions", "stage": "pretraining", "nature": "real", "content_types": "code", "tokens": null, "description": "More Information needed", "author": "MatrixStudio", "hf_id": "MatrixStudio/Codeforces-Python-Submissions", "downloads": 726, "likes": 45, "license": null, "languages": null, "task_categories": null, "created_at": "2024-02-01T14:38:30+00:00", "last_modified": "2025-03-13T03:02:21+00:00", "citation": null}
{"dataset_id": "mlfoundations/dclm-baseline-1.0", "dataset_url": "https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0", "stage": "pretraining", "nature": "real", "content_types": "web", "tokens": 4000000000000, "description": "DCLM-baseline is a pretraining dataset of 4T tokens / 3B documents derived from Common Crawl, designed as a research baseline for the DCLM benchmark to demonstrate data curation strategies.", "author": "mlfoundations", "hf_id": "mlfoundations/dclm-baseline-1.0", "downloads": 472782, "likes": 250, "license": "cc-by-4.0", "languages": null, "task_categories": null, "created_at": "2024-06-17T18:57:13+00:00", "last_modified": "2024-07-22T15:27:52+00:00", "citation": null}
{"dataset_id": "mlfoundations/dclm-baseline-1.0-parquet", "dataset_url": "https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0-parquet", "stage": "pretraining", "nature": "real", "content_types": "web", "tokens": 4000000000000, "description": "DCLM-baseline is a pretraining dataset of 4T tokens / 3B documents derived from Common Crawl, designed to demonstrate the effectiveness of data curation strategies for training language models.", "author": "mlfoundations", "hf_id": "mlfoundations/dclm-baseline-1.0-parquet", "downloads": 5375, "likes": 32, "license": "cc-by-4.0", "languages": ["en"], "task_categories": null, "created_at": "2024-06-30T20:31:14+00:00", "last_modified": "2024-07-19T17:35:58+00:00", "citation": null}
{"dataset_id": "nickrosh/Evol-Instruct-Code-80k-v1", "dataset_url": "https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1", "stage": "post-training", "nature": "real", "content_types": "code, instruction-following", "tokens": null, "description": "The dataset is an open source implementation of Evol-Instruct-Code, as referenced in the WizardCoder Paper, focusing on code instruction generation.", "author": "nickrosh", "hf_id": "nickrosh/Evol-Instruct-Code-80k-v1", "downloads": 1110, "likes": 245, "license": "cc-by-nc-sa-4.0", "languages": null, "task_categories": null, "created_at": "2023-07-08T04:31:37+00:00", "last_modified": "2023-07-11T02:05:26+00:00", "citation": null}
{"dataset_id": "NousResearch/hermes-function-calling-v1", "dataset_url": "https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1", "stage": "post-training", "nature": "synthetic", "content_types": "conversation, instruction-following, tool-use", "tokens": null, "description": "This dataset is a compilation of structured output and function calling data used to train LLM models in performing function calls and returning structured output based on natural language instructions. It features various conversational scenarios with AI agents executing single or multiple function calls.", "author": "NousResearch", "hf_id": "NousResearch/hermes-function-calling-v1", "downloads": 1818, "likes": 363, "license": "apache-2.0", "languages": ["en"], "task_categories": ["text-generation", "question-answering", "feature-extraction"], "created_at": "2024-08-14T01:22:36+00:00", "last_modified": "2025-12-22T14:19:42+00:00", "citation": null}
{"dataset_id": "NovaSky-AI/Sky-T1_data_17k", "dataset_url": "https://huggingface.co/datasets/NovaSky-AI/Sky-T1_data_17k", "stage": "pretraining", "nature": "real", "content_types": "code, math, other", "tokens": null, "description": "Sky-T1_data_17k is a training dataset used to train Sky-T1-32B-Preview, containing coding, math, and science/puzzle data from various sources.", "author": "NovaSky-AI", "hf_id": "NovaSky-AI/Sky-T1_data_17k", "downloads": 275, "likes": 186, "license": "apache-2.0", "languages": null, "task_categories": null, "created_at": "2025-01-11T19:49:17+00:00", "last_modified": "2025-01-14T10:36:09+00:00", "citation": null}
{"dataset_id": "nvidia/AceMath-Instruct-Training-Data", "dataset_url": "https://huggingface.co/datasets/nvidia/AceMath-Instruct-Training-Data", "stage": "post-training", "nature": "mixed", "content_types": "code, instruction-following, math", "tokens": null, "description": "The dataset contains training data for AceMath-Instruct models, including general-purpose and math-specific supervised fine-tuning samples generated by Qwen2.5-Math and GPT-4o-mini.", "author": "nvidia", "hf_id": "nvidia/AceMath-Instruct-Training-Data", "downloads": 359, "likes": 62, "license": "cc-by-nc-4.0", "languages": ["en"], "task_categories": null, "created_at": "2025-01-15T16:14:38+00:00", "last_modified": "2025-01-17T12:41:19+00:00", "citation": null}
{"dataset_id": "nvidia/AceMath-RewardBench", "dataset_url": "https://huggingface.co/datasets/nvidia/AceMath-RewardBench", "stage": "post-training", "nature": "real", "content_types": "evaluation, math", "tokens": null, "description": "The AceMath-RewardBench evaluation dataset evaluates capabilities of a math reward model using the best-of-N (N=8) setting for 7 datasets, each containing mathematical questions paired with multiple model-generated solutions and ground truth scores.", "author": "nvidia", "hf_id": "nvidia/AceMath-RewardBench", "downloads": 352, "likes": 6, "license": "cc-by-nc-4.0", "languages": ["en"], "task_categories": ["question-answering"], "created_at": "2025-01-14T18:33:27+00:00", "last_modified": "2025-01-17T07:29:49+00:00", "citation": null}
{"dataset_id": "nvidia/AceMath-RM-Training-Data", "dataset_url": "https://huggingface.co/datasets/nvidia/AceMath-RM-Training-Data", "stage": "post-training", "nature": "real", "content_types": "conversation, math, preference", "tokens": null, "description": "The dataset contains math questions paired with multiple responses, used to train a reward model for evaluating math reasoning solutions.", "author": "nvidia", "hf_id": "nvidia/AceMath-RM-Training-Data", "downloads": 347, "likes": 14, "license": "cc-by-nc-4.0", "languages": ["en"], "task_categories": null, "created_at": "2025-01-14T18:30:46+00:00", "last_modified": "2025-01-17T07:30:42+00:00", "citation": null}
{"dataset_id": "nvidia/Llama-Nemotron-Post-Training-Dataset", "dataset_url": "https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset", "stage": "post-training", "nature": "synthetic", "content_types": "code, conversation, instruction-following, math, preference, reasoning, safety", "tokens": null, "description": "This dataset is a compilation of SFT and RL data that supports improvements of math, code, general reasoning, and instruction following capabilities of the original Llama instruct model, in support of NVIDIAs release of Llama-3.1-Nemotron-Ultra-253B-v1, Llama-3.3-Nemotron-Super-49B-v1 and Llama-3.1-Nemotron-Nano-8B-v1.", "author": "nvidia", "hf_id": "nvidia/Llama-Nemotron-Post-Training-Dataset", "downloads": 5303, "likes": 623, "license": "cc-by-4.0", "languages": null, "task_categories": null, "created_at": "2025-03-13T21:01:09+00:00", "last_modified": "2025-05-08T17:51:50+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-3-Nano-RL-Training-Blend", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-3-Nano-RL-Training-Blend", "stage": "post-training", "nature": "synthetic", "content_types": "code, instruction-following, math, qa", "tokens": null, "description": "Nemotron-3-Nano-RL-Training-Blend is a curated dataset blend used to train the Nemotron-3-Nano-30B-A3B model, consisting of multiple component datasets with specified mixing ratios.", "author": "nvidia", "hf_id": "nvidia/Nemotron-3-Nano-RL-Training-Blend", "downloads": 441, "likes": 12, "license": "odc-by", "languages": ["en"], "task_categories": null, "created_at": "2025-12-13T00:16:24+00:00", "last_modified": "2025-12-15T14:44:37+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-Agentic-v1", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-Agentic-v1", "stage": "post-training", "nature": "synthetic", "content_types": "conversation, tool-use", "tokens": null, "description": "The Nemotron-Agentic-Tool-Use-v1 dataset is designed to strengthen models capabilities as interactive, tool-using agents. It focuses on multi-turn conversations where language models decompose user goals, decide when to call tools, and reason over tool outputs to complete tasks reliably and safely.", "author": "nvidia", "hf_id": "nvidia/Nemotron-Agentic-v1", "downloads": 1106, "likes": 126, "license": "cc-by-4.0", "languages": ["en"], "task_categories": null, "created_at": "2025-12-14T02:52:50+00:00", "last_modified": "2025-12-15T13:48:35+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-CC-Code-v1", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-CC-Code-v1", "stage": "pretraining", "nature": "mixed", "content_types": "code, math, other, reasoning, web", "tokens": 6600000000000, "description": "The Nemotron-Pre-Training-Dataset-v2.1 extends the previously released Nemotron pretraining datasets with refreshed, higher-quality, and more diverse data across math, code, English Common Crawl, and large-scale synthetic corpora.", "author": "nvidia", "hf_id": "nvidia/Nemotron-CC-Code-v1", "downloads": 2148, "likes": 11, "license": "other", "languages": null, "task_categories": ["text-generation"], "created_at": "2025-12-06T18:05:00+00:00", "last_modified": "2025-12-22T17:29:20+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-CC-Math-v1", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-CC-Math-v1", "stage": "pretraining", "nature": "real", "content_types": "code, math, reasoning", "tokens": 133000000000, "description": "Nemotron-CC-Math is a large-scale, high-quality math corpus extracted from Common Crawl, designed to preserve and surface high-value mathematical and code content for pretraining large language models.", "author": "nvidia", "hf_id": "nvidia/Nemotron-CC-Math-v1", "downloads": 7168, "likes": 60, "license": "other", "languages": null, "task_categories": ["text-generation"], "created_at": "2025-08-14T08:03:50+00:00", "last_modified": "2025-12-23T00:17:16+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-CC-v2", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-CC-v2", "stage": "pretraining", "nature": "mixed", "content_types": "code, instruction-following, math, multilingual, qa", "tokens": 6585800000000, "description": "This pretraining dataset preserves high-value math and code while enriching it with diverse multilingual Q&A, fueling the next generation of intelligent, globally-capable models.", "author": "nvidia", "hf_id": "nvidia/Nemotron-CC-v2", "downloads": 40257, "likes": 96, "license": "other", "languages": null, "task_categories": ["text-generation"], "created_at": "2025-08-14T05:49:54+00:00", "last_modified": "2025-12-23T00:16:50+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-CC-v2.1", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-CC-v2.1", "stage": "pretraining", "nature": "mixed", "content_types": "code, math, multilingual, other, reasoning, web", "tokens": 6600000000000, "description": "The Nemotron-Pre-Training-Dataset-v2.1 extends the previously released Nemotron pretraining datasets with refreshed, higher-quality, and more diverse data across math, code, English Common Crawl, and large-scale synthetic corpora. Designed for the NVIDIA Nemotron 3 family of LLMs, the dataset introduces new Common Crawl code extraction, 2.5T new English web tokens, updated GitHub-sourced source-code corpora, and specialized STEM reasoning datasets.", "author": "nvidia", "hf_id": "nvidia/Nemotron-CC-v2.1", "downloads": 23532, "likes": 91, "license": "other", "languages": null, "task_categories": ["text-generation"], "created_at": "2025-12-12T19:25:01+00:00", "last_modified": "2025-12-22T17:09:52+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-Competitive-Programming-v1", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-Competitive-Programming-v1", "stage": "post-training", "nature": "synthetic", "content_types": "code, reasoning", "tokens": null, "description": "Nemotron-Competitive-Programming-v1 is a large-scale synthetic coding and reasoning dataset designed for supervised fine-tuning (SFT) tasks of code completion and code critique, combining Python and C++ samples across unique competitive programming questions and including a cross-domain subset called InfiniByte.", "author": "nvidia", "hf_id": "nvidia/Nemotron-Competitive-Programming-v1", "downloads": 3221, "likes": 11, "license": "cc-by-4.0", "languages": ["en"], "task_categories": null, "created_at": "2025-12-15T02:13:24+00:00", "last_modified": "2025-12-15T10:30:24+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-Instruction-Following-Chat-v1", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-Instruction-Following-Chat-v1", "stage": "post-training", "nature": "mixed", "content_types": "conversation, instruction-following, other", "tokens": null, "description": "The Nemotron-Instruction-Following-Chat-v1 dataset is designed to strengthen model interactive capabilities, including open-ended chat, instruction following, and structured output generation. It combines refreshed chat data with synthetic dialogues produced by frontier models.", "author": "nvidia", "hf_id": "nvidia/Nemotron-Instruction-Following-Chat-v1", "downloads": 3847, "likes": 99, "license": "cc-by-4.0", "languages": ["en"], "task_categories": null, "created_at": "2025-12-14T02:21:24+00:00", "last_modified": "2025-12-15T05:27:15+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-Math-Proofs-v1", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-Math-Proofs-v1", "stage": "post-training", "nature": "mixed", "content_types": "code, math, reasoning", "tokens": null, "description": "Nemotron-Math-Proofs-v1 is a large-scale mathematical reasoning dataset containing natural language proof problems, formalizations into Lean 4 theorem statements, and model-generated reasoning trajectories culminating in Lean 4 proofs.", "author": "nvidia", "hf_id": "nvidia/Nemotron-Math-Proofs-v1", "downloads": 1447, "likes": 82, "license": "cc-by-sa-4.0", "languages": ["en"], "task_categories": null, "created_at": "2025-12-14T15:40:40+00:00", "last_modified": "2025-12-18T14:54:01+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-Math-v2", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-Math-v2", "stage": "post-training", "nature": "mixed", "content_types": "math, reasoning, tool-use", "tokens": null, "description": "Nemotron-Math-v2 is a large-scale mathematical reasoning dataset containing approximately 347K high-quality mathematical problems and 7M model-generated reasoning trajectories, integrating human-authored problem sets with systematically generated solution traces produced under multiple reasoning modes and tool-use configurations.", "author": "nvidia", "hf_id": "nvidia/Nemotron-Math-v2", "downloads": 4689, "likes": 20, "license": "cc-by-4.0", "languages": ["en"], "task_categories": null, "created_at": "2025-12-15T09:09:15+00:00", "last_modified": "2025-12-19T16:00:53+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-Personas-USA", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-Personas-USA", "stage": "post-training", "nature": "synthetic", "content_types": "other", "tokens": 940000000, "description": "Nemotron-Personas-USA is a dataset of synthetically-generated personas grounded in real-world demographic, geographic and personality trait distributions to capture the diversity and richness of the US population.", "author": "nvidia", "hf_id": "nvidia/Nemotron-Personas-USA", "downloads": 5520, "likes": 238, "license": "cc-by-4.0", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2025-06-09T04:12:11+00:00", "last_modified": "2025-12-16T19:13:23+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-Post-Training-Dataset-v1", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v1", "stage": "post-training", "nature": "synthetic", "content_types": "code, instruction-following, math, reasoning, tool-use", "tokens": null, "description": "This dataset is a compilation of SFT data supporting improvements in math, code, stem, general reasoning, and tool calling capabilities of the original Llama instruct model.", "author": "nvidia", "hf_id": "nvidia/Nemotron-Post-Training-Dataset-v1", "downloads": 11619, "likes": 169, "license": "cc-by-4.0", "languages": null, "task_categories": null, "created_at": "2025-07-29T23:41:37+00:00", "last_modified": "2025-08-25T20:03:33+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-Post-Training-Dataset-v2", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v2", "stage": "post-training", "nature": "mixed", "content_types": "code, conversation, instruction-following, math, multilingual, reasoning", "tokens": null, "description": "This dataset extends NVIDIA's post-training dataset releases with SFT and RL data into five target languages: Spanish, French, German, Italian and Japanese, supporting improvements in math, code, general reasoning, and instruction following capabilities.", "author": "nvidia", "hf_id": "nvidia/Nemotron-Post-Training-Dataset-v2", "downloads": 8667, "likes": 83, "license": "cc-by-4.0", "languages": ["en", "de", "it", "fr", "es", "ja"], "task_categories": null, "created_at": "2025-08-13T17:24:50+00:00", "last_modified": "2025-08-21T04:29:18+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-Pretraining-Code-v1", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-Code-v1", "stage": "pretraining", "nature": "mixed", "content_types": "code, math, multilingual, qa", "tokens": 65858000000000, "description": "This pretraining dataset preserves high-value math and code while enriching it with diverse multilingual Q&A for generative AI model training.", "author": "nvidia", "hf_id": "nvidia/Nemotron-Pretraining-Code-v1", "downloads": 3740, "likes": 56, "license": "other", "languages": null, "task_categories": ["text-generation"], "created_at": "2025-08-14T05:51:47+00:00", "last_modified": "2025-12-23T00:13:54+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-Pretraining-Code-v2", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-Code-v2", "stage": "pretraining", "nature": "mixed", "content_types": "code, math, multilingual, other, reasoning, web", "tokens": 6600000000000, "description": "The Nemotron-Pre-Training-Dataset-v2.1 extends the previously released Nemotron pretraining datasets with refreshed, higher-quality, and more diverse data across math, code, English Common Crawl, and large-scale synthetic corpora.", "author": "nvidia", "hf_id": "nvidia/Nemotron-Pretraining-Code-v2", "downloads": 6405, "likes": 92, "license": "other", "languages": null, "task_categories": ["text-generation"], "created_at": "2025-11-25T00:07:50+00:00", "last_modified": "2025-12-22T17:10:23+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-Pretraining-Dataset-sample", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-Dataset-sample", "stage": "pretraining", "nature": "mixed", "content_types": "code, instruction-following, math, multilingual, qa", "tokens": 65858000000000, "description": "This pretraining dataset preserves high-value math and code while enriching it with diverse multilingual Q&A for generative AI model training.", "author": "nvidia", "hf_id": "nvidia/Nemotron-Pretraining-Dataset-sample", "downloads": 1067, "likes": 33, "license": "other", "languages": null, "task_categories": null, "created_at": "2025-08-14T06:06:21+00:00", "last_modified": "2025-12-22T17:07:37+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-Pretraining-SFT-v1", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-SFT-v1", "stage": "pretraining", "nature": "mixed", "content_types": "code, instruction-following, math, multilingual, qa", "tokens": 6585800000000, "description": "This pretraining dataset for generative AI model training preserves high-value math and code while enriching it with diverse multilingual Q&A.", "author": "nvidia", "hf_id": "nvidia/Nemotron-Pretraining-SFT-v1", "downloads": 11363, "likes": 49, "license": "other", "languages": null, "task_categories": ["text-generation"], "created_at": "2025-08-14T05:52:58+00:00", "last_modified": "2025-12-23T00:16:25+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-Pretraining-Specialized-v1", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-Specialized-v1", "stage": "pretraining", "nature": "mixed", "content_types": "books, code, math, multilingual, other, reasoning, web", "tokens": 6600000000000, "description": "The Nemotron-Pre-Training-Dataset-v2.1 extends the previously released Nemotron pretraining datasets with refreshed, higher-quality, and more diverse data across math, code, English Common Crawl, and large-scale synthetic corpora. Designed for the NVIDIA Nemotron 3 family of LLMs, the dataset introduces new Common Crawl code extraction, 2.5T new English web tokens, updated GitHub-sourced source-code corpora, and specialized STEM reasoning datasets.", "author": "nvidia", "hf_id": "nvidia/Nemotron-Pretraining-Specialized-v1", "downloads": 8453, "likes": 57, "license": "cc-by-4.0", "languages": null, "task_categories": ["text-generation"], "created_at": "2025-12-14T08:27:18+00:00", "last_modified": "2025-12-22T17:17:17+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-RL-agent-workplace_assistant", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-RL-agent-workplace_assistant", "stage": "post-training", "nature": "synthetic", "content_types": "other, tool-use", "tokens": null, "description": "The Nemotron-RL-agent-workplace_assistant is a tool use - multi step agentic environment that tests the agents ability to execute tasks in a workplace setting, containing a sandbox environment with five databases, 26 tools, and 690 tasks representing common business activities.", "author": "nvidia", "hf_id": "nvidia/Nemotron-RL-agent-workplace_assistant", "downloads": 463, "likes": 9, "license": null, "languages": null, "task_categories": null, "created_at": "2025-11-14T06:35:51+00:00", "last_modified": "2025-12-12T20:40:49+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-RL-coding-competitive_coding", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-RL-coding-competitive_coding", "stage": "post-training", "nature": "synthetic", "content_types": "code, reasoning", "tokens": null, "description": "The Nemotron-RL-coding-competitive_coding dataset is a python-only, reasoning-based, synthetic dataset containing competitive coding style problems and their unit test cases collected from CodeContests and Open-R1.", "author": "nvidia", "hf_id": "nvidia/Nemotron-RL-coding-competitive_coding", "downloads": 839, "likes": 11, "license": "cc-by-sa-4.0", "languages": ["en"], "task_categories": null, "created_at": "2025-12-12T20:14:38+00:00", "last_modified": "2025-12-15T15:03:13+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-RL-instruction_following", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-RL-instruction_following", "stage": "post-training", "nature": "real", "content_types": "instruction-following", "tokens": null, "description": "The Nemotron-RL-instruction_following dataset combines prompts from the WildChat-1M dataset with instructions from the Open-Instruct code base, designed for evaluating and training models on objective instruction adherence.", "author": "nvidia", "hf_id": "nvidia/Nemotron-RL-instruction_following", "downloads": 157, "likes": 7, "license": null, "languages": null, "task_categories": null, "created_at": "2025-11-14T06:38:29+00:00", "last_modified": "2025-12-12T06:49:56+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-RL-instruction_following-structured_outputs", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-RL-instruction_following-structured_outputs", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following", "tokens": null, "description": "The Nemotron-RL-instruction_following-structured_outputs dataset tests the ability of the model to follow output formatting instructions under schema constraints under the JSON format. Each problem consists of three components: The document, output formatting Instruction (Schema), and question.", "author": "nvidia", "hf_id": "nvidia/Nemotron-RL-instruction_following-structured_outputs", "downloads": 405, "likes": 25, "license": null, "languages": null, "task_categories": null, "created_at": "2025-11-13T18:44:28+00:00", "last_modified": "2025-12-12T06:50:14+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-RL-knowledge-mcqa", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-RL-knowledge-mcqa", "stage": "post-training", "nature": "synthetic", "content_types": "qa", "tokens": null, "description": "The Nemotron-RL-knowledge-mcqa is a multi-domain synthetic multiple-choice question-answering (MCQA) dataset containing knowledge based questions.", "author": "nvidia", "hf_id": "nvidia/Nemotron-RL-knowledge-mcqa", "downloads": 475, "likes": 7, "license": null, "languages": null, "task_categories": null, "created_at": "2025-11-14T07:19:45+00:00", "last_modified": "2025-12-12T06:50:47+00:00", "citation": null}
{"dataset_id": "nvidia/Nemotron-Science-v1", "dataset_url": "https://huggingface.co/datasets/nvidia/Nemotron-Science-v1", "stage": "post-training", "nature": "synthetic", "content_types": "qa, reasoning", "tokens": null, "description": "Nemotron-Science-v1 is a synthetic science reasoning dataset containing two subsets: MCQA with synthetic science questions and reasoning traces, and RQA with synthetic chemistry questions, designed to enhance large language model reasoning capabilities in scientific domains.", "author": "nvidia", "hf_id": "nvidia/Nemotron-Science-v1", "downloads": 1261, "likes": 14, "license": "cc-by-4.0", "languages": ["en"], "task_categories": null, "created_at": "2025-12-14T01:16:19+00:00", "last_modified": "2025-12-15T03:40:01+00:00", "citation": null}
{"dataset_id": "nvidia/OpenCodeReasoning", "dataset_url": "https://huggingface.co/datasets/nvidia/OpenCodeReasoning", "stage": "post-training", "nature": "synthetic", "content_types": "code, instruction-following", "tokens": null, "description": "OpenCodeReasoning is the largest reasoning-based synthetic dataset for coding, comprising 735,255 samples in Python across 28,319 unique competitive programming questions, designed for supervised fine-tuning (SFT).", "author": "nvidia", "hf_id": "nvidia/OpenCodeReasoning", "downloads": 4014, "likes": 519, "license": "cc-by-4.0", "languages": null, "task_categories": ["text-generation"], "created_at": "2025-04-01T20:07:48+00:00", "last_modified": "2025-05-04T23:54:22+00:00", "citation": null}
{"dataset_id": "nvidia/OpenCodeReasoning-2", "dataset_url": "https://huggingface.co/datasets/nvidia/OpenCodeReasoning-2", "stage": "post-training", "nature": "synthetic", "content_types": "code, reasoning", "tokens": null, "description": "OpenCodeReasoning-2 is the largest reasoning-based synthetic dataset to date for coding, comprising 1.4M samples in Python and 1.1M samples in C++ across 34,799 unique competitive programming questions. It is designed for supervised fine-tuning (SFT) tasks of code completion and code critique.", "author": "nvidia", "hf_id": "nvidia/OpenCodeReasoning-2", "downloads": 1536, "likes": 48, "license": "cc-by-4.0", "languages": null, "task_categories": ["text-generation"], "created_at": "2025-05-03T21:50:05+00:00", "last_modified": "2025-05-17T00:19:15+00:00", "citation": null}
{"dataset_id": "nvidia/OpenMathInstruct-2", "dataset_url": "https://huggingface.co/datasets/nvidia/OpenMathInstruct-2", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following, math", "tokens": null, "description": "OpenMathInstruct-2 is a math instruction tuning dataset containing 14M problem-solution pairs generated using the Llama3.1-405B-Instruct model, derived from GSM8K and MATH training sets.", "author": "nvidia", "hf_id": "nvidia/OpenMathInstruct-2", "downloads": 17074, "likes": 224, "license": "cc-by-4.0", "languages": ["en"], "task_categories": ["question-answering", "text-generation"], "created_at": "2024-09-28T16:37:52+00:00", "last_modified": "2024-11-25T20:07:28+00:00", "citation": null}
{"dataset_id": "nvidia/OpenMathReasoning", "dataset_url": "https://huggingface.co/datasets/nvidia/OpenMathReasoning", "stage": "post-training", "nature": "real", "content_types": "math, reasoning", "tokens": null, "description": "OpenMathReasoning is a large-scale math reasoning dataset containing 306K unique mathematical problems from AoPS forums, with 3.2M chain-of-thought solutions, 1.7M tool-integrated reasoning solutions, and 566K GenSelect samples.", "author": "nvidia", "hf_id": "nvidia/OpenMathReasoning", "downloads": 15765, "likes": 390, "license": "cc-by-4.0", "languages": ["en"], "task_categories": ["question-answering", "text-generation"], "created_at": "2025-04-22T05:44:36+00:00", "last_modified": "2025-05-27T18:43:44+00:00", "citation": null}
{"dataset_id": "open-r1/Big-Math-RL-Verified-Processed", "dataset_url": "https://huggingface.co/datasets/open-r1/Big-Math-RL-Verified-Processed", "stage": "post-training", "nature": "real", "content_types": "math", "tokens": null, "description": "This is a processed version of SynthLabsAI/Big-Math-RL-Verified where filters have been applied and difficulty subsets created based on solve rates.", "author": "open-r1", "hf_id": "open-r1/Big-Math-RL-Verified-Processed", "downloads": 1060, "likes": 25, "license": null, "languages": null, "task_categories": null, "created_at": "2025-04-02T08:19:14+00:00", "last_modified": "2025-04-11T18:30:23+00:00", "citation": null}
{"dataset_id": "open-r1/codeforces", "dataset_url": "https://huggingface.co/datasets/open-r1/codeforces", "stage": "post-training", "nature": "real", "content_types": "code", "tokens": null, "description": "This dataset includes more than 10k unique problems from CodeForces covering contests up to 2025, with generated checker code and additional challenging test cases for improved verifiability.", "author": "open-r1", "hf_id": "open-r1/codeforces", "downloads": 9653, "likes": 83, "license": "cc-by-4.0", "languages": null, "task_categories": null, "created_at": "2025-02-12T11:54:21+00:00", "last_modified": "2025-05-19T15:18:01+00:00", "citation": null}
{"dataset_id": "open-r1/codeforces-cots", "dataset_url": "https://huggingface.co/datasets/open-r1/codeforces-cots", "stage": "post-training", "nature": "mixed", "content_types": "code, reasoning", "tokens": null, "description": "CodeForces-CoTs is a large-scale dataset for training reasoning models on competitive programming tasks, consisting of 10k CodeForces problems with reasoning traces generated by DeepSeek R1.", "author": "open-r1", "hf_id": "open-r1/codeforces-cots", "downloads": 3693, "likes": 198, "license": "cc-by-4.0", "languages": null, "task_categories": null, "created_at": "2025-02-27T10:35:02+00:00", "last_modified": "2025-03-28T12:21:06+00:00", "citation": null}
{"dataset_id": "open-r1/codeforces-submissions", "dataset_url": "https://huggingface.co/datasets/open-r1/codeforces-submissions", "stage": "pretraining", "nature": "real", "content_types": "code", "tokens": null, "description": "This dataset includes millions of real user (human) code submissions to the CodeForces website.", "author": "open-r1", "hf_id": "open-r1/codeforces-submissions", "downloads": 606, "likes": 8, "license": "cc-by-4.0", "languages": null, "task_categories": null, "created_at": "2025-02-17T00:12:25+00:00", "last_modified": "2025-05-14T13:49:02+00:00", "citation": null}
{"dataset_id": "open-r1/DAPO-Math-17k-Processed", "dataset_url": "https://huggingface.co/datasets/open-r1/DAPO-Math-17k-Processed", "stage": "post-training", "nature": "real", "content_types": "math", "tokens": null, "description": "This is a processed version of DAPO-Math-17k where prompts have been deduplicated and reformatted for compatibility with TRL's GRPO trainer, including derived English and Chinese subsets.", "author": "open-r1", "hf_id": "open-r1/DAPO-Math-17k-Processed", "downloads": 5290, "likes": 53, "license": null, "languages": null, "task_categories": null, "created_at": "2025-04-10T08:38:05+00:00", "last_modified": "2025-11-10T14:02:12+00:00", "citation": null}
{"dataset_id": "open-r1/Mixture-of-Thoughts", "dataset_url": "https://huggingface.co/datasets/open-r1/Mixture-of-Thoughts", "stage": "post-training", "nature": "real", "content_types": "code, math, reasoning", "tokens": null, "description": "Mixture-of-Thoughts is a curated dataset of 350k verified reasoning traces distilled from DeepSeek-R1, spanning tasks in mathematics, coding, and science, designed to teach language models to reason step-by-step.", "author": "open-r1", "hf_id": "open-r1/Mixture-of-Thoughts", "downloads": 4690, "likes": 295, "license": null, "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2025-05-11T19:33:11+00:00", "last_modified": "2025-05-26T15:25:56+00:00", "citation": null}
{"dataset_id": "open-r1/OpenR1-Math-220k", "dataset_url": "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k", "stage": "post-training", "nature": "mixed", "content_types": "math, reasoning", "tokens": null, "description": "OpenR1-Math-220k is a dataset for mathematical reasoning consisting of 220k math problems with reasoning traces generated by DeepSeek R1 for problems from NuminaMath 1.5, verified for correctness.", "author": "open-r1", "hf_id": "open-r1/OpenR1-Math-220k", "downloads": 12528, "likes": 687, "license": "apache-2.0", "languages": ["en"], "task_categories": null, "created_at": "2025-02-10T13:41:48+00:00", "last_modified": "2025-02-18T11:45:27+00:00", "citation": null}
{"dataset_id": "open-r1/OpenR1-Math-Raw", "dataset_url": "https://huggingface.co/datasets/open-r1/OpenR1-Math-Raw", "stage": "post-training", "nature": "real", "content_types": "math, reasoning", "tokens": null, "description": "OpenR1-Math-Raw is a large-scale dataset for mathematical reasoning consisting of 516k math problems sourced from AI-MO/NuminaMath-1.5 with reasoning traces generated by DeepSeek R1, verified using Math Verify and LLM-as-Judge.", "author": "open-r1", "hf_id": "open-r1/OpenR1-Math-Raw", "downloads": 495, "likes": 76, "license": "apache-2.0", "languages": ["en"], "task_categories": null, "created_at": "2025-02-12T11:56:48+00:00", "last_modified": "2025-02-24T15:22:26+00:00", "citation": null}
{"dataset_id": "open-r1/OpenThoughts-114k-math", "dataset_url": "https://huggingface.co/datasets/open-r1/OpenThoughts-114k-math", "stage": "post-training", "nature": "real", "content_types": "math", "tokens": null, "description": "This dataset contains filtered and metadata-enriched math content from the original OpenThoughts-114k dataset, mapping DeepSeek-R1 model outputs to original questions from NuminaMath-CoT and verifying correctness with Math-Verify.", "author": "open-r1", "hf_id": "open-r1/OpenThoughts-114k-math", "downloads": 507, "likes": 87, "license": null, "languages": null, "task_categories": null, "created_at": "2025-01-29T18:49:03+00:00", "last_modified": "2025-01-30T11:05:51+00:00", "citation": null}
{"dataset_id": "open-r1/s1K-1.1", "dataset_url": "https://huggingface.co/datasets/open-r1/s1K-1.1", "stage": "pretraining", "nature": "synthetic", "content_types": "other", "tokens": null, "description": "This dataset is a formatted version of simplescaling/s1K-1.1 to include a `messages` column for direct use within TRL.", "author": "open-r1", "hf_id": "open-r1/s1K-1.1", "downloads": 55, "likes": 2, "license": null, "languages": null, "task_categories": null, "created_at": "2025-02-16T21:52:18+00:00", "last_modified": "2025-02-17T14:04:32+00:00", "citation": null}
{"dataset_id": "open-thoughts/open-thoughts-114k", "dataset_url": "https://huggingface.co/datasets/open-thoughts/open-thoughts-114k", "stage": "post-training", "nature": "synthetic", "content_types": "code, math, reasoning", "tokens": null, "description": "Open synthetic reasoning dataset with 114k high-quality examples covering math, science, code, and puzzles!", "author": "open-thoughts", "hf_id": "open-thoughts/OpenThoughts-114k", "downloads": 107043, "likes": 781, "license": "apache-2.0", "languages": null, "task_categories": null, "created_at": "2025-01-27T20:02:16+00:00", "last_modified": "2025-08-31T00:24:46+00:00", "citation": null}
{"dataset_id": "open-thoughts/OpenThoughts-114k", "dataset_url": "https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k", "stage": "post-training", "nature": "synthetic", "content_types": "code, math, reasoning", "tokens": null, "description": "Open synthetic reasoning dataset with 114k high-quality examples covering math, science, code, and puzzles!", "author": "open-thoughts", "hf_id": "open-thoughts/OpenThoughts-114k", "downloads": 107043, "likes": 781, "license": "apache-2.0", "languages": null, "task_categories": null, "created_at": "2025-01-27T20:02:16+00:00", "last_modified": "2025-08-31T00:24:46+00:00", "citation": null}
{"dataset_id": "open-thoughts/OpenThoughts2-1M", "dataset_url": "https://huggingface.co/datasets/open-thoughts/OpenThoughts2-1M", "stage": "post-training", "nature": "synthetic", "content_types": "code, math, reasoning", "tokens": null, "description": "OpenThoughts2-1M is a synthetic reasoning dataset with 1M high-quality examples covering math, science, code, and puzzles, built upon OpenThoughts-114k and augmented with existing datasets like OpenR1 and additional math and code reasoning data.", "author": "open-thoughts", "hf_id": "open-thoughts/OpenThoughts2-1M", "downloads": 1445, "likes": 146, "license": "apache-2.0", "languages": null, "task_categories": null, "created_at": "2025-04-03T02:41:44+00:00", "last_modified": "2025-06-05T16:27:27+00:00", "citation": null}
{"dataset_id": "open-thoughts/OpenThoughts3-1.2M", "dataset_url": "https://huggingface.co/datasets/open-thoughts/OpenThoughts3-1.2M", "stage": "post-training", "nature": "real", "content_types": "code, math, reasoning", "tokens": null, "description": "OpenThoughts3-1.2M is a reasoning dataset containing 850,000 math questions, 250,000 code questions, and 100,000 science questions, annotated with QwQ-32B.", "author": "open-thoughts", "hf_id": "open-thoughts/OpenThoughts3-1.2M", "downloads": 12337, "likes": 197, "license": "apache-2.0", "languages": null, "task_categories": ["text-generation"], "created_at": "2025-05-28T21:51:11+00:00", "last_modified": "2025-06-09T16:14:06+00:00", "citation": null}
{"dataset_id": "openai/gsm8k", "dataset_url": "https://huggingface.co/datasets/openai/gsm8k", "stage": "post-training", "nature": "real", "content_types": "math, qa, reasoning", "tokens": null, "description": "GSM8K is a dataset of 8.5K high quality linguistically diverse grade school math word problems designed to support question answering on basic mathematical problems requiring multi-step reasoning.", "author": "openai", "hf_id": "openai/gsm8k", "downloads": 424793, "likes": 1083, "license": ["mit"], "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2022-04-12T10:22:10+00:00", "last_modified": "2025-12-20T18:53:44+00:00", "citation": null}
{"dataset_id": "OpenAssistant/oasst1", "dataset_url": "https://huggingface.co/datasets/OpenAssistant/oasst1", "stage": "post-training", "nature": "real", "content_types": "conversation, instruction-following, multilingual", "tokens": null, "description": "OpenAssistant Conversations (OASST1) is a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with quality ratings, resulting in over 10,000 fully annotated conversation trees.", "author": "OpenAssistant", "hf_id": "OpenAssistant/oasst1", "downloads": 10166, "likes": 1462, "license": "apache-2.0", "languages": ["en", "es", "ru", "de", "pl", "th", "vi", "sv", "bn", "da", "he", "it", "fa", "sk", "id", "nb", "el", "nl", "hu", "eu", "zh", "eo", "ja", "ca", "cs", "bg", "fi", "pt", "tr", "ro", "ar", "uk", "gl", "fr", "ko"], "task_categories": null, "created_at": "2023-04-13T15:48:16+00:00", "last_modified": "2023-05-02T13:21:21+00:00", "citation": null}
{"dataset_id": "PleIAs/SYNTH", "dataset_url": "https://huggingface.co/datasets/PleIAs/SYNTH", "stage": "pretraining", "nature": "synthetic", "content_types": "multilingual, other, reasoning", "tokens": 75000000000, "description": "SYNTH is a fully synthetic dataset derived from Wikipedia articles, designed for training small reasoning models with amplified knowledge and reasoning traces.", "author": "PleIAs", "hf_id": "PleIAs/SYNTH", "downloads": 25449, "likes": 210, "license": "cdla-permissive-2.0", "languages": ["en", "fr", "it", "es", "de", "pl", "nl", "la"], "task_categories": ["text-generation", "zero-shot-classification", "summarization"], "created_at": "2025-11-10T14:08:26+00:00", "last_modified": "2025-11-11T14:38:51+00:00", "citation": null}
{"dataset_id": "PrimeIntellect/NuminaMath-QwQ-CoT-5M", "dataset_url": "https://huggingface.co/datasets/PrimeIntellect/NuminaMath-QwQ-CoT-5M", "stage": "post-training", "nature": "synthetic", "content_types": "math, reasoning", "tokens": null, "description": "INTELLECT-MATH is a 7B parameter model optimized for mathematical reasoning, trained in two stages: SFT on verified QwQ outputs and RL using the PRIME-RL recipe.", "author": "PrimeIntellect", "hf_id": "PrimeIntellect/NuminaMath-QwQ-CoT-5M", "downloads": 680, "likes": 54, "license": "mit", "languages": null, "task_categories": null, "created_at": "2025-01-22T00:18:40+00:00", "last_modified": "2025-01-22T21:00:36+00:00", "citation": null}
{"dataset_id": "PrimeIntellect/real-world-swe-problems", "dataset_url": "https://huggingface.co/datasets/PrimeIntellect/real-world-swe-problems", "stage": "pretraining", "nature": "synthetic", "content_types": "other", "tokens": null, "description": "This is a subset of the task data used to construct SYNTHETIC-1.", "author": "PrimeIntellect", "hf_id": "PrimeIntellect/real-world-swe-problems", "downloads": 69, "likes": 13, "license": null, "languages": null, "task_categories": null, "created_at": "2025-02-05T01:36:49+00:00", "last_modified": "2025-02-06T21:47:31+00:00", "citation": null}
{"dataset_id": "PrimeIntellect/stackexchange-question-answering", "dataset_url": "https://huggingface.co/datasets/PrimeIntellect/stackexchange-question-answering", "stage": "pretraining", "nature": "synthetic", "content_types": "other", "tokens": null, "description": "This is a subset of the task data used to construct SYNTHETIC-1.", "author": "PrimeIntellect", "hf_id": "PrimeIntellect/stackexchange-question-answering", "downloads": 397, "likes": 11, "license": "mit", "languages": null, "task_categories": null, "created_at": "2025-02-05T01:37:03+00:00", "last_modified": "2025-02-06T21:47:00+00:00", "citation": null}
{"dataset_id": "PrimeIntellect/SYNTHETIC-1", "dataset_url": "https://huggingface.co/datasets/PrimeIntellect/SYNTHETIC-1", "stage": "post-training", "nature": "synthetic", "content_types": "code, math, other, qa", "tokens": null, "description": "SYNTHETIC-1 is a reasoning dataset obtained from Deepseek-R1, generated with crowdsourced compute and annotated with diverse verifiers such as LLM judges or symbolic mathematics verifiers.", "author": "PrimeIntellect", "hf_id": "PrimeIntellect/SYNTHETIC-1", "downloads": 1173, "likes": 60, "license": "apache-2.0", "languages": null, "task_categories": null, "created_at": "2025-02-20T03:24:55+00:00", "last_modified": "2025-02-21T02:22:19+00:00", "citation": null}
{"dataset_id": "PrimeIntellect/SYNTHETIC-1-SFT-Data", "dataset_url": "https://huggingface.co/datasets/PrimeIntellect/SYNTHETIC-1-SFT-Data", "stage": "post-training", "nature": "synthetic", "content_types": "code, math, other, qa", "tokens": null, "description": "SYNTHETIC-1 is a reasoning dataset generated from Deepseek-R1, consisting of crowdsourced reasoning traces across multiple domains including mathematics, algorithmic coding, real-world software engineering, open-ended STEM question answering, and synthetic code understanding.", "author": "PrimeIntellect", "hf_id": "PrimeIntellect/SYNTHETIC-1-SFT-Data", "downloads": 332, "likes": 34, "license": "apache-2.0", "languages": null, "task_categories": null, "created_at": "2025-02-20T22:15:14+00:00", "last_modified": "2025-02-21T02:21:56+00:00", "citation": null}
{"dataset_id": "PrimeIntellect/SYNTHETIC-2-SFT-verified", "dataset_url": "https://huggingface.co/datasets/PrimeIntellect/SYNTHETIC-2-SFT-verified", "stage": "post-training", "nature": "synthetic", "content_types": "code, math, reasoning", "tokens": null, "description": "SYNTHETIC-2 is an open reasoning dataset spanning math, coding, and general reasoning tasks with reasoning traces generated collaboratively. It includes high-quality traces from Deepseek-R1-0528 for SFT and multiple traces from smaller models for difficulty estimation.", "author": "PrimeIntellect", "hf_id": "PrimeIntellect/SYNTHETIC-2-SFT-verified", "downloads": 288, "likes": 6, "license": null, "languages": null, "task_categories": null, "created_at": "2025-07-09T19:40:58+00:00", "last_modified": "2025-07-10T21:31:25+00:00", "citation": null}
{"dataset_id": "PrimeIntellect/synthetic-code-understanding", "dataset_url": "https://huggingface.co/datasets/PrimeIntellect/synthetic-code-understanding", "stage": "pretraining", "nature": "synthetic", "content_types": "other", "tokens": null, "description": "This is a subset of the task data used to construct SYNTHETIC-1.", "author": "PrimeIntellect", "hf_id": "PrimeIntellect/synthetic-code-understanding", "downloads": 106, "likes": 18, "license": null, "languages": null, "task_categories": null, "created_at": "2025-02-05T01:36:24+00:00", "last_modified": "2025-02-15T01:06:29+00:00", "citation": null}
{"dataset_id": "PrimeIntellect/verifiable-coding-problems", "dataset_url": "https://huggingface.co/datasets/PrimeIntellect/verifiable-coding-problems", "stage": "pretraining", "nature": "synthetic", "content_types": "other", "tokens": null, "description": "This is a subset of the task data used to construct SYNTHETIC-1.", "author": "PrimeIntellect", "hf_id": "PrimeIntellect/verifiable-coding-problems", "downloads": 642, "likes": 36, "license": null, "languages": null, "task_categories": null, "created_at": "2025-02-05T01:28:49+00:00", "last_modified": "2025-02-06T21:49:12+00:00", "citation": null}
{"dataset_id": "PrimeIntellect/verifiable-math-problems", "dataset_url": "https://huggingface.co/datasets/PrimeIntellect/verifiable-math-problems", "stage": "pretraining", "nature": "synthetic", "content_types": "other", "tokens": null, "description": "This is a subset of the task data used to construct SYNTHETIC-1.", "author": "PrimeIntellect", "hf_id": "PrimeIntellect/verifiable-math-problems", "downloads": 422, "likes": 21, "license": null, "languages": null, "task_categories": null, "created_at": "2025-02-05T01:36:04+00:00", "last_modified": "2025-02-06T21:48:51+00:00", "citation": null}
{"dataset_id": "QuixiAI/dolphin-r1", "dataset_url": "https://huggingface.co/datasets/QuixiAI/dolphin-r1", "stage": "post-training", "nature": "real", "content_types": "conversation, reasoning", "tokens": null, "description": "An 800k sample dataset composed of reasoning samples from DeepSeek-R1, Gemini 2.0 flash thinking, and Dolphin chat, intended for training R1-style reasoning models.", "author": "QuixiAI", "hf_id": "QuixiAI/dolphin-r1", "downloads": 701, "likes": 291, "license": "apache-2.0", "languages": null, "task_categories": null, "created_at": "2025-01-30T02:44:13+00:00", "last_modified": "2025-01-30T18:51:36+00:00", "citation": null}
{"dataset_id": "Replete-AI/code_bagel", "dataset_url": "https://huggingface.co/datasets/Replete-AI/code_bagel", "stage": "pretraining", "nature": "real", "content_types": "code, instruction-following", "tokens": 800000000, "description": "This dataset contains over 3.2 million lines of high quality, filtered, uncensored, deduplicated, unique coding data from various sources, supporting over 100 coding languages.", "author": "Replete-AI", "hf_id": "Replete-AI/code_bagel", "downloads": 446, "likes": 98, "license": "apache-2.0", "languages": ["en"], "task_categories": null, "created_at": "2024-05-10T03:47:29+00:00", "last_modified": "2024-10-08T22:42:41+00:00", "citation": null}
{"dataset_id": "RJT1990/GeneralThoughtArchive", "dataset_url": "https://huggingface.co/datasets/RJT1990/GeneralThoughtArchive", "stage": "post-training", "nature": "real", "content_types": "qa, reasoning", "tokens": null, "description": "This dataset contains questions, reference answers, reasoning traces, and final answers from several popular reasoning models, including DeepSeek-R1, DeepSeek-R1-Zero, OpenThoughts-32B, LIMO, and others.", "author": "RJT1990", "hf_id": "RJT1990/GeneralThoughtArchive", "downloads": 3178, "likes": 70, "license": "mit", "languages": ["en"], "task_categories": null, "created_at": "2025-03-14T12:31:34+00:00", "last_modified": "2025-09-05T11:30:13+00:00", "citation": null}
{"dataset_id": "rombodawg/code_bagel", "dataset_url": "https://huggingface.co/datasets/rombodawg/code_bagel", "stage": "pretraining", "nature": "real", "content_types": "code, instruction-following", "tokens": 800000000, "description": "This dataset contains over 3.2 million lines of high quality, filtered, uncensored, deduplicated, unique coding data from various sources, supporting over 100 coding languages.", "author": "rombodawg", "hf_id": "rombodawg/code_bagel", "downloads": 97, "likes": 6, "license": "apache-2.0", "languages": ["en"], "task_categories": null, "created_at": "2024-10-08T18:42:01+00:00", "last_modified": "2024-10-08T20:40:49+00:00", "citation": null}
{"dataset_id": "sailor2/sea-commoncrawl", "dataset_url": "https://huggingface.co/datasets/sailor2/sea-commoncrawl", "stage": "pretraining", "nature": "real", "content_types": "web", "tokens": null, "description": "The dataset is a filtered version of the Common Crawl web dataset, containing web content.", "author": "sailor2", "hf_id": "sailor2/sea-commoncrawl", "downloads": 2687, "likes": 0, "license": "odc-by", "languages": null, "task_categories": null, "created_at": "2024-10-30T01:25:02+00:00", "last_modified": "2024-12-04T08:10:42+00:00", "citation": null}
{"dataset_id": "ServiceNow-AI/R1-Distill-SFT", "dataset_url": "https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT", "stage": "post-training", "nature": "mixed", "content_types": "instruction-following, other", "tokens": null, "description": "The dataset consists of millions of samples distilled from R1-32b using Numina-math and Tulu, intended for supervised fine-tuning (SFT).", "author": "ServiceNow-AI", "hf_id": "ServiceNow-AI/R1-Distill-SFT", "downloads": 2108, "likes": 311, "license": "cc-by-nc-sa-4.0", "languages": null, "task_categories": null, "created_at": "2025-01-25T20:31:49+00:00", "last_modified": "2025-02-08T22:46:58+00:00", "citation": null}
{"dataset_id": "simplescaling/s1K", "dataset_url": "https://huggingface.co/datasets/simplescaling/s1K", "stage": "post-training", "nature": "real", "content_types": "math, reasoning", "tokens": null, "description": "s1K is a dataset of 1,000 examples of diverse, high-quality & difficult questions with distilled reasoning traces & solutions from Gemini Thining.", "author": "simplescaling", "hf_id": "simplescaling/s1K", "downloads": 1973, "likes": 232, "license": "apache-2.0", "languages": null, "task_categories": null, "created_at": "2025-01-14T07:54:43+00:00", "last_modified": "2025-02-11T01:14:45+00:00", "citation": null}
{"dataset_id": "SkunkworksAI/reasoning-0.01", "dataset_url": "https://huggingface.co/datasets/SkunkworksAI/reasoning-0.01", "stage": "midtraining", "nature": "synthetic", "content_types": "reasoning", "tokens": null, "description": "A synthetic dataset of reasoning chains for a wide variety of tasks, used across multiple reasoning experiments and projects.", "author": "SkunkworksAI", "hf_id": "SkunkworksAI/reasoning-0.01", "downloads": 647, "likes": 285, "license": "apache-2.0", "languages": null, "task_categories": null, "created_at": "2024-07-05T17:37:36+00:00", "last_modified": "2024-09-14T16:06:30+00:00", "citation": null}
{"dataset_id": "Skywork/Skywork-OR1-RL-Data", "dataset_url": "https://huggingface.co/datasets/Skywork/Skywork-OR1-RL-Data", "stage": "post-training", "nature": "real", "content_types": "code, math", "tokens": null, "description": "This dataset contains verifiable, challenging, and diverse math problems (105K) and coding questions (14K) used to train the Skywork-OR1 model series via rule-based reinforcement learning.", "author": "Skywork", "hf_id": "Skywork/Skywork-OR1-RL-Data", "downloads": 1040, "likes": 59, "license": null, "languages": null, "task_categories": null, "created_at": "2025-04-12T10:01:22+00:00", "last_modified": "2025-05-29T02:27:10+00:00", "citation": null}
{"dataset_id": "stanfordnlp/wikitablequestions", "dataset_url": "https://huggingface.co/datasets/stanfordnlp/wikitablequestions", "stage": "pretraining", "nature": "real", "content_types": "qa", "tokens": null, "description": "The WikiTableQuestions dataset is a large-scale dataset for the task of question answering on semi-structured tables.", "author": "stanfordnlp", "hf_id": "stanfordnlp/wikitablequestions", "downloads": 1759, "likes": 29, "license": ["cc-by-4.0"], "languages": ["en"], "task_categories": ["question-answering"], "created_at": "2022-03-14T11:16:52+00:00", "last_modified": "2024-01-18T11:19:00+00:00", "citation": null}
{"dataset_id": "statmt/cc100", "dataset_url": "https://huggingface.co/datasets/statmt/cc100", "stage": "pretraining", "nature": "real", "content_types": "multilingual, web", "tokens": null, "description": "This corpus comprises of monolingual data for 100+ languages and also includes data for romanized languages, constructed using urls and paragraph indices from the CC-Net repository by processing January-December 2018 Commoncrawl snapshots.", "author": "statmt", "hf_id": "statmt/cc100", "downloads": 1803, "likes": 100, "license": ["unknown"], "languages": ["af", "am", "ar", "as", "az", "be", "bg", "bn", "br", "bs", "ca", "cs", "cy", "da", "de", "el", "en", "eo", "es", "et", "eu", "fa", "ff", "fi", "fr", "fy", "ga", "gd", "gl", "gn", "gu", "ha", "he", "hi", "hr", "ht", "hu", "hy", "id", "ig", "is", "it", "ja", "jv", "ka", "kk", "km", "kn", "ko", "ku", "ky", "la", "lg", "li", "ln", "lo", "lt", "lv", "mg", "mk", "ml", "mn", "mr", "ms", "my", "ne", "nl", "no", "ns", "om", "or", "pa", "pl", "ps", "pt", "qu", "rm", "ro", "ru", "sa", "sc", "sd", "si", "sk", "sl", "so", "sq", "sr", "ss", "su", "sv", "sw", "ta", "te", "th", "tl", "tn", "tr", "ug", "uk", "ur", "uz", "vi", "wo", "xh", "yi", "yo", "zh", "zu"], "task_categories": ["text-generation", "fill-mask"], "created_at": "2022-03-02T23:29:22+00:00", "last_modified": "2024-03-05T12:15:34+00:00", "citation": null}
{"dataset_id": "stingning/ultrachat", "dataset_url": "https://huggingface.co/datasets/stingning/ultrachat", "stage": "post-training", "nature": "synthetic", "content_types": "conversation", "tokens": null, "description": "An open-source, large-scale, and multi-round dialogue data powered by Turbo APIs, generated using two separate ChatGPT Turbo APIs where one acts as the user and the other generates responses. The dataset is divided into three sectors: Questions about the World, Writing and Creation, and Assistance on Existent Materials.", "author": "stingning", "hf_id": "stingning/ultrachat", "downloads": 2582, "likes": 467, "license": "mit", "languages": ["en"], "task_categories": ["conversational", "text-generation"], "created_at": "2023-04-20T15:15:28+00:00", "last_modified": "2024-02-22T02:26:29+00:00", "citation": null}
{"dataset_id": "SynthLabsAI/Big-Math-RL-UNVERIFIED", "dataset_url": "https://huggingface.co/datasets/SynthLabsAI/Big-Math-RL-UNVERIFIED", "stage": "post-training", "nature": "real", "content_types": "math", "tokens": null, "description": "Big-Math-Unverified is an offshoot dataset of Big-Math containing math problems that were not solved in Llama-3.1-8B or 405B rollouts, with answers that have not been verified for correctness.", "author": "SynthLabsAI", "hf_id": "SynthLabsAI/Big-Math-RL-UNVERIFIED", "downloads": 17, "likes": 1, "license": null, "languages": null, "task_categories": null, "created_at": "2025-03-06T17:10:55+00:00", "last_modified": "2025-03-06T17:12:01+00:00", "citation": null}
{"dataset_id": "SynthLabsAI/Big-Math-RL-Verified", "dataset_url": "https://huggingface.co/datasets/SynthLabsAI/Big-Math-RL-Verified", "stage": "post-training", "nature": "real", "content_types": "math, reasoning", "tokens": null, "description": "Big-Math is a large-scale, high-quality math dataset containing over 250,000 rigorously filtered and verified problems, specifically curated for reinforcement learning training in language models.", "author": "SynthLabsAI", "hf_id": "SynthLabsAI/Big-Math-RL-Verified", "downloads": 5131, "likes": 213, "license": "apache-2.0", "languages": ["en"], "task_categories": ["question-answering", "text-generation"], "created_at": "2025-02-20T19:32:03+00:00", "last_modified": "2025-03-25T15:33:48+00:00", "citation": null}
{"dataset_id": "teknium/OpenHermes-2.5", "dataset_url": "https://huggingface.co/datasets/teknium/OpenHermes-2.5", "stage": "post-training", "nature": "mixed", "content_types": "conversation, instruction-following", "tokens": null, "description": "The Open Hermes 2.5 dataset is a compilation and curation of many open source datasets and custom created synthetic datasets, primarily synthetically generated instruction and chat samples.", "author": "teknium", "hf_id": "teknium/OpenHermes-2.5", "downloads": 6835, "likes": 780, "license": null, "languages": ["eng"], "task_categories": null, "created_at": "2023-11-12T16:44:26+00:00", "last_modified": "2024-04-15T08:18:12+00:00", "citation": null}
{"dataset_id": "theblackcat102/evol-codealpaca-v1", "dataset_url": "https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1", "stage": "post-training", "nature": "mixed", "content_types": "code, multilingual", "tokens": null, "description": "This dataset is an evolution of the HuggingFaceH4/CodeAlpaca_20K dataset, augmented using 10 strategies including a new language augmentation strategy to convert instructions into Chinese. It aims to facilitate the recreation of wizardcoder models using newer pretrained models and to serve as a testing ground for improved augmentation strategies.", "author": "theblackcat102", "hf_id": "theblackcat102/evol-codealpaca-v1", "downloads": 2516, "likes": 171, "license": "apache-2.0", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2023-07-23T01:28:44+00:00", "last_modified": "2024-03-10T23:59:30+00:00", "citation": null}
{"dataset_id": "THUDM/LongAlign-10k", "dataset_url": "https://huggingface.co/datasets/THUDM/LongAlign-10k", "stage": "post-training", "nature": "real", "content_types": "instruction-following", "tokens": null, "description": "LongAlign-10k is a dataset containing 10,000 long instruction data of 8k-64k in length, designed for LLM alignment on long context.", "author": "zai-org", "hf_id": "zai-org/LongAlign-10k", "downloads": 483, "likes": 81, "license": null, "languages": ["en", "zh"], "task_categories": ["question-answering"], "created_at": "2024-01-29T15:49:36+00:00", "last_modified": "2024-02-22T11:39:00+00:00", "citation": null}
{"dataset_id": "TIGER-Lab/MathInstruct", "dataset_url": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct", "stage": "post-training", "nature": "real", "content_types": "instruction-following, math", "tokens": null, "description": "MathInstruct is a meticulously curated instruction tuning dataset focusing on hybrid use of chain-of-thought and program-of-thought rationales, covering diverse mathematical fields.", "author": "TIGER-Lab", "hf_id": "TIGER-Lab/MathInstruct", "downloads": 4055, "likes": 295, "license": "mit", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2023-09-11T14:21:02+00:00", "last_modified": "2024-05-15T00:06:46+00:00", "citation": null}
{"dataset_id": "TIGER-Lab/WebInstructSub", "dataset_url": "https://huggingface.co/datasets/TIGER-Lab/WebInstructSub", "stage": "post-training", "nature": "real", "content_types": "instruction-following", "tokens": null, "description": "This dataset contains high-quality instruction-response pairs extracted from web forums like StackExchange and Socratic, focusing on domains such as math, science, and humanities.", "author": "TIGER-Lab", "hf_id": "TIGER-Lab/WebInstructSub", "downloads": 1406, "likes": 159, "license": "apache-2.0", "languages": ["en"], "task_categories": ["question-answering"], "created_at": "2024-05-15T14:32:00+00:00", "last_modified": "2024-10-27T03:19:23+00:00", "citation": null}
{"dataset_id": "TigerResearch/pretrain_zh", "dataset_url": "https://huggingface.co/datasets/TigerResearch/pretrain_zh", "stage": "pretraining", "nature": "real", "content_types": "books, web, wikipedia", "tokens": null, "description": "Chinese pretraining dataset containing books, webtext, and Wikipedia content.", "author": "TigerResearch", "hf_id": "TigerResearch/pretrain_zh", "downloads": 2141, "likes": 117, "license": null, "languages": null, "task_categories": null, "created_at": "2023-06-01T01:45:01+00:00", "last_modified": "2023-06-14T13:50:32+00:00", "citation": null}
{"dataset_id": "tiiuae/falcon-refinedweb", "dataset_url": "https://huggingface.co/datasets/tiiuae/falcon-refinedweb", "stage": "pretraining", "nature": "real", "content_types": "web", "tokens": null, "description": "Falcon RefinedWeb is a massive English web dataset built by TII from CommonCrawl through stringent filtering and deduplication, used for pretraining large language models.", "author": "tiiuae", "hf_id": "tiiuae/falcon-refinedweb", "downloads": 46840, "likes": 879, "license": "odc-by", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2023-05-07T14:57:27+00:00", "last_modified": "2023-06-20T12:38:07+00:00", "citation": null}
{"dataset_id": "togethercomputer/RedPajama-Data-1T", "dataset_url": "https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T", "stage": "pretraining", "nature": "real", "content_types": "books, code, qa, web, wikipedia", "tokens": 1200000000000, "description": "RedPajama is a clean-room, fully open-source implementation of the LLaMa dataset, consisting of web, code, academic, and encyclopedia content.", "author": "togethercomputer", "hf_id": "togethercomputer/RedPajama-Data-1T", "downloads": 2069, "likes": 1117, "license": null, "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2023-04-17T06:28:35+00:00", "last_modified": "2024-06-17T11:36:03+00:00", "citation": null}
{"dataset_id": "togethercomputer/RedPajama-Data-V2", "dataset_url": "https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2", "stage": "pretraining", "nature": "real", "content_types": "web", "tokens": 50600000000000, "description": "RedPajama-V2 is an open dataset for training large language models, including over 100B text documents from 84 CommonCrawl snapshots processed with the CCNet pipeline.", "author": "togethercomputer", "hf_id": "togethercomputer/RedPajama-Data-V2", "downloads": 5647, "likes": 389, "license": null, "languages": ["en", "de", "fr", "es", "it"], "task_categories": ["text-generation"], "created_at": "2023-10-26T01:15:21+00:00", "last_modified": "2024-11-21T09:33:17+00:00", "citation": null}
{"dataset_id": "uonlp/CulturaX", "dataset_url": "https://huggingface.co/datasets/uonlp/CulturaX", "stage": "pretraining", "nature": "real", "content_types": "multilingual, web, wikipedia", "tokens": 6300000000000, "description": "CulturaX is a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for large language model (LLM) development. It combines the most recent iteration of mC4 with all accessible OSCAR corpora, undergoing meticulous cleaning and deduplication.", "author": "uonlp", "hf_id": "uonlp/CulturaX", "downloads": 31031, "likes": 562, "license": null, "languages": ["af", "als", "am", "an", "ar", "arz", "as", "ast", "av", "az", "azb", "ba", "bar", "bcl", "be", "bg", "bh", "bn", "bo", "bpy", "br", "bs", "bxr", "ca", "cbk", "ce", "ceb", "ckb", "cs", "cv", "cy", "da", "de", "dsb", "dv", "el", "eml", "en", "eo", "es", "et", "eu", "fa", "fi", "fr", "frr", "fy", "ga", "gd", "gl", "gn", "gom", "gu", "he", "hi", "hr", "hsb", "ht", "hu", "hy", "ia", "id", "ie", "ilo", "io", "is", "it", "ja", "jbo", "jv", "ka", "kk", "km", "kn", "ko", "krc", "ku", "kv", "kw", "ky", "la", "lb", "lez", "li", "lmo", "lo", "lrc", "lt", "lv", "mai", "mg", "mhr", "min", "mk", "ml", "mn", "mr", "mrj", "ms", "mt", "mwl", "my", "myv", "mzn", "nah", "nap", "nds", "ne", "new", "nl", "nn", "no", "oc", "or", "os", "pa", "pam", "pl", "pms", "pnb", "ps", "pt", "qu", "rm", "ro", "ru", "rue", "sa", "sah", "scn", "sd", "sh", "si", "sk", "sl", "so", "sq", "sr", "su", "sv", "sw", "ta", "te", "tg", "th", "tk", "tl", "tr", "tt", "tyv", "ug", "uk", "ur", "uz", "vec", "vi", "vls", "vo", "wa", "war", "wuu", "xal", "xmf", "yi", "yo", "yue", "zh"], "task_categories": ["text-generation", "fill-mask"], "created_at": "2023-09-04T08:20:39+00:00", "last_modified": "2024-12-16T17:24:53+00:00", "citation": null}
{"dataset_id": "virtuoussy/Multi-subject-RLVR", "dataset_url": "https://huggingface.co/datasets/virtuoussy/Multi-subject-RLVR", "stage": "post-training", "nature": "real", "content_types": "qa", "tokens": null, "description": "This dataset consists of college-level multiple-choice QA pairs originally written in Chinese, covering at least 48 first-level subjects, converted into free-form QA pairs and translated into English for evaluation in RL with verifiable rewards.", "author": "virtuoussy", "hf_id": "virtuoussy/Multi-subject-RLVR", "downloads": 141, "likes": 66, "license": "apache-2.0", "languages": ["en"], "task_categories": ["question-answering"], "created_at": "2025-03-31T07:35:23+00:00", "last_modified": "2025-04-16T09:47:25+00:00", "citation": null}
{"dataset_id": "Vivacem/MMIQC", "dataset_url": "https://huggingface.co/datasets/Vivacem/MMIQC", "stage": "post-training", "nature": "mixed", "content_types": "math, other, qa", "tokens": null, "description": "MMIQC is a mixture of question-response pairs extracted from Mathematics Stack Exchange pages and synthetic data augmented from MATH and GSM8K.", "author": "Vivacem", "hf_id": "Vivacem/MMIQC", "downloads": 147, "likes": 18, "license": "apache-2.0", "languages": null, "task_categories": null, "created_at": "2024-01-17T06:36:25+00:00", "last_modified": "2024-01-20T01:51:28+00:00", "citation": null}
{"dataset_id": "vngrs-ai/vngrs-web-corpus", "dataset_url": "https://huggingface.co/datasets/vngrs-ai/vngrs-web-corpus", "stage": "pretraining", "nature": "real", "content_types": "web", "tokens": 25330000000, "description": "vngrs-web-corpus is a mixed-dataset made of cleaned Turkish sections of OSCAR-2201 and mC4, intended for pretraining language models.", "author": "vngrs-ai", "hf_id": "vngrs-ai/vngrs-web-corpus", "downloads": 1102, "likes": 22, "license": "cc-by-nc-sa-4.0", "languages": ["tr"], "task_categories": null, "created_at": "2024-03-12T11:53:55+00:00", "last_modified": "2024-04-01T14:39:39+00:00", "citation": null}
{"dataset_id": "wikimedia/structured-wikipedia", "dataset_url": "https://huggingface.co/datasets/wikimedia/structured-wikipedia", "stage": "pretraining", "nature": "real", "content_types": "wikipedia", "tokens": null, "description": "Early beta release of pre-parsed English and French Wikipedia articles including infoboxes, abstracts, descriptions, main images, and article sections in structured JSON format.", "author": "wikimedia", "hf_id": "wikimedia/structured-wikipedia", "downloads": 1222, "likes": 89, "license": ["cc-by-sa-4.0", "gfdl"], "languages": ["en", "fr"], "task_categories": null, "created_at": "2024-09-19T14:11:27+00:00", "last_modified": "2025-05-29T12:42:31+00:00", "citation": null}
{"dataset_id": "wikimedia/wikipedia", "dataset_url": "https://huggingface.co/datasets/wikimedia/wikipedia", "stage": "pretraining", "nature": "real", "content_types": "wikipedia", "tokens": null, "description": "A dataset containing cleaned articles from Wikipedia across all languages, with each article stripped of markdown and unwanted sections.", "author": "wikimedia", "hf_id": "wikimedia/wikipedia", "downloads": 68078, "likes": 1023, "license": ["cc-by-sa-3.0", "gfdl"], "languages": ["ab", "ace", "ady", "af", "alt", "am", "ami", "an", "ang", "anp", "ar", "arc", "ary", "arz", "as", "ast", "atj", "av", "avk", "awa", "ay", "az", "azb", "ba", "ban", "bar", "bbc", "bcl", "be", "bg", "bh", "bi", "bjn", "blk", "bm", "bn", "bo", "bpy", "br", "bs", "bug", "bxr", "ca", "cbk", "cdo", "ce", "ceb", "ch", "chr", "chy", "ckb", "co", "cr", "crh", "cs", "csb", "cu", "cv", "cy", "da", "dag", "de", "dga", "din", "diq", "dsb", "dty", "dv", "dz", "ee", "el", "eml", "en", "eo", "es", "et", "eu", "ext", "fa", "fat", "ff", "fi", "fj", "fo", "fon", "fr", "frp", "frr", "fur", "fy", "ga", "gag", "gan", "gcr", "gd", "gl", "glk", "gn", "gom", "gor", "got", "gpe", "gsw", "gu", "guc", "gur", "guw", "gv", "ha", "hak", "haw", "hbs", "he", "hi", "hif", "hr", "hsb", "ht", "hu", "hy", "hyw", "ia", "id", "ie", "ig", "ik", "ilo", "inh", "io", "is", "it", "iu", "ja", "jam", "jbo", "jv", "ka", "kaa", "kab", "kbd", "kbp", "kcg", "kg", "ki", "kk", "kl", "km", "kn", "ko", "koi", "krc", "ks", "ksh", "ku", "kv", "kw", "ky", "la", "lad", "lb", "lbe", "lez", "lfn", "lg", "li", "lij", "lld", "lmo", "ln", "lo", "lt", "ltg", "lv", "lzh", "mad", "mai", "map", "mdf", "mg", "mhr", "mi", "min", "mk", "ml", "mn", "mni", "mnw", "mr", "mrj", "ms", "mt", "mwl", "my", "myv", "mzn", "nah", "nan", "nap", "nds", "ne", "new", "nia", "nl", "nn", "no", "nov", "nqo", "nrf", "nso", "nv", "ny", "oc", "olo", "om", "or", "os", "pa", "pag", "pam", "pap", "pcd", "pcm", "pdc", "pfl", "pi", "pih", "pl", "pms", "pnb", "pnt", "ps", "pt", "pwn", "qu", "rm", "rmy", "rn", "ro", "ru", "rue", "rup", "rw", "sa", "sah", "sat", "sc", "scn", "sco", "sd", "se", "sg", "sgs", "shi", "shn", "si", "sk", "skr", "sl", "sm", "smn", "sn", "so", "sq", "sr", "srn", "ss", "st", "stq", "su", "sv", "sw", "szl", "szy", "ta", "tay", "tcy", "te", "tet", "tg", "th", "ti", "tk", "tl", "tly", "tn", "to", "tpi", "tr", "trv", "ts", "tt", "tum", "tw", "ty", "tyv", "udm", "ug", "uk", "ur", "uz", "ve", "vec", "vep", "vi", "vls", "vo", "vro", "wa", "war", "wo", "wuu", "xal", "xh", "xmf", "yi", "yo", "yue", "za", "zea", "zgh", "zh", "zu"], "task_categories": ["text-generation", "fill-mask"], "created_at": "2022-03-02T23:29:22+00:00", "last_modified": "2024-01-09T09:40:51+00:00", "citation": null}
{"dataset_id": "zwhe99/DeepMath-103K", "dataset_url": "https://huggingface.co/datasets/zwhe99/DeepMath-103K", "stage": "post-training", "nature": "real", "content_types": "math", "tokens": null, "description": "DeepMath-103K is a large-scale dataset featuring challenging, verifiable, and decontaminated math problems tailored for RL and SFT.", "author": "zwhe99", "hf_id": "zwhe99/DeepMath-103K", "downloads": 18160, "likes": 285, "license": "mit", "languages": ["en"], "task_categories": ["text-generation", "text2text-generation"], "created_at": "2025-04-14T10:41:33+00:00", "last_modified": "2025-05-29T03:37:07+00:00", "citation": null}
{"dataset_id": "mlabonne/open-perfectblend", "dataset_url": "https://huggingface.co/datasets/mlabonne/open-perfectblend", "stage": "post-training", "nature": "real", "content_types": "code, conversation, instruction-following, math", "tokens": "unknown", "description": "Open-PerfectBlend is an open-source reproduction of an instruction dataset from the paper 'The Perfect Blend: Redefining RLHF with Mixture of Judges', containing chat, math, code, and instruction-following data.", "author": "mlabonne", "hf_id": "mlabonne/open-perfectblend", "downloads": 510, "likes": 56, "license": "apache-2.0", "languages": null, "task_categories": null, "created_at": "2024-10-13T16:04:55+00:00", "last_modified": "2025-01-15T20:01:32+00:00", "citation": null}
{"dataset_id": "mlabonne/orca-agentinstruct-1M-v1-cleaned", "dataset_url": "https://huggingface.co/datasets/mlabonne/orca-agentinstruct-1M-v1-cleaned", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following, other", "tokens": "unknown", "description": "This is a cleaned version of the microsoft/orca-agentinstruct-1M-v1 dataset, which is fully synthetic and uses raw text from the web as seed data.", "author": "mlabonne", "hf_id": "mlabonne/orca-agentinstruct-1M-v1-cleaned", "downloads": 187, "likes": 63, "license": "cdla-permissive-2.0", "languages": ["en"], "task_categories": ["question-answering"], "created_at": "2024-11-17T18:09:55+00:00", "last_modified": "2025-01-25T16:01:27+00:00", "citation": null}
{"dataset_id": "FuseAI/FuseChat-Mixture", "dataset_url": "https://huggingface.co/datasets/FuseAI/FuseChat-Mixture", "stage": "post-training", "nature": "mixed", "content_types": "code, conversation, instruction-following, math", "tokens": "2048", "description": "FuseChat-Mixture is a comprehensive training dataset covering different styles and capabilities, featuring both human-written and model-generated content, spanning general instruction-following and specific skills.", "author": "FuseAI", "hf_id": "FuseAI/FuseChat-Mixture", "downloads": 219, "likes": 28, "license": "apache-2.0", "languages": ["en"], "task_categories": null, "created_at": "2024-02-26T06:38:54+00:00", "last_modified": "2024-03-03T07:11:24+00:00", "citation": null}
{"dataset_id": "MegaScience/MegaScience", "dataset_url": "https://huggingface.co/datasets/MegaScience/MegaScience", "stage": "post-training", "nature": "real", "content_types": "instruction-following, reasoning", "tokens": "unknown", "description": "MegaScience is a large-scale mixture of high-quality open-source datasets focused on science reasoning, consisting of 1.25 million instances collected from NaturalReasoning, Nemotron-Science, and TextbookReasoning.", "author": "MegaScience", "hf_id": "MegaScience/MegaScience", "downloads": 7917, "likes": 120, "license": "cc-by-nc-sa-4.0", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2025-07-18T08:55:30+00:00", "last_modified": "2025-07-24T04:55:24+00:00", "citation": null}
{"dataset_id": "microsoft/orca-math-word-problems-200k", "dataset_url": "https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k", "stage": "post-training", "nature": "synthetic", "content_types": "math", "tokens": "unknown", "description": "This dataset contains ~200K grade school math word problems. All the answers in this dataset is generated using Azure GPT4-Turbo.", "author": "microsoft", "hf_id": "microsoft/orca-math-word-problems-200k", "downloads": 8679, "likes": 466, "license": "mit", "languages": ["en"], "task_categories": ["question-answering"], "created_at": "2024-03-01T00:56:17+00:00", "last_modified": "2024-03-04T18:01:08+00:00", "citation": null}
{"dataset_id": "inclusionAI/Ling-Coder-SFT", "dataset_url": "https://huggingface.co/datasets/inclusionAI/Ling-Coder-SFT", "stage": "post-training", "nature": "synthetic", "content_types": "code, qa", "tokens": "unknown", "description": "Ling-Coder-SFT is a subset of SFT data used for training Ling-Coder Lite, containing over 5 million English and Chinese samples covering more than 20 programming languages and various topics including text-to-code, code completion, code execution reasoning, complex algorithm question-and-answer, and the use of popular Python libraries.", "author": "inclusionAI", "hf_id": "inclusionAI/Ling-Coder-SFT", "downloads": 1457, "likes": 34, "license": "apache-2.0", "languages": ["en", "zh"], "task_categories": ["text-generation"], "created_at": "2025-03-08T12:01:26+00:00", "last_modified": "2025-03-27T12:40:12+00:00", "citation": null}
{"dataset_id": "OpenCoder-LLM/opc-sft-stage2", "dataset_url": "https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage2", "stage": "post-training", "nature": "mixed", "content_types": "code, instruction-following", "tokens": "unknown", "description": "The OpenCoder dataset's sft-stage2 is used for instruction tuning in Stage 2 of OpenCoder, consisting of educational instructions, evolutionary instructions, evaluation instructions, and package-related instructions.", "author": "OpenCoder-LLM", "hf_id": "OpenCoder-LLM/opc-sft-stage2", "downloads": 1326, "likes": 96, "license": "mit", "languages": null, "task_categories": null, "created_at": "2024-11-08T17:33:31+00:00", "last_modified": "2024-11-24T06:41:13+00:00", "citation": null}
{"dataset_id": "m-a-p/CodeFeedback-Filtered-Instruction", "dataset_url": "https://huggingface.co/datasets/m-a-p/CodeFeedback-Filtered-Instruction", "stage": "post-training", "nature": "mixed", "content_types": "code, instruction-following", "tokens": "unknown", "description": "CodeFeedback-Filtered-Instruction is a curated collection of high-quality single-turn code instructions filtered from multiple open-source datasets using an LLM to retain only the most complex queries.", "author": "m-a-p", "hf_id": "m-a-p/CodeFeedback-Filtered-Instruction", "downloads": 2212, "likes": 176, "license": "apache-2.0", "languages": ["en"], "task_categories": ["question-answering"], "created_at": "2024-02-26T07:42:38+00:00", "last_modified": "2024-02-26T09:25:26+00:00", "citation": null}
{"dataset_id": "gretelai/synthetic_text_to_sql", "dataset_url": "https://huggingface.co/datasets/gretelai/synthetic_text_to_sql", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following", "tokens": "23M", "description": "gretelai/synthetic_text_to_sql is a synthetic Text-to-SQL dataset containing 105,851 records across 100 domains, designed to train models to generate SQL queries from natural language prompts.", "author": "gretelai", "hf_id": "gretelai/synthetic_text_to_sql", "downloads": 2671, "likes": 619, "license": "apache-2.0", "languages": ["en"], "task_categories": ["question-answering", "table-question-answering", "text-generation"], "created_at": "2024-03-21T16:04:08+00:00", "last_modified": "2025-12-16T19:17:20+00:00", "citation": null}
{"dataset_id": "Post-training-Data-Flywheel/AutoIF-instruct-61k-with-funcs", "dataset_url": "https://huggingface.co/datasets/Post-training-Data-Flywheel/AutoIF-instruct-61k-with-funcs", "stage": "post-training", "nature": "real", "content_types": "instruction-following", "tokens": "unknown", "description": "AutoIF-instruct-61k-with-funcs is a curated dataset of 61,000 instruction-following examples, specifically designed for post-training fine-tuning of language models with a focus on incorporating function-calling capabilities.", "author": "Post-training-Data-Flywheel", "hf_id": "Post-training-Data-Flywheel/AutoIF-instruct-61k-with-funcs", "downloads": 223, "likes": 6, "license": null, "languages": null, "task_categories": null, "created_at": "2024-10-03T19:05:25+00:00", "last_modified": "2024-10-03T19:06:40+00:00", "citation": null}
{"dataset_id": "argilla/ifeval-like-data", "dataset_url": "https://huggingface.co/datasets/argilla/ifeval-like-data", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following", "tokens": "unknown", "description": "This dataset contains instruction-response pairs synthetically generated using Qwen/Qwen2.5-72B-Instruct following the style of the google/IFEval dataset.", "author": "argilla", "hf_id": "argilla/ifeval-like-data", "downloads": 926, "likes": 46, "license": "other", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2024-09-30T22:37:02+00:00", "last_modified": "2024-10-17T09:43:39+00:00", "citation": null}
{"dataset_id": "kurakurai/luth-sft", "dataset_url": "https://huggingface.co/datasets/kurakurai/luth-sft", "stage": "post-training", "nature": "mixed", "content_types": "instruction-following, math, other", "tokens": "338M", "description": "This dataset was used to fine-tune models to enhance their French capabilities on tasks such as instruction following, mathematics, and general knowledge, with some improvement in English due to knowledge transfer.", "author": "kurakurai", "hf_id": "kurakurai/luth-sft", "downloads": 185, "likes": 11, "license": "odc-by", "languages": ["fr"], "task_categories": ["text-generation"], "created_at": "2025-08-06T19:21:15+00:00", "last_modified": "2025-10-12T19:47:06+00:00", "citation": null}
{"dataset_id": "ServiceNow-AI/M2Lingual", "dataset_url": "https://huggingface.co/datasets/ServiceNow-AI/M2Lingual", "stage": "post-training", "nature": "real", "content_types": "conversation, instruction-following", "tokens": "unknown", "description": "The M2Lingual dataset is a multi-turn multilingual resource designed for instruction fine-tuning of LLMs, encompassing diverse conversation scenarios across multiple languages.", "author": "ServiceNow-AI", "hf_id": "ServiceNow-AI/M2Lingual", "downloads": 484, "likes": 42, "license": "cc-by-nc-sa-4.0", "languages": ["zh", "ne", "uk", "ja", "zu", "ku", "ig", "mg", "fi", "si", "id", "sw", "ar", "sv", "ru", "yo", "en", "ht", "kn", "ta", "te", "sq", "mr", "am", "wo", "it", "tr", "ha", "pl", "el", "lt", "ms", "jv", "sn", "ml", "ps", "ky", "es", "ga", "gu", "ko", "vi", "sd", "fa", "nl", "hu", "so", "pa", "bn", "pt", "da", "hi", "eu", "de", "ur", "su", "xh", "fr"], "task_categories": null, "created_at": "2024-06-13T00:47:39+00:00", "last_modified": "2024-08-13T19:01:43+00:00", "citation": null}
{"dataset_id": "Salesforce/xlam-function-calling-60k", "dataset_url": "https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k", "stage": "post-training", "nature": "synthetic", "content_types": "tool-use", "tokens": "unknown", "description": "The dataset contains 60,000 verified function-calling examples generated by APIGen pipeline, ensuring reliability through format checking, function execution, and semantic verification.", "author": "Salesforce", "hf_id": "Salesforce/xlam-function-calling-60k", "downloads": 3328, "likes": 559, "license": "cc-by-4.0", "languages": ["en"], "task_categories": ["question-answering", "text-generation", "reinforcement-learning"], "created_at": "2024-06-13T02:27:32+00:00", "last_modified": "2025-01-24T19:25:58+00:00", "citation": null}
{"dataset_id": "Bingguang/FunReason-MT", "dataset_url": "https://huggingface.co/datasets/Bingguang/FunReason-MT", "stage": "post-training", "nature": "synthetic", "content_types": "tool-use", "tokens": "unknown", "description": "The FunReason-MT dataset contains 16,000 high-quality multi-turn samples generated using a three-phase data synthesis framework, focusing on complex trajectories requiring environment-API graph interactions, advanced tool-query synthesis, and guided iterative chain for reliable Chain-of-Thought generation.", "author": "Bingguang", "hf_id": "Bingguang/FunReason-MT", "downloads": 331, "likes": 47, "license": "apache-2.0", "languages": ["en"], "task_categories": ["question-answering", "text-generation"], "created_at": "2025-10-23T02:14:56+00:00", "last_modified": "2025-11-18T02:52:24+00:00", "citation": null}
{"dataset_id": "Team-ACE/ToolACE", "dataset_url": "https://huggingface.co/datasets/Team-ACE/ToolACE", "stage": "post-training", "nature": "synthetic", "content_types": "tool-use", "tokens": "unknown", "description": "ToolACE is an automatic agentic pipeline designed to generate Accurate, Complex, and divErse tool-learning data, leveraging a self-evolution synthesis process to curate a comprehensive API pool and generate dialogs through agent interplay.", "author": "Team-ACE", "hf_id": "Team-ACE/ToolACE", "downloads": 995, "likes": 153, "license": "apache-2.0", "languages": ["en", "zh"], "task_categories": ["text-generation"], "created_at": "2024-08-21T06:02:38+00:00", "last_modified": "2024-09-04T02:37:59+00:00", "citation": null}
{"dataset_id": "Salesforce/APIGen-MT-5k", "dataset_url": "https://huggingface.co/datasets/Salesforce/APIGen-MT-5k", "stage": "post-training", "nature": "synthetic", "content_types": "conversation, tool-use", "tokens": "unknown", "description": "This dataset contains 5000 multi-turn trajectories generated by APIGen-MT, an automated pipeline for synthesizing realistic agentic datasets. It focuses on retail and airline domains and includes verified, high-quality interactions between AI agents and humans.", "author": "Salesforce", "hf_id": "Salesforce/APIGen-MT-5k", "downloads": 678, "likes": 88, "license": "cc-by-nc-4.0", "languages": ["en"], "task_categories": ["question-answering", "text-generation", "reinforcement-learning"], "created_at": "2025-04-29T03:29:33+00:00", "last_modified": "2025-10-10T16:53:50+00:00", "citation": null}
{"dataset_id": "allenai/WildChat-4.8M", "dataset_url": "https://huggingface.co/datasets/allenai/WildChat-4.8M", "stage": "post-training", "nature": "real", "content_types": "conversation, reasoning", "tokens": "unknown", "description": "WildChat-4.8M is a collection of 3,199,860 conversations between human users and ChatGPT, containing only non-toxic user inputs and ChatGPT responses as flagged by OpenAI Moderations API or Detoxify.", "author": "allenai", "hf_id": "allenai/WildChat-4.8M", "downloads": 2442, "likes": 113, "license": "odc-by", "languages": null, "task_categories": ["text-generation", "question-answering"], "created_at": "2025-08-08T16:46:24+00:00", "last_modified": "2025-08-11T15:12:58+00:00", "citation": null}
{"dataset_id": "lmsys/lmsys-chat-1m", "dataset_url": "https://huggingface.co/datasets/lmsys/lmsys-chat-1m", "stage": "post-training", "nature": "real", "content_types": "conversation", "tokens": "unknown", "description": "This dataset contains one million real-world conversations with 25 state-of-the-art LLMs collected from 210K unique IP addresses on the Vicuna demo and Chatbot Arena website.", "author": "lmsys", "hf_id": "lmsys/lmsys-chat-1m", "downloads": 5168, "likes": 779, "license": null, "languages": null, "task_categories": ["conversational"], "created_at": "2023-09-20T06:33:44+00:00", "last_modified": "2024-07-27T09:28:42+00:00", "citation": null}
{"dataset_id": "lmarena-ai/arena-human-preference-100k", "dataset_url": "https://huggingface.co/datasets/lmarena-ai/arena-human-preference-100k", "stage": "post-training", "nature": "real", "content_types": "conversation, preference", "tokens": "unknown", "description": "This dataset contains leaderboard conversation data collected between June 2024 and August 2024, including English human preference evaluations used to develop Arena Explorer.", "author": "lmarena-ai", "hf_id": "lmarena-ai/arena-human-preference-100k", "downloads": 592, "likes": 49, "license": null, "languages": null, "task_categories": null, "created_at": "2025-02-04T02:49:55+00:00", "last_modified": "2025-02-11T23:48:51+00:00", "citation": null}
{"dataset_id": "Skywork/Skywork-Reward-Preference-80K-v0.2", "dataset_url": "https://huggingface.co/datasets/Skywork/Skywork-Reward-Preference-80K-v0.2", "stage": "post-training", "nature": "real", "content_types": "preference", "tokens": "unknown", "description": "Skywork Reward Preference 80K is a subset of 80K preference pairs sourced from publicly available data, used to train reward models for LLMs.", "author": "Skywork", "hf_id": "Skywork/Skywork-Reward-Preference-80K-v0.2", "downloads": 718, "likes": 62, "license": null, "languages": null, "task_categories": null, "created_at": "2024-10-11T17:09:42+00:00", "last_modified": "2024-10-25T01:58:25+00:00", "citation": null}
{"dataset_id": "argilla/ultrafeedback-binarized-preferences-cleaned", "dataset_url": "https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned", "stage": "post-training", "nature": "real", "content_types": "preference", "tokens": "unknown", "description": "This dataset is a cleaned version of UltraFeedback binarized preferences, removing TruthfulQA contamination and aligning formatting with other similar datasets for easier integration.", "author": "argilla", "hf_id": "argilla/ultrafeedback-binarized-preferences-cleaned", "downloads": 3745, "likes": 154, "license": "mit", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2023-12-05T11:07:34+00:00", "last_modified": "2023-12-11T14:22:19+00:00", "citation": null}
{"dataset_id": "BAAI/Infinity-Preference", "dataset_url": "https://huggingface.co/datasets/BAAI/Infinity-Preference", "stage": "post-training", "nature": "real", "content_types": "instruction-following, preference", "tokens": "unknown", "description": "Infinity-Preference adjusts preference attribute weights on each task based on Infinity Instruct's capability labelling system. It contains 59438 evenly sampled instructions from Infinity-Instruct's instruction set for each task type, each accompanied by a preference pair sampled from Gemma-2-9B-IT, annotated by task-specific preference attribute weights and ArmoRM.", "author": "BAAI", "hf_id": "BAAI/Infinity-Preference", "downloads": 211, "likes": 80, "license": "apache-2.0", "languages": ["en", "zh"], "task_categories": ["text-generation", "question-answering", "text2text-generation"], "created_at": "2024-08-28T15:25:50+00:00", "last_modified": "2024-08-30T09:54:37+00:00", "citation": null}
{"dataset_id": "Vezora/Code-Preference-Pairs", "dataset_url": "https://huggingface.co/datasets/Vezora/Code-Preference-Pairs", "stage": "post-training", "nature": "synthetic", "content_types": "code, preference", "tokens": "unknown", "description": "The Code-Preference-Pairs Dataset is a synthetic dataset designed to train models to identify and produce code without bugs. It contains pairs of duplicate code examples where the rejected example has introduced bugs and the accepted example is the fixed version.", "author": "Vezora", "hf_id": "Vezora/Code-Preference-Pairs", "downloads": 286, "likes": 28, "license": "other", "languages": null, "task_categories": null, "created_at": "2024-07-27T22:17:45+00:00", "last_modified": "2024-07-28T07:38:33+00:00", "citation": null}
{"dataset_id": "mlabonne/orpo-dpo-mix-40k", "dataset_url": "https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k", "stage": "post-training", "nature": "real", "content_types": "preference, safety", "tokens": "unknown", "description": "This dataset is designed for ORPO or DPO training, combining high-quality DPO datasets with filtered chosen answers.", "author": "mlabonne", "hf_id": "mlabonne/orpo-dpo-mix-40k", "downloads": 482, "likes": 297, "license": "apache-2.0", "languages": ["en"], "task_categories": ["text-generation"], "created_at": "2024-04-17T17:23:51+00:00", "last_modified": "2024-10-17T21:44:52+00:00", "citation": null}
{"dataset_id": "nvidia/HelpSteer3", "dataset_url": "https://huggingface.co/datasets/nvidia/HelpSteer3", "stage": "post-training", "nature": "real", "content_types": "code, instruction-following, other, preference", "tokens": "unknown", "description": "HelpSteer3 is a dataset designed to align models to become more helpful in responding to user prompts, containing preference, feedback, and edit samples across domains like General, STEM, Code, and Multilingual.", "author": "nvidia", "hf_id": "nvidia/HelpSteer3", "downloads": 2645, "likes": 93, "license": "cc-by-4.0", "languages": ["en", "zh", "ko", "fr", "es", "ru", "ja", "de", "it", "pt", "pl", "id", "nl", "vi"], "task_categories": null, "created_at": "2025-03-13T16:18:41+00:00", "last_modified": "2025-11-16T07:18:00+00:00", "citation": null}
{"dataset_id": "lmsys/chatbot_arena_conversations", "dataset_url": "https://huggingface.co/datasets/lmsys/chatbot_arena_conversations", "stage": "post-training", "nature": "real", "content_types": "conversation, preference", "tokens": "unknown", "description": "This dataset contains 33K cleaned conversations with pairwise human preferences collected from 13K unique IP addresses on the Chatbot Arena from April to June 2023.", "author": "lmsys", "hf_id": "lmsys/chatbot_arena_conversations", "downloads": 1523, "likes": 432, "license": "cc", "languages": null, "task_categories": ["conversational"], "created_at": "2023-07-18T11:57:07+00:00", "last_modified": "2023-09-30T01:04:44+00:00", "citation": null}
{"dataset_id": "AmazonScience/FalseReject", "dataset_url": "https://huggingface.co/datasets/AmazonScience/FalseReject", "stage": "post-training", "nature": "synthetic", "content_types": "instruction-following, safety", "tokens": "unknown", "description": "FalseReject is a dataset designed to mitigate over-refusal behavior in large language models, containing adversarially generated benign prompts across 44 safety-related categories paired with structured responses.", "author": "AmazonScience", "hf_id": "AmazonScience/FalseReject", "downloads": 542, "likes": 30, "license": "cc-by-nc-4.0", "languages": ["en"], "task_categories": ["text-generation", "text2text-generation", "fill-mask"], "created_at": "2025-05-09T17:17:38+00:00", "last_modified": "2025-05-14T19:16:44+00:00", "citation": null}
{"dataset_id": "HumanLLMs/Human-Like-DPO-Dataset", "dataset_url": "https://huggingface.co/datasets/HumanLLMs/Human-Like-DPO-Dataset", "stage": "post-training", "nature": "real", "content_types": "conversation", "tokens": "unknown", "description": "This dataset was created to improve conversational fluency and engagement in large language models, containing conversational questions and human-like versus formal responses across 256 topics.", "author": "HumanLLMs", "hf_id": "HumanLLMs/Human-Like-DPO-Dataset", "downloads": 973, "likes": 242, "license": "llama3", "languages": ["en"], "task_categories": null, "created_at": "2024-05-19T10:24:19+00:00", "last_modified": "2025-01-12T21:01:07+00:00", "citation": null}
{"dataset_id": "facebook/research-plan-gen", "dataset_url": "https://huggingface.co/datasets/facebook/research-plan-gen", "stage": "post-training", "nature": "mixed", "content_types": "instruction-following, other", "tokens": "unknown", "description": "Research Plan Generation dataset with three subsets covering ML, Arxiv, and PubMed research papers, containing research tasks with evaluation rubrics and reference solutions.", "author": "facebook", "hf_id": "facebook/research-plan-gen", "downloads": 491, "likes": 121, "license": null, "languages": null, "task_categories": null, "created_at": "2025-12-28T14:03:33+00:00", "last_modified": "2025-12-28T18:28:59+00:00", "citation": null}
